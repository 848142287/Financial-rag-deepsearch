"""
æ•°æ®å»é‡APIç«¯ç‚¹
æä¾›Milvuså‘é‡å’ŒNeo4jèŠ‚ç‚¹çš„å»é‡åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import Optional
from app.core.structured_logging import get_structured_logger

from app.services.vector_deduplicator import get_vector_deduplicator
from app.services.neo4j_deduplicator import get_neo4j_deduplicator

logger = get_structured_logger(__name__)
router = APIRouter(prefix="/api/v1/dedup", tags=["æ•°æ®å»é‡"])


# ============================================================================
# Milvuså‘é‡å»é‡ç«¯ç‚¹
# ============================================================================

class VectorDedupRequest(BaseModel):
    """å‘é‡å»é‡è¯·æ±‚"""
    limit: Optional[int] = Field(default=10000, description="å¤„ç†çš„æœ€å¤§å‘é‡æ•°é‡")
    dry_run: bool = Field(default=True, description="æ˜¯å¦åªåˆ†æä¸åˆ é™¤")


@router.post("/vectors/analyze")
async def analyze_duplicate_vectors(request: VectorDedupRequest):
    """
    åˆ†æMilvusä¸­çš„é‡å¤å‘é‡
    """
    try:
        deduplicator = await get_vector_deduplicator()
        result = await deduplicator.find_duplicate_vectors(
            limit=request.limit,
            dry_run=request.dry_run
        )
        return {
            "success": True,
            "message": f"åˆ†æå®Œæˆï¼Œå‘ç° {result['duplicate_groups']} ç»„é‡å¤",
            "data": result
        }
    except Exception as e:
        logger.error(f"å‘é‡å»é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/vectors/delete")
async def delete_duplicate_vectors(request: VectorDedupRequest):
    """
    åˆ é™¤Milvusä¸­çš„é‡å¤å‘é‡
    """
    try:
        if request.dry_run:
            raise HTTPException(
                status_code=400,
                detail="è¯·å…ˆè¿è¡Œ analyze å¹¶ç¡®è®¤åå†åˆ é™¤ï¼ˆè®¾ç½® dry_run=Falseï¼‰"
            )

        deduplicator = await get_vector_deduplicator()
        result = await deduplicator.find_duplicate_vectors(
            limit=request.limit,
            dry_run=False
        )

        return {
            "success": True,
            "message": f"åˆ é™¤å®Œæˆï¼Œåˆ é™¤äº† {result['duplicates_deleted']} ä¸ªé‡å¤å‘é‡",
            "data": result
        }
    except Exception as e:
        logger.error(f"å‘é‡å»é‡åˆ é™¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/vectors/stats")
async def get_vector_stats():
    """è·å–Milvusé›†åˆç»Ÿè®¡ä¿¡æ¯"""
    try:
        deduplicator = await get_vector_deduplicator()
        stats = await deduplicator.get_collection_stats()
        return {
            "success": True,
            "data": stats
        }
    except Exception as e:
        logger.error(f"è·å–å‘é‡ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# Neo4jå»é‡ç«¯ç‚¹
# ============================================================================

class Neo4jDedupRequest(BaseModel):
    """Neo4jå»é‡è¯·æ±‚"""
    entity_type: Optional[str] = Field(default=None, description="æŒ‡å®šå®ä½“ç±»å‹")
    limit: Optional[int] = Field(default=1000, description="æœ€å¤šæ£€æŸ¥çš„èŠ‚ç‚¹æ•°")
    dry_run: bool = Field(default=True, description="æ˜¯å¦åªåˆ†æä¸æ‰§è¡Œ")


@router.post("/neo4j/nodes/analyze")
async def analyze_duplicate_nodes(request: Neo4jDedupRequest):
    """
    åˆ†æNeo4jä¸­çš„é‡å¤èŠ‚ç‚¹
    """
    try:
        deduplicator = await get_neo4j_deduplicator()
        result = await deduplicator.find_duplicate_nodes(
            entity_type=request.entity_type,
            limit=request.limit
        )
        return {
            "success": True,
            "message": f"åˆ†æå®Œæˆï¼Œå‘ç° {result['duplicate_groups']} ç»„é‡å¤èŠ‚ç‚¹",
            "data": result
        }
    except Exception as e:
        logger.error(f"Neo4jèŠ‚ç‚¹å»é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/neo4j/nodes/merge")
async def merge_duplicate_nodes(request: Neo4jDedupRequest):
    """
    åˆå¹¶Neo4jä¸­çš„é‡å¤èŠ‚ç‚¹
    """
    try:
        deduplicator = await get_neo4j_deduplicator()
        result = await deduplicator.merge_duplicate_nodes(dry_run=request.dry_run)
        return {
            "success": True,
            "message": f"åˆå¹¶å®Œæˆï¼Œå¤„ç†äº† {result['merged_groups']} ç»„é‡å¤èŠ‚ç‚¹",
            "data": result
        }
    except Exception as e:
        logger.error(f"åˆå¹¶Neo4jèŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/neo4j/relationships/analyze")
async def analyze_duplicate_relationships():
    """åˆ†æNeo4jä¸­çš„é‡å¤å…³ç³»"""
    try:
        deduplicator = await get_neo4j_deduplicator()
        result = await deduplicator.find_duplicate_relationships()
        return {
            "success": True,
            "message": f"åˆ†æå®Œæˆï¼Œå‘ç° {result['duplicate_groups']} ç»„é‡å¤å…³ç³»",
            "data": result
        }
    except Exception as e:
        logger.error(f"Neo4jå…³ç³»å»é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/neo4j/relationships/delete")
async def delete_duplicate_relationships(dry_run: bool = True):
    """åˆ é™¤Neo4jä¸­çš„é‡å¤å…³ç³»"""
    try:
        deduplicator = await get_neo4j_deduplicator()
        result = await deduplicator.delete_duplicate_relationships(dry_run=dry_run)
        return {
            "success": True,
            "message": f"åˆ é™¤å®Œæˆï¼Œåˆ é™¤äº† {result['relationships_deleted']} æ¡é‡å¤å…³ç³»",
            "data": result
        }
    except Exception as e:
        logger.error(f"åˆ é™¤Neo4jå…³ç³»å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/neo4j/setup-constraints")
async def setup_unique_constraints():
    """
    è®¾ç½®Neo4jå”¯ä¸€çº¦æŸ
    è¿™æ˜¯æœ€æœ‰æ•ˆçš„é˜²æ­¢é‡å¤çš„æ–¹æ³•
    """
    try:
        deduplicator = await get_neo4j_deduplicator()
        result = await deduplicator.setup_unique_constraints()
        return {
            "success": True,
            "message": f"è®¾ç½®å®Œæˆï¼š{len(result['constraints_created'])} ä¸ªçº¦æŸï¼Œ{len(result['indexes_created'])} ä¸ªç´¢å¼•",
            "data": result
        }
    except Exception as e:
        logger.error(f"è®¾ç½®å”¯ä¸€çº¦æŸå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/neo4j/stats")
async def get_neo4j_stats():
    """è·å–Neo4jå›¾ç»Ÿè®¡ä¿¡æ¯"""
    try:
        deduplicator = await get_neo4j_deduplicator()
        stats = await deduplicator.get_graph_stats()
        return {
            "success": True,
            "data": stats
        }
    except Exception as e:
        logger.error(f"è·å–Neo4jç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# ç»¼åˆå»é‡ç«¯ç‚¹
# ============================================================================

class FullDedupRequest(BaseModel):
    """å®Œæ•´å»é‡è¯·æ±‚"""
    vector_limit: Optional[int] = Field(default=10000)
    node_limit: Optional[int] = Field(default=1000)
    dry_run: bool = Field(default=True, description="æ˜¯å¦åªåˆ†æä¸æ‰§è¡Œ")


@router.post("/full/analyze")
async def full_dedup_analysis(request: FullDedupRequest):
    """
    å®Œæ•´å»é‡åˆ†æï¼šå‘é‡å’ŒèŠ‚ç‚¹
    """
    try:
        results = {}

        # 1. å‘é‡å»é‡åˆ†æ
        vector_dedup = await get_vector_deduplicator()
        results["vectors"] = await vector_dedup.find_duplicate_vectors(
            limit=request.vector_limit,
            dry_run=True
        )

        # 2. Neo4jèŠ‚ç‚¹å»é‡åˆ†æ
        neo4j_dedup = await get_neo4j_deduplicator()
        results["neo4j_nodes"] = await neo4j_dedup.find_duplicate_nodes(
            limit=request.node_limit
        )

        # 3. Neo4jå…³ç³»å»é‡åˆ†æ
        results["neo4j_relationships"] = await neo4j_dedup.find_duplicate_relationships()

        summary = {
            "total_duplicate_vectors": results["vectors"]["duplicates_found"],
            "total_duplicate_nodes": results["neo4j_nodes"]["total_duplicates"],
            "total_duplicate_relationships": results["neo4j_relationships"]["total_duplicates"],
            "dry_run": request.dry_run
        }

        return {
            "success": True,
            "message": "å®Œæ•´å»é‡åˆ†æå®Œæˆ",
            "summary": summary,
            "data": results
        }

    except Exception as e:
        logger.error(f"å®Œæ•´å»é‡åˆ†æå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/full/execute")
async def full_dedup_execute(request: FullDedupRequest, background_tasks: BackgroundTasks):
    """
    æ‰§è¡Œå®Œæ•´å»é‡ï¼ˆåå°ä»»åŠ¡ï¼‰
    """
    try:
        if request.dry_run:
            raise HTTPException(
                status_code=400,
                detail="è¯·å…ˆè¿è¡Œåˆ†æå¹¶ç¡®è®¤åå†æ‰§è¡Œï¼ˆè®¾ç½® dry_run=Falseï¼‰"
            )

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(execute_full_dedup, request)

        return {
            "success": True,
            "message": "å»é‡ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°æ‰§è¡Œ"
        }

    except Exception as e:
        logger.error(f"æäº¤å»é‡ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def execute_full_dedup(request: FullDedupRequest):
    """åå°æ‰§è¡Œå®Œæ•´å»é‡"""
    try:
        logger.info("ğŸš€ å¼€å§‹åå°å»é‡ä»»åŠ¡")

        # 1. å‘é‡å»é‡
        vector_dedup = await get_vector_deduplicator()
        vector_result = await vector_dedup.find_duplicate_vectors(
            limit=request.vector_limit,
            dry_run=False
        )
        logger.info(f"âœ… å‘é‡å»é‡å®Œæˆ: {vector_result['duplicates_deleted']} ä¸ª")

        # 2. Neo4jèŠ‚ç‚¹å»é‡
        neo4j_dedup = await get_neo4j_deduplicator()
        node_result = await neo4j_dedup.merge_duplicate_nodes(dry_run=False)
        logger.info(f"âœ… èŠ‚ç‚¹å»é‡å®Œæˆ: {node_result['nodes_deleted']} ä¸ª")

        # 3. Neo4jå…³ç³»å»é‡
        rel_result = await neo4j_dedup.delete_duplicate_relationships(dry_run=False)
        logger.info(f"âœ… å…³ç³»å»é‡å®Œæˆ: {rel_result['relationships_deleted']} æ¡")

        logger.info("ğŸ‰ åå°å»é‡ä»»åŠ¡å…¨éƒ¨å®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ åå°å»é‡ä»»åŠ¡å¤±è´¥: {e}")


# ============================================================================
# å¿«é€Ÿå»é‡ç«¯ç‚¹ï¼ˆæ¨èä½¿ç”¨ï¼‰
# ============================================================================

@router.post("/quick-setup")
async def quick_setup_and_dedup():
    """
    å¿«é€Ÿè®¾ç½®å’Œå»é‡ï¼ˆæ¨èï¼‰
    1. è®¾ç½®å”¯ä¸€çº¦æŸ
    2. åˆ†æé‡å¤æ•°æ®
    3. è¿”å›å»é‡å»ºè®®
    """
    try:
        results = {}

        # 1. è®¾ç½®çº¦æŸ
        neo4j_dedup = await get_neo4j_deduplicator()
        constraint_result = await neo4j_dedup.setup_unique_constraints()
        results["constraints"] = constraint_result

        # 2. åˆ†æå‘é‡
        vector_dedup = await get_vector_deduplicator()
        vector_result = await vector_dedup.find_duplicate_vectors(limit=5000, dry_run=True)
        results["vectors_analysis"] = {
            "duplicate_groups": vector_result["duplicate_groups"],
            "duplicates_found": vector_result["duplicates_found"]
        }

        # 3. åˆ†æèŠ‚ç‚¹
        node_result = await neo4j_dedup.find_duplicate_nodes(limit=500)
        results["nodes_analysis"] = {
            "duplicate_groups": node_result["duplicate_groups"],
            "total_duplicates": node_result["total_duplicates"]
        }

        # 4. åˆ†æå…³ç³»
        rel_result = await neo4j_dedup.find_duplicate_relationships()
        results["relationships_analysis"] = {
            "duplicate_groups": rel_result["duplicate_groups"],
            "total_duplicates": rel_result["total_duplicates"]
        }

        # 5. ç»Ÿè®¡
        stats = {
            "neo4j": await neo4j_dedup.get_graph_stats(),
            "milvus": await vector_dedup.get_collection_stats()
        }
        results["current_stats"] = stats

        # 6. å»ºè®®
        recommendations = []
        if vector_result["duplicates_found"] > 0:
            recommendations.append(f"å‘ç° {vector_result['duplicates_found']} ä¸ªé‡å¤å‘é‡ï¼Œå»ºè®®æ¸…ç†")
        if node_result["total_duplicates"] > 0:
            recommendations.append(f"å‘ç° {node_result['total_duplicates']} ä¸ªé‡å¤èŠ‚ç‚¹ï¼Œå»ºè®®åˆå¹¶")
        if rel_result["total_duplicates"] > 0:
            recommendations.append(f"å‘ç° {rel_result['total_duplicates']} æ¡é‡å¤å…³ç³»ï¼Œå»ºè®®åˆ é™¤")

        results["recommendations"] = recommendations

        return {
            "success": True,
            "message": "å¿«é€Ÿè®¾ç½®å’Œåˆ†æå®Œæˆ",
            "data": results
        }

    except Exception as e:
        logger.error(f"å¿«é€Ÿè®¾ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
