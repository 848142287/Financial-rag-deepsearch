"""
å¼‚æ­¥åå°ä»»åŠ¡ç³»ç»Ÿ
å®ç°å‘é‡embeddingå’ŒNeo4jçŸ¥è¯†å›¾è°±æŠ½å–çš„å¼‚æ­¥åå°å¤„ç†
ä¸é˜»å¡å‰ç«¯ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘
"""

from app.tasks.unified_task_manager import celery_app
from app.core.structured_logging import get_structured_logger
from typing import Dict, Any, List
import asyncio
from concurrent.futures import ThreadPoolExecutor

logger = get_structured_logger(__name__)

# çº¿ç¨‹æ± æ‰§è¡Œå™¨ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡
thread_pool = ThreadPoolExecutor(max_workers=4, thread_name_prefix="vector_kg_worker")

@celery_app.task(
    bind=True,
    name='app.tasks.async_vector_kg_tasks.vectorize_document_async',
    soft_time_limit=600,  # 10åˆ†é’Ÿ
    max_retries=2,
    default_retry_delay=60
)
def vectorize_document_async(
    self,
    document_id: str,
    chunks_data: List[Dict[str, Any]],
    collection_name: str = "financial_documents"
):
    """
    å¼‚æ­¥å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨åˆ°Milvus

    Args:
        document_id: æ–‡æ¡£ID
        chunks_data: æ–‡æ¡£å—åˆ—è¡¨
        collection_name: Milvusé›†åˆåç§°

    Returns:
        å‘é‡åŒ–ç»“æœç»Ÿè®¡
    """
    task_id = self.request.id
    logger.info(f"ğŸš€ [å¼‚æ­¥ä»»åŠ¡] å¼€å§‹å‘é‡åŒ–æ–‡æ¡£ {document_id}, å…±{len(chunks_data)}ä¸ªå—")

    try:
        self.update_state(
            state='PROGRESS',
            meta={'status': 'åˆå§‹åŒ–embeddingæœåŠ¡', 'progress': 10}
        )

        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œå‘é‡åŒ–ä»»åŠ¡
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                _vectorize_document(document_id, chunks_data, collection_name, self.update_state)
            )
            logger.info(f"âœ… [å¼‚æ­¥ä»»åŠ¡] æ–‡æ¡£ {document_id} å‘é‡åŒ–å®Œæˆ: {result['vectors_count']}ä¸ªå‘é‡")
            return result
        finally:
            loop.close()

    except Exception as e:
        logger.error(f"âŒ [å¼‚æ­¥ä»»åŠ¡] æ–‡æ¡£ {document_id} å‘é‡åŒ–å¤±è´¥: {e}")
        raise

async def _vectorize_document(
    document_id: str,
    chunks_data: List[Dict[str, Any]],
    collection_name: str,
    progress_callback=None
) -> Dict[str, Any]:
    """å¼‚æ­¥å‘é‡åŒ–æ–‡æ¡£ä¸»é€»è¾‘"""

    from app.services.embeddings.unified_embedding_service import get_embedding_service
    from app.services.vectorstore.milvus_vector_store import MilvusVectorStore
    from app.core.database import SessionLocal
    from sqlalchemy import text

    # 1. åˆå§‹åŒ–embeddingæœåŠ¡
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'åˆå§‹åŒ–embeddingæœåŠ¡', 'progress': 20})

    embedding_service = get_embedding_service()
    await embedding_service.initialize()

    # 2. æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆæ”¯æŒå¹¶å‘ï¼‰
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'ç”Ÿæˆå‘é‡embeddings', 'progress': 40})

    all_texts = [chunk.get('text', '') for chunk in chunks_data if chunk.get('text')]
    all_texts = [text for text in all_texts if text.strip()]  # è¿‡æ»¤ç©ºæ–‡æœ¬

    logger.info(f"ğŸ“ å‡†å¤‡å‘é‡åŒ– {len(all_texts)} ä¸ªæ–‡æœ¬å—")

    # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…å†…å­˜æº¢å‡º
    batch_size = 32
    all_vectors = []

    for i in range(0, len(all_texts), batch_size):
        batch_texts = all_texts[i:i+batch_size]
        logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(all_texts) + batch_size - 1)//batch_size}")

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘ç”Ÿæˆå‘é‡
        vectors = await embedding_service.embed_batch(batch_texts)
        all_vectors.extend(vectors)

        if progress_callback:
            progress_pct = 40 + int((i + len(batch_texts)) / len(all_texts) * 30)
            progress_callback(state='PROGRESS', meta={'status': f'ç”Ÿæˆå‘é‡embeddings ({progress_pct}%)', 'progress': progress_pct})

    # 3. å­˜å‚¨åˆ°Milvus
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'å­˜å‚¨å‘é‡åˆ°Milvus', 'progress': 70})

    vector_store = MilvusVectorStore()

    # å‡†å¤‡æ’å…¥æ•°æ®
    insert_data = []
    for i, (chunk, vector) in enumerate(zip(chunks_data, all_vectors)):
        if not chunk.get('text') or not chunk.get('text').strip():
            continue

        insert_data.append({
            'document_id': document_id,
            'chunk_index': i,
            'text': chunk.get('text', ''),
            'vector': vector.tolist() if hasattr(vector, 'tolist') else vector,
            'metadata': {
                'page': chunk.get('page', 0),
                'section': chunk.get('section', ''),
                'chunk_type': chunk.get('type', 'text')
            }
        })

    # æ‰¹é‡æ’å…¥Milvus
    result = await vector_store.insert_documents(
        collection_name=collection_name,
        documents=insert_data
    )

    # 4. æ›´æ–°æ•°æ®åº“çŠ¶æ€
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'æ›´æ–°æ•°æ®åº“', 'progress': 90})

    db = SessionLocal()
    try:
        db.execute(
            text("""
                UPDATE documents
                SET vectorization_status='completed',
                    vectorization_completed_at=NOW(),
                    vectors_count=:count
                WHERE id=:id
            """),
            {'count': len(insert_data), 'id': document_id}
        )
        db.commit()
    finally:
        db.close()

    logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(insert_data)}ä¸ªå‘é‡å·²å­˜å‚¨åˆ°Milvus")

    return {
        'document_id': document_id,
        'vectors_count': len(insert_data),
        'collection_name': collection_name,
        'status': 'success'
    }

@celery_app.task(
    bind=True,
    name='app.tasks.async_vector_kg_tasks.extract_knowledge_graph_async',
    soft_time_limit=900,  # 15åˆ†é’Ÿ
    max_retries=2,
    default_retry_delay=60
)
def extract_knowledge_graph_async(
    self,
    document_id: str,
    parsed_content: str,
    graph_name: str = "financial_kg"
):
    """
    å¼‚æ­¥æŠ½å–Neo4jçŸ¥è¯†å›¾è°±

    Args:
        document_id: æ–‡æ¡£ID
        parsed_content: è§£æåçš„æ–‡æ¡£å†…å®¹
        graph_name: å›¾è°±åç§°

    Returns:
        çŸ¥è¯†å›¾è°±æŠ½å–ç»“æœç»Ÿè®¡
    """
    task_id = self.request.id
    logger.info(f"ğŸš€ [å¼‚æ­¥ä»»åŠ¡] å¼€å§‹æŠ½å–çŸ¥è¯†å›¾è°± {document_id}")

    try:
        self.update_state(
            state='PROGRESS',
            meta={'status': 'åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æœåŠ¡', 'progress': 10}
        )

        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒçŸ¥è¯†å›¾è°±æŠ½å–
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                _extract_knowledge_graph(document_id, parsed_content, graph_name, self.update_state)
            )
            logger.info(f"âœ… [å¼‚æ­¥ä»»åŠ¡] æ–‡æ¡£ {document_id} çŸ¥è¯†å›¾è°±æŠ½å–å®Œæˆ: {result['entities_count']}ä¸ªå®ä½“, {result['relationships_count']}ä¸ªå…³ç³»")
            return result
        finally:
            loop.close()

    except Exception as e:
        logger.error(f"âŒ [å¼‚æ­¥ä»»åŠ¡] æ–‡æ¡£ {document_id} çŸ¥è¯†å›¾è°±æŠ½å–å¤±è´¥: {e}")
        raise

async def _extract_knowledge_graph(
    document_id: str,
    parsed_content: str,
    graph_name: str,
    progress_callback=None
) -> Dict[str, Any]:
    """å¼‚æ­¥æŠ½å–çŸ¥è¯†å›¾è°±ä¸»é€»è¾‘"""

    from app.services.financial_entity_extractor import FinancialEntityExtractor
    from app.services.financial_relationship_extractor import FinancialRelationshipExtractor
    from app.services.financial_metrics_extractor import FinancialMetricsExtractor
    from app.core.database import SessionLocal
    from sqlalchemy import text
    from neo4j import GraphDatabase
    from app.core.config import settings

    # 1. åˆå§‹åŒ–Neo4jè¿æ¥
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'è¿æ¥Neo4j', 'progress': 20})

    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_user, settings.neo4j_password)
    )

    # 2. æŠ½å–å®ä½“
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'æŠ½å–å®ä½“', 'progress': 40})

    entity_extractor = FinancialEntityExtractor()
    entities = await entity_extractor.extract_entities(parsed_content)

    logger.info(f"ğŸ“Š æŠ½å–åˆ° {len(entities)} ä¸ªå®ä½“")

    # 3. æŠ½å–å…³ç³»
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'æŠ½å–å…³ç³»', 'progress': 60})

    relationship_extractor = FinancialRelationshipExtractor()
    relationships = await relationship_extractor.extract_relationships(parsed_content, entities)

    logger.info(f"ğŸ”— æŠ½å–åˆ° {len(relationships)} ä¸ªå…³ç³»")

    # 4. æŠ½å–è´¢åŠ¡æŒ‡æ ‡
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'æŠ½å–è´¢åŠ¡æŒ‡æ ‡', 'progress': 70})

    metrics_extractor = FinancialMetricsExtractor()
    metrics = await metrics_extractor.extract_metrics(parsed_content)

    logger.info(f"ğŸ“ˆ æŠ½å–åˆ° {len(metrics)} ä¸ªè´¢åŠ¡æŒ‡æ ‡")

    # 5. å­˜å‚¨åˆ°Neo4j
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'å­˜å‚¨åˆ°Neo4j', 'progress': 80})

    async def store_to_neo4j():
        """å¼‚æ­¥å­˜å‚¨åˆ°Neo4j"""
        with driver.session() as session:
            # å­˜å‚¨å®ä½“
            for entity in entities:
                await session.run(
                    """
                    MERGE (e:Entity {id: $id})
                    SET e.name = $name,
                        e.type = $type,
                        e.document_id = $document_id,
                        e.properties = $properties,
                        e.updated_at = datetime()
                    """,
                    id=entity.get('id', f"{document_id}_{entity.get('name', '')}_{entity.get('type', '')}"),
                    name=entity.get('name', ''),
                    type=entity.get('type', ''),
                    document_id=document_id,
                    properties=entity.get('properties', {})
                )

            # å­˜å‚¨å…³ç³»
            for rel in relationships:
                await session.run(
                    """
                    MATCH (source:Entity {id: $source_id})
                    MATCH (target:Entity {id: $target_id})
                    MERGE (source)-[r:RELATIONSHIP {type: $rel_type}]->(target)
                    SET r.document_id = $document_id,
                        r.properties = $properties,
                        r.updated_at = datetime()
                    """,
                    source_id=rel.get('source_id'),
                    target_id=rel.get('target_id'),
                    rel_type=rel.get('type', 'RELATED_TO'),
                    document_id=document_id,
                    properties=rel.get('properties', {})
                )

            # å­˜å‚¨è´¢åŠ¡æŒ‡æ ‡
            for metric in metrics:
                await session.run(
                    """
                    MERGE (m:Metric {name: $name, document_id: $document_id})
                    SET m.value = $value,
                        m.unit = $unit,
                        m.period = $period,
                        m.properties = $properties,
                        m.updated_at = datetime()
                    """,
                    name=metric.get('name', ''),
                    document_id=document_id,
                    value=metric.get('value', ''),
                    unit=metric.get('unit', ''),
                    period=metric.get('period', ''),
                    properties=metric.get('properties', {})
                )

    await store_to_neo4j()

    # 6. æ›´æ–°æ•°æ®åº“çŠ¶æ€
    if progress_callback:
        progress_callback(state='PROGRESS', meta={'status': 'æ›´æ–°æ•°æ®åº“', 'progress': 90})

    db = SessionLocal()
    try:
        db.execute(
            text("""
                UPDATE documents
                SET kg_extraction_status='completed',
                    kg_extraction_completed_at=NOW(),
                    entities_count=:entities,
                    relationships_count=:relationships,
                    metrics_count=:metrics
                WHERE id=:id
            """),
            {
                'entities': len(entities),
                'relationships': len(relationships),
                'metrics': len(metrics),
                'id': document_id
            }
        )
        db.commit()
    finally:
        db.close()

    driver.close()

    logger.info(f"âœ… çŸ¥è¯†å›¾è°±æŠ½å–å®Œæˆ: {len(entities)}ä¸ªå®ä½“, {len(relationships)}ä¸ªå…³ç³», {len(metrics)}ä¸ªæŒ‡æ ‡")

    return {
        'document_id': document_id,
        'entities_count': len(entities),
        'relationships_count': len(relationships),
        'metrics_count': len(metrics),
        'graph_name': graph_name,
        'status': 'success'
    }

@celery_app.task(
    bind=True,
    name='app.tasks.async_vector_kg_tasks.pipeline_document_async',
    soft_time_limit=1800,  # 30åˆ†é’Ÿ
    max_retries=1
)
def pipeline_document_async(
    self,
    document_id: str,
    parsed_content: str,
    chunks_data: List[Dict[str, Any]]
):
    """
    å®Œæ•´çš„å¼‚æ­¥å¤„ç†æµæ°´çº¿ï¼šå‘é‡åŒ– + çŸ¥è¯†å›¾è°±æŠ½å–

    Args:
        document_id: æ–‡æ¡£ID
        parsed_content: è§£æåçš„å†…å®¹
        chunks_data: æ–‡æ¡£å—æ•°æ®

    Returns:
        å®Œæ•´å¤„ç†ç»“æœ
    """
    task_id = self.request.id
    logger.info(f"ğŸš€ [å¼‚æ­¥æµæ°´çº¿] å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id}")

    try:
        # 1. è§¦å‘å‘é‡åŒ–ä»»åŠ¡
        logger.info(f"ğŸ“ è§¦å‘å‘é‡åŒ–ä»»åŠ¡...")
        vector_task = vectorize_document_async.apply_async(
            args=[document_id, chunks_data],
            link=self.request.id
        )

        # 2. è§¦å‘çŸ¥è¯†å›¾è°±æŠ½å–ä»»åŠ¡
        logger.info(f"ğŸ“Š è§¦å‘çŸ¥è¯†å›¾è°±æŠ½å–ä»»åŠ¡...")
        kg_task = extract_knowledge_graph_async.apply_async(
            args=[document_id, parsed_content],
            link=self.request.id
        )

        # ç­‰å¾…ä¸¤ä¸ªä»»åŠ¡å®Œæˆ
        vector_result = vector_task.get(timeout=600)
        kg_result = kg_task.get(timeout=900)

        logger.info(f"âœ… [å¼‚æ­¥æµæ°´çº¿] æ–‡æ¡£ {document_id} å¤„ç†å®Œæˆ")

        return {
            'document_id': document_id,
            'vector_result': vector_result,
            'kg_result': kg_result,
            'status': 'completed'
        }

    except Exception as e:
        logger.error(f"âŒ [å¼‚æ­¥æµæ°´çº¿] æ–‡æ¡£ {document_id} å¤„ç†å¤±è´¥: {e}")
        raise

# å¯¼å‡ºä»»åŠ¡
__all__ = [
    'vectorize_document_async',
    'extract_knowledge_graph_async',
    'pipeline_document_async'
]
