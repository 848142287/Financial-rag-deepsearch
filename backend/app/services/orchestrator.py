"""
è½»é‡çº§æ–‡æ¡£å¤„ç†ç¼–æ’å™¨ - æ›¿ä»£CoreServiceIntegrator
å°†2758è¡Œæ‹†åˆ†ä¸ºå¤šä¸ªå°æœåŠ¡ï¼Œé€šè¿‡ç¼–æ’å™¨åè°ƒ
"""

from datetime import datetime
from app.core.structured_logging import get_structured_logger

logger = get_structured_logger(__name__)

class DocumentProcessingOrchestrator:
    """
    æ–‡æ¡£å¤„ç†ç¼–æ’å™¨ï¼ˆè½»é‡çº§ï¼‰

    èŒè´£ï¼š
    - åè°ƒå„ä¸ªæœåŠ¡
    - ç®¡ç†å¤„ç†æµç¨‹
    - æ”¶é›†å¤„ç†ç»“æœ

    ä¸åŒ…å«å…·ä½“ä¸šåŠ¡é€»è¾‘ï¼Œä»…åšåè°ƒ
    """

    def __init__(self, services: Dict[str, Any]):
        self.services = services
        self._services = {}

    async def initialize(self):
        """åˆå§‹åŒ–å„ä¸ªæœåŠ¡"""
        # å¯¼å…¥å¹¶åˆå§‹åŒ–å„ä¸ªç‹¬ç«‹æœåŠ¡
        from app.services.parsing.document_parsing_service import DocumentParsingService
        from app.services.vector.vector_generation_service import VectorGenerationService
        from app.services.storage.storage_coordinator_service import StorageCoordinatorService
        from app.services.optimized_entity_extractor import OptimizedEntityExtractor

        # åˆå§‹åŒ–è§£ææœåŠ¡
        self._services['parser'] = DocumentParsingService(self.services)
        logger.info("âœ… æ–‡æ¡£è§£ææœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–å‘é‡ç”ŸæˆæœåŠ¡
        self._services['vector_gen'] = VectorGenerationService(
            embedding_service=self.services['embedding'],
            batch_size=50,
            max_concurrent=10
        )
        logger.info("âœ… å‘é‡ç”ŸæˆæœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–å­˜å‚¨åè°ƒæœåŠ¡
        self._services['storage'] = StorageCoordinatorService(self.services)
        logger.info("âœ… å­˜å‚¨åè°ƒæœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–å®ä½“æå–å™¨
        self._services['entity_extractor'] = OptimizedEntityExtractor(
            config={'enable_llm_fallback': True}
        )
        logger.info("âœ… å®ä½“æå–å™¨å·²åˆå§‹åŒ–")

        self._initialized = True
        logger.info("âœ… æ–‡æ¡£å¤„ç†ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")

    async def process_document(
        self,
        file_content: bytes,
        filename: str,
        document_id: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆç¼–æ’å™¨ï¼‰

        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {filename}")

        result = {
            'document_id': document_id,
            'filename': filename,
            'processing_start': datetime.now().isoformat(),
            'stages': {}
        }

        try:
            # é˜¶æ®µ1: è§£ææ–‡æ¡£
            logger.info("ğŸ“„ é˜¶æ®µ1: æ–‡æ¡£è§£æ...")
            text_content, markdown_content, parse_result = await self._services['parser'].parse_document(
                file_content, filename, document_id
            )

            if not text_content and not markdown_content:
                error_msg = "æ–‡æ¡£è§£æå¤±è´¥"
                logger.error(f"âŒ {error_msg}")
                return {**result, 'success': False, 'error': error_msg}

            result['stages']['parsing'] = {
                'status': 'completed',
                'method': parse_result.get('method'),
                'text_length': len(text_content),
                'markdown_length': len(markdown_content)
            }

            # é˜¶æ®µ2: æ–‡æ¡£åˆ†å‰²
            logger.info("âœ‚ï¸ é˜¶æ®µ2: æ–‡æ¡£åˆ†å‰²...")
            chunks_data = await self._chunk_document(text_content, {})
            logger.info(f"âœ… æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(chunks_data)} ä¸ªchunks")

            result['stages']['chunking'] = {
                'status': 'completed',
                'chunk_count': len(chunks_data)
            }

            # é˜¶æ®µ3: å®ä½“æå–
            logger.info("ğŸ”— é˜¶æ®µ3: å®ä½“æå–...")
            entities, relationships = await self._extract_entities(chunks_data, document_id)

            result['stages']['entities'] = {
                'status': 'completed',
                'entity_count': len(entities),
                'relationship_count': len(relationships)
            }

            # é˜¶æ®µ4: å‘é‡ç”Ÿæˆ
            logger.info("ğŸ”¢ é˜¶æ®µ4: å‘é‡ç”Ÿæˆ...")
            chunks_with_embeddings = await self._services['vector_gen'].generate_vectors_batch(
                chunks_data, document_id
            )

            # éªŒè¯å‘é‡è´¨é‡
            vector_stats = self._services['vector_gen'].validate_embeddings(chunks_with_embeddings)

            result['stages']['embeddings'] = {
                'status': 'completed',
                'chunks_processed': len(chunks_with_embeddings),
                'vector_dimension': vector_stats['dimension'],
                'valid_rate': vector_stats['valid_rate']
            }

            # é˜¶æ®µ5: å¹¶è¡Œå­˜å‚¨
            logger.info("ğŸ’¾ é˜¶æ®µ5: æ•°æ®å­˜å‚¨...")
            storage_result = await self._services['storage'].store_all(
                document_id,
                chunks_with_embeddings,
                entities,
                relationships,
                enable_kg=True
            )

            result['stages']['storage'] = storage_result

            result['processing_end'] = datetime.now().isoformat()
            result['status'] = 'completed'
            result['success'] = True

            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}")
            return result

        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            result['status'] = 'failed'
            result['error'] = str(e)
            result['success'] = False
            return result

    async def _chunk_document(
        self,
        text_content: str,
        analysis_result: Dict
    ) -> list:
        """åˆ†å‰²æ–‡æ¡£ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„chunker
        # ä¸ºäº†ç¤ºä¾‹ï¼Œè¿”å›ç®€å•çš„åˆ†å—
        chunk_size = 1000
        chunks = []

        for i in range(0, len(text_content), chunk_size):
            chunks.append({
                'chunk_id': f"chunk_{i // chunk_size}",
                'document_id': '',
                'chunk_index': i // chunk_size,
                'content': text_content[i:i + chunk_size],
                'metadata': {}
            })

        return chunks

    async def _extract_entities(
        self,
        chunks: list,
        document_id: str
    ) -> tuple:
        """æå–å®ä½“å’Œå…³ç³»"""
        from app.services.unified_document_service import UnifiedChunk

        # è½¬æ¢ä¸ºUnifiedChunkæ ¼å¼
        unified_chunks = [
            UnifiedChunk(
                chunk_id=chunk.get('chunk_id'),
                document_id=document_id,
                chunk_index=chunk.get('chunk_index'),
                content=chunk.get('content'),
                metadata=chunk.get('metadata', {})
            )
            for chunk in chunks
        ]

        # æå–å®ä½“
        extracted_entities = await self._services['entity_extractor'].extract_entities_batch(
            unified_chunks,
            config={'min_confidence': 0.6}
        )

        # è½¬æ¢æ ¼å¼
        entities = [
            {
                'name': ent.text,
                'type': ent.entity_type,
                'confidence': ent.confidence,
                'source': ent.source,
                'properties': ent.metadata or {}
            }
            for ent in extracted_entities
        ]

        # æå–å…³ç³»ï¼ˆç®€åŒ–ç‰ˆï¼‰
        relationships = self._extract_relationships_simple(entities, chunks)

        return entities, relationships

    def _extract_relationships_simple(
        self,
        entities: list,
        chunks: list
    ) -> list:
        """ç®€åŒ–ç‰ˆå…³ç³»æå–"""
        relationships = []

        # åŸºäºå…±ç°æå–å…³ç³»
        for chunk in chunks:
            chunk_entities = [
                e for e in entities
                if e.get('name') and e.get('name') in chunk.get('content', '')
            ]

            # ä¸ºæ¯å¯¹å®ä½“å»ºç«‹å…³ç³»
            for i, ent1 in enumerate(chunk_entities):
                for ent2 in chunk_entities[i+1:]:
                    relationships.append({
                        'from_entity': ent1['name'],
                        'to_entity': ent2['name'],
                        'type': 'RELATED_TO',
                        'confidence': 0.7
                    })

        return relationships[:500]  # é™åˆ¶æ•°é‡
