"""
çœŸå®çš„Qwenå¤šæ¨¡æ€å¤§æ¨¡å‹æœåŠ¡
é›†æˆé˜¿é‡Œäº‘çš„Qwen3-VL-Plusã€Qwen-VL-OCRã€Qwen2.5-VL-Embeddingç­‰æ¨¡å‹
"""

import asyncio
import base64
import logging
import os
from datetime import datetime
from typing import Dict, List, Any, Optional, Union
from dataclasses import dataclass
import json
import requests
from PIL import Image
import io
from app.services.minio_service import MinIOService

try:
    import dashscope
    from dashscope import Generation, MultiModalConversation, MultiModalEmbedding, TextEmbedding
    from http import HTTPStatus
    DASHSCOPE_AVAILABLE = True
except ImportError:
    DASHSCOPE_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("dashscope SDK not installed, falling back to HTTP API")

logger = logging.getLogger(__name__)

# MinIO æœåŠ¡å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_minio_service = None

def get_minio_service():
    """è·å– MinIO æœåŠ¡å®ä¾‹"""
    global _minio_service
    if _minio_service is None:
        _minio_service = MinIOService()
    return _minio_service


@dataclass
class RealQwenConfig:
    """çœŸå®QwenæœåŠ¡é…ç½® - å¼ºåˆ¶ä½¿ç”¨é«˜çº§å¤šæ¨¡æ€æ¨¡å‹"""
    api_key: str = "sk-5233a3a4b1a24426b6846a432794bbe2"
    # DashScope SDK ä½¿ç”¨åŸç”Ÿ APIï¼Œä¸éœ€è¦ compatible-mode
    base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    multimodal_model: str = "qwen-vl-plus"  # å¤šæ¨¡æ€æ¨¡å‹ï¼ˆç”¨äºå›¾åƒ+æ–‡æœ¬åˆ†æï¼‰
    text_model: str = "qwen-plus"  # çº¯æ–‡æœ¬æ¨¡å‹ï¼ˆç”¨äºå®ä½“æå–ã€æ–‡æœ¬ç”Ÿæˆç­‰ï¼‰
    ocr_model: str = "qwen-vl-ocr-latest"  # OCRä¸“ç”¨æ¨¡å‹
    embedding_model: str = "text-embedding-v4"  # å¤šæ¨¡æ€åµŒå…¥æ¨¡å‹
    text_embedding_model: str = "text-embedding-v4"  # çº¯æ–‡æœ¬åµŒå…¥æ¨¡å‹
    rerank_model: str = "qwen3-rerank"  # é‡æ’åºæ¨¡å‹
    timeout: int = 120  # å¢åŠ è¶…æ—¶æ—¶é—´
    max_retries: int = 3
    temperature: float = 0.3
    max_tokens: int = 8000
    # å¼ºåˆ¶å¯ç”¨æ‰€æœ‰é«˜çº§åŠŸèƒ½
    enable_image_analysis: bool = True
    enable_chart_analysis: bool = True
    enable_formula_extraction: bool = True
    enable_entity_extraction: bool = True


class RealQwenService:
    """çœŸå®Qwenå¤šæ¨¡æ€æœåŠ¡ç±»"""

    def __init__(self, config: Optional[RealQwenConfig] = None):
        if config is None:
            config = RealQwenConfig()
        self.config = config

        # åˆå§‹åŒ–dashscope
        if DASHSCOPE_AVAILABLE:
            dashscope.api_key = self.config.api_key
            logger.info("ä½¿ç”¨DashScope SDK")
        else:
            logger.warning("DashScope SDKæœªå®‰è£…ï¼Œä½¿ç”¨HTTP API")

    async def analyze_document_multimodal(self, file_content: bytes, filename: str, sections: List[Dict]) -> Dict[str, Any]:
        """ä½¿ç”¨qwen-vl-plusè¿›è¡Œå¤šæ¨¡æ€æ–‡æ¡£åˆ†æ"""
        logger.info(f"ä½¿ç”¨{self.config.multimodal_model}è¿›è¡Œå¤šæ¨¡æ€åˆ†æ...")

        try:
            if DASHSCOPE_AVAILABLE:
                return await self._analyze_with_sdk(file_content, filename, sections)
            else:
                return await self._analyze_with_http(file_content, filename, sections)
        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€åˆ†æå¤±è´¥: {e}")
            return self._get_fallback_analysis(sections)

    async def _analyze_with_sdk(self, file_content: bytes, filename: str, sections: List[Dict]) -> Dict[str, Any]:
        """ä½¿ç”¨DashScope SDKè¿›è¡Œåˆ†æ - ä½¿ç”¨OCRæ–¹å¼æå–å›¾ç‰‡æ–‡æœ¬"""
        analysis_results = {
            'model_used': f"{self.config.multimodal_model}+{self.config.ocr_model}",
            'analysis_timestamp': datetime.now().isoformat(),
            'summary': '',
            'sections_analysis': [],
            'images_found': [],
            'charts_found': [],
            'formulas_found': []
        }

        # ä½¿ç”¨OCRæœåŠ¡æå–å›¾ç‰‡æ–‡æœ¬
        try:
            from app.services.ocr_service import get_ocr_service
            ocr_service = get_ocr_service()

            import fitz  # PyMuPDF
            pdf_document = fitz.open(stream=file_content, filetype="pdf")

            all_text = ""
            for page_num in range(min(len(pdf_document), 10)):  # é™åˆ¶å¤„ç†å‰10é¡µ
                page = pdf_document[page_num]

                # æå–æ–‡æœ¬
                text = page.get_text()
                all_text += f"\n\n--- ç¬¬ {page_num + 1} é¡µ ---\n\n{text}"

                # è½¬æ¢é¡µé¢ä¸ºå›¾ç‰‡è¿›è¡ŒOCR
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                img_data = pix.tobytes("png")

                # ä½¿ç”¨OCRæå–å›¾ç‰‡ä¸­çš„æ–‡æœ¬
                try:
                    logger.info(f"ä½¿ç”¨{self.config.ocr_model}å¯¹ç¬¬{page_num + 1}é¡µè¿›è¡ŒOCR...")

                    # è°ƒç”¨OCRæœåŠ¡æå–æ–‡æœ¬
                    ocr_result = await ocr_service.extract_text_from_image(
                        image_bytes=img_data,
                        prompt="""è¯·è¯†åˆ«å›¾ç‰‡ä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ­£æ–‡ã€è¡¨æ ¼ã€å›¾è¡¨ç­‰ã€‚
è¯·ä¿æŒåŸæ–‡çš„æ ¼å¼å’Œç»“æ„ï¼ŒæŒ‰é€»è¾‘è¾“å‡ºè¯†åˆ«ç»“æœã€‚

è¯·ç‰¹åˆ«æ³¨æ„ï¼š
1. å¦‚æœæœ‰è¡¨æ ¼ï¼Œè¯·ç”¨è¡¨æ ¼æ ¼å¼å‘ˆç°
2. å¦‚æœæœ‰å›¾è¡¨ï¼Œè¯·æè¿°å›¾è¡¨çš„å†…å®¹å’Œæ•°æ®
3. å¦‚æœæœ‰å…¬å¼ï¼Œè¯·ç”¨æ–‡å­—æè¿°å…¬å¼
4. ä¿æŒæ®µè½ç»“æ„"""
                    )

                    if ocr_result['success']:
                        ocr_text = ocr_result['text']
                        logger.info(f"âœ… OCRæˆåŠŸï¼Œæå–äº† {len(ocr_text)} å­—ç¬¦")

                        # ä½¿ç”¨LLMåˆ†æOCRæå–çš„æ–‡æœ¬
                        analysis_prompt = f"""åŸºäºä»¥ä¸‹OCRè¯†åˆ«çš„æ–‡æœ¬å†…å®¹ï¼Œè¯·æä¾›ç»“æ„åŒ–åˆ†æï¼š

è¯†åˆ«çš„æ–‡æœ¬å†…å®¹ï¼š
{ocr_text[:2000]}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
  "title": "é¡µé¢ä¸»æ ‡é¢˜",
  "summary": "å†…å®¹æ‘˜è¦ï¼ˆ100å­—ä»¥å†…ï¼‰",
  "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"],
  "has_images": true/false,
  "has_charts": true/false,
  "has_formulas": true/false,
  "content_types": ["è¡¨æ ¼", "å›¾è¡¨", "å…¬å¼", "æ­£æ–‡"],
  "entities": ["å®ä½“1", "å®ä½“2"],
  "key_data": ["å…³é”®æ•°æ®1", "å…³é”®æ•°æ®2"]
}}"""

                        messages = [
                            {
                                "role": "user",
                                "content": analysis_prompt
                            }
                        ]

                        # ä½¿ç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹åˆ†æOCRç»“æœ
                        response = Generation.call(
                            model="qwen-plus",  # ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹åˆ†æOCRç»“æœ
                            messages=messages,
                            temperature=0.3,
                            max_tokens=1500
                        )

                        if response.status_code == 200:
                            result_text = response.output.text
                            try:
                                # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
                                if result_text.startswith('```json'):
                                    result_text = result_text[7:]
                                elif result_text.startswith('```'):
                                    result_text = result_text[3:]
                                if result_text.endswith('```'):
                                    result_text = result_text[:-3]
                                result_text = result_text.strip()

                                result_json = json.loads(result_text)

                                # æ„å»ºåˆ†æç»“æœ
                                section_analysis = {
                                    'page': page_num + 1,
                                    'title': result_json.get('title', f'ç¬¬ {page_num + 1} é¡µ'),
                                    'summary': result_json.get('summary', ocr_text[:200]),
                                    'key_points': result_json.get('key_points', []),
                                    'has_images': result_json.get('has_images', False),
                                    'has_charts': result_json.get('has_charts', False),
                                    'has_formulas': result_json.get('has_formulas', False),
                                    'ocr_text': ocr_text,
                                    'content_types': result_json.get('content_types', [])
                                }

                                analysis_results['sections_analysis'].append(section_analysis)

                                # è®°å½•å›¾ç‰‡ã€å›¾è¡¨ã€å…¬å¼ä¿¡æ¯
                                if section_analysis['has_images']:
                                    analysis_results['images_found'].append({
                                        'page': page_num + 1,
                                        'description': f"æ£€æµ‹åˆ°å›¾ç‰‡å†…å®¹ (OCRè¯†åˆ«)"
                                    })

                                if section_analysis['has_charts']:
                                    analysis_results['charts_found'].append({
                                        'page': page_num + 1,
                                        'analysis': "æ£€æµ‹åˆ°å›¾è¡¨æˆ–æ•°æ® (OCRè¯†åˆ«)"
                                    })

                                if section_analysis['has_formulas']:
                                    analysis_results['formulas_found'].append({
                                        'page': page_num + 1,
                                        'explanation': "æ£€æµ‹åˆ°æ•°å­¦å…¬å¼ (OCRè¯†åˆ«)"
                                    })

                                logger.info(f"âœ… ç¬¬ {page_num + 1} é¡µåˆ†æå®Œæˆ")

                            except json.JSONDecodeError:
                                logger.warning(f"æ— æ³•è§£æç¬¬ {page_num + 1} é¡µçš„åˆ†æç»“æœï¼Œä½¿ç”¨åŸå§‹OCRæ–‡æœ¬")
                                # ä½¿ç”¨OCRæ–‡æœ¬ä½œä¸ºç»“æœ
                                section_analysis = {
                                    'page': page_num + 1,
                                    'title': f'ç¬¬ {page_num + 1} é¡µ',
                                    'summary': ocr_text[:200],
                                    'key_points': [],
                                    'has_images': False,
                                    'has_charts': False,
                                    'has_formulas': False,
                                    'ocr_text': ocr_text
                                }
                                analysis_results['sections_analysis'].append(section_analysis)
                        else:
                            logger.error(f"åˆ†æOCRç»“æœå¤±è´¥: {response.status_code}")
                            # ç›´æ¥ä½¿ç”¨OCRæ–‡æœ¬
                            section_analysis = {
                                'page': page_num + 1,
                                'title': f'ç¬¬ {page_num + 1} é¡µ',
                                'summary': ocr_text[:200],
                                'key_points': [],
                                'has_images': False,
                                'has_charts': False,
                                'has_formulas': False,
                                'ocr_text': ocr_text
                            }
                            analysis_results['sections_analysis'].append(section_analysis)
                    else:
                        logger.error(f"OCRå¤±è´¥: {ocr_result.get('error', 'Unknown error')}")

                except Exception as e:
                    logger.error(f"ç¬¬ {page_num + 1} é¡µOCRå¤„ç†å¤±è´¥: {e}")
                    # ä½¿ç”¨çº¯æ–‡æœ¬ä½œä¸ºå›é€€
                    section_analysis = {
                        'page': page_num + 1,
                        'title': f'ç¬¬ {page_num + 1} é¡µ',
                        'summary': text[:200] if text else '',
                        'key_points': [],
                        'has_images': False,
                        'has_charts': False,
                        'has_formulas': False,
                        'extracted_text': text
                    }
                    if text.strip():
                        analysis_results['sections_analysis'].append(section_analysis)

            pdf_document.close()

            # ç”Ÿæˆæ•´ä½“æ‘˜è¦
            if all_text:
                summary_messages = [
                    {
                        "role": "user",
                        "content": f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š

{all_text[:2000]}...

æ‘˜è¦åº”è¯¥åŒ…å«ï¼š
1. æ–‡æ¡£ä¸»é¢˜
2. ä¸»è¦å†…å®¹
3. å…³é”®ç»“è®º"""
                    }
                ]

                summary_response = Generation.call(
                    model=self.config.multimodal_model,
                    messages=summary_messages,
                    temperature=0.3,
                    max_tokens=300
                )

                if summary_response.status_code == 200:
                    analysis_results['summary'] = summary_response.output.text

        except Exception as e:
            logger.error(f"PDFå¤„ç†å¤±è´¥: {e}")
            return await self._analyze_text_only(sections)

        return analysis_results

    async def _analyze_text_only(self, sections: List[Dict]) -> Dict[str, Any]:
        """çº¯æ–‡æœ¬åˆ†æï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        analysis_results = {
            'model_used': self.config.multimodal_model,
            'analysis_timestamp': datetime.now().isoformat(),
            'summary': 'è¿™æ˜¯ä¸€ä¸ªPDFæ–‡æ¡£çš„æ–‡æœ¬åˆ†æ',
            'sections_analysis': [],
            'images_found': [],
            'charts_found': [],
            'formulas_found': []
        }

        for section in sections[:10]:
            section_analysis = {
                'page': section.get('page', 1),
                'title': section.get('title', 'æœªå‘½åç« èŠ‚'),
                'summary': section.get('content', '')[:300],
                'key_points': [],
                'has_images': False,
                'has_charts': False,
                'has_formulas': False
            }

            # ç®€å•æ£€æµ‹
            content = section.get('content', '')
            if 'å›¾' in content or 'image' in content.lower():
                section_analysis['has_images'] = True
                analysis_results['images_found'].append({
                    'page': section.get('page', 1),
                    'description': 'æ£€æµ‹åˆ°å›¾ç‰‡å†…å®¹'
                })

            if 'è¡¨' in content or 'chart' in content.lower() or 'æ•°æ®' in content:
                section_analysis['has_charts'] = True
                analysis_results['charts_found'].append({
                    'page': section.get('page', 1),
                    'analysis': 'æ£€æµ‹åˆ°å›¾è¡¨æˆ–æ•°æ®'
                })

            if any(word in content for word in ['å…¬å¼', 'equation', 'Î£', 'âˆ‘', 'âˆ«', 'Â±']):
                section_analysis['has_formulas'] = True
                analysis_results['formulas_found'].append({
                    'page': section.get('page', 1),
                    'explanation': 'æ£€æµ‹åˆ°æ•°å­¦å…¬å¼'
                })

            analysis_results['sections_analysis'].append(section_analysis)

        return analysis_results

    async def _analyze_with_http(self, file_content: bytes, filename: str, sections: List[Dict]) -> Dict[str, Any]:
        """ä½¿ç”¨HTTP APIè¿›è¡Œåˆ†æ"""
        # å®ç°HTTP APIè°ƒç”¨é€»è¾‘
        logger.info("ä½¿ç”¨HTTP APIè¿›è¡Œåˆ†æ")
        return await self._analyze_text_only(sections)

    def _get_fallback_analysis(self, sections: List[Dict]) -> Dict[str, Any]:
        """è·å–å›é€€åˆ†æç»“æœ"""
        return {
            'model_used': self.config.multimodal_model,
            'analysis_timestamp': datetime.now().isoformat(),
            'summary': 'æ–‡æ¡£åˆ†ææ‘˜è¦',
            'sections_analysis': [],
            'images_found': [],
            'charts_found': [],
            'formulas_found': []
        }

    async def extract_entities_relationships(self, text_content: str) -> tuple[List[Dict], List[Dict]]:
        """ä½¿ç”¨qwen-vl-plusæå–å®ä½“å…³ç³» - ä¼˜åŒ–ç‰ˆï¼ˆä¸Šä¸‹æ–‡ç¼©å‡+ç¼“å­˜ï¼‰"""
        logger.info(f"ğŸ” å¼€å§‹æå–å®ä½“å…³ç³»... (æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦)")

        try:
            # ã€ä¼˜åŒ–1ã€‘æ£€æŸ¥Redisç¼“å­˜
            import hashlib
            import redis.asyncio as redis

            text_hash = hashlib.md5(text_content.encode('utf-8')).hexdigest()
            cache_key = f"entity_extraction:{text_hash}"

            try:
                redis_client = await redis.Redis(
                    host='redis',
                    port=6379,
                    password='redis123456',
                    db=3,  # ä½¿ç”¨DB3ç”¨äºå®ä½“ç¼“å­˜
                    decode_responses=False
                )

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_data = await redis_client.get(cache_key)
                if cached_data:
                    import json
                    cached_result = json.loads(cached_data)
                    logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„å®ä½“å…³ç³» (hash: {text_hash[:8]}...)")
                    return cached_result['entities'], cached_result['relationships']

            except Exception as e:
                logger.warning(f"Redisç¼“å­˜æ£€æŸ¥å¤±è´¥: {e}")

            # ã€ä¼˜åŒ–2ã€‘æ™ºèƒ½ä¸Šä¸‹æ–‡ç¼©å‡ - åªå¤„ç†å…³é”®éƒ¨åˆ†
            max_length = 3000  # ä»6000å‡å°‘åˆ°3000ï¼ˆå‡å°‘50%å¤„ç†æ—¶é—´ï¼‰

            if len(text_content) > max_length:
                # æ™ºèƒ½æˆªå–ç­–ç•¥ï¼šä¼˜å…ˆä¿ç•™å¼€å¤´å’Œç»“å°¾ï¼Œè·³è¿‡ä¸­é—´é‡å¤å†…å®¹
                text_start = text_content[:1500]  # å‰1500å­—ç¬¦ï¼ˆé€šå¸¸åŒ…å«é‡è¦ä¿¡æ¯ï¼‰
                text_end = text_content[-1500:] if len(text_content) > 3000 else ""  # å1500å­—ç¬¦

                # ç»„åˆå…³é”®éƒ¨åˆ†
                text_to_process = text_start + "\n...\n" + text_end if text_end else text_start

                logger.info(f"ğŸ“ æ–‡æœ¬æ™ºèƒ½ç¼©å‡: {len(text_content)} â†’ {len(text_to_process)} å­—ç¬¦ (ä¿ç•™{len(text_to_process)/len(text_content)*100:.1f}%)")
            else:
                text_to_process = text_content

            # æ”¹è¿›çš„æç¤ºè¯ - æ›´å…·ä½“çš„æŒ‡ä»¤ï¼Œå¼ºè°ƒå…³ç³»æå–
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æ¡£å®ä½“å’Œå…³ç³»è¯†åˆ«ä¸“å®¶ã€‚è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­ä»”ç»†æå–å®ä½“å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

æ–‡æœ¬å†…å®¹ï¼š
{text_to_process}

è¯·æŒ‰ä»¥ä¸‹è¦æ±‚æå–ï¼š

1. **å®ä½“ç±»å‹**ï¼š
   - å…¬å¸/æœºæ„ï¼ˆå¦‚ï¼šå¯’æ­¦çºªã€åä¸ºã€è‹±ä¼Ÿè¾¾ã€æµ·å…‰ã€ä¸­ç§‘é™¢ï¼‰
   - äº§å“/æŠ€æœ¯ï¼ˆå¦‚ï¼šAIèŠ¯ç‰‡ã€CPUã€GPUã€æ€å…ƒ590ã€A100ã€æ˜‡è…¾910ï¼‰
   - äººç‰©/è§’è‰²ï¼ˆå¦‚ï¼šCEOã€ä¸“å®¶ã€åˆ†æå¸ˆï¼‰
   - æ•°å€¼/æŒ‡æ ‡ï¼ˆå¦‚ï¼š1600äº¿å…ƒã€100%ã€2023å¹´ï¼‰
   - åœ°ç†ä½ç½®ï¼ˆå¦‚ï¼šä¸­å›½ã€ç¾å›½ï¼‰

2. **å…³ç³»ç±»å‹**ï¼ˆé‡è¦ï¼å¿…é¡»æå–å…³ç³»ï¼‰ï¼š
   - ç”Ÿäº§å…³ç³»ï¼šAå…¬å¸ç”Ÿäº§Bäº§å“ï¼ˆä¾‹ï¼šè‹±ä¼Ÿè¾¾ç”Ÿäº§A100èŠ¯ç‰‡ï¼‰
   - ç«äº‰å…³ç³»ï¼šAä¸Båœ¨å¸‚åœºä¸Šç«äº‰ï¼ˆä¾‹ï¼šè‹±ä¼Ÿè¾¾ä¸AMDç«äº‰ï¼‰
   - åˆä½œå…³ç³»ï¼šAä¸Bæœ‰åˆä½œå…³ç³»ï¼ˆä¾‹ï¼šåä¸ºä¸å¯’æ­¦çºªåˆä½œï¼‰
   - æ‰€å±å…³ç³»ï¼šAå±äºBç±»åˆ«æˆ–ä½äºBåœ°
   - å¯¹æ¯”å…³ç³»ï¼šAè¾¾åˆ°Bæ•°å€¼æˆ–å…·æœ‰Bç‰¹å¾

3. **æå–ç¤ºä¾‹**ï¼š
   æ–‡æœ¬ï¼š"è‹±ä¼Ÿè¾¾ç”Ÿäº§A100 GPUèŠ¯ç‰‡ï¼Œä¸å¯’æ­¦çºªçš„æ€å…ƒ590ç«äº‰"
   å®ä½“ï¼š[è‹±ä¼Ÿè¾¾, A100, GPUèŠ¯ç‰‡, å¯’æ­¦çºª, æ€å…ƒ590]
   å…³ç³»ï¼š
   - [è‹±ä¼Ÿè¾¾, ç”Ÿäº§, A100]
   - [è‹±ä¼Ÿè¾¾, ç«äº‰, å¯’æ­¦çºª]
   - [å¯’æ­¦çºª, ç”Ÿäº§, æ€å…ƒ590]

è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
{{
  "entities": [
    {{"name": "è‹±ä¼Ÿè¾¾", "type": "å…¬å¸", "confidence": 0.95}},
    {{"name": "A100", "type": "äº§å“", "confidence": 0.95}}
  ],
  "relationships": [
    {{"from_entity": "è‹±ä¼Ÿè¾¾", "to_entity": "A100", "type": "ç”Ÿäº§", "confidence": 0.9}},
    {{"from_entity": "è‹±ä¼Ÿè¾¾", "to_entity": "å¯’æ­¦çºª", "type": "ç«äº‰", "confidence": 0.85}}
  ]
}}

é‡è¦æç¤ºï¼š
- å¿…é¡»æå–å®ä½“ä¹‹é—´çš„å…³ç³»ï¼Œè¿™æ˜¯æœ€é‡è¦çš„ä»»åŠ¡
- å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å…³ç³»ï¼Œè‡³å°‘æå–"æåŠ"å…³ç³»ï¼ˆåœ¨åŒä¸€å¥è¯ä¸­å‡ºç°çš„å®ä½“ï¼‰
- åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
- å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•å®ä½“ï¼Œè¿”å› {{"entities": [], "relationships": []}}"""

            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸“ä¸šçš„é‡‘èæ–‡æ¡£åˆ†æä¸“å®¶ï¼Œæ“…é•¿æå–å®ä½“å’Œå…³ç³»ã€‚å¿…é¡»è¿”å›çº¯JSONæ ¼å¼ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ]

            # è°ƒç”¨æ¨¡å‹ - ä½¿ç”¨çº¯æ–‡æœ¬æ¨¡å‹è¿›è¡Œå®ä½“æå–
            response = Generation.call(
                model=self.config.text_model,
                messages=messages,
                temperature=0.05,  # é™ä½æ¸©åº¦
                max_tokens=3000,    # å¢åŠ è¾“å‡ºé•¿åº¦
                result_format='message'  # ç¡®ä¿æ¶ˆæ¯æ ¼å¼
            )

            if response.status_code == 200:
                # ä¿®å¤NoneTypeé”™è¯¯: æ£€æŸ¥response.output.textæ˜¯å¦ä¸ºNone
                result_text = response.output.text if response.output.text else ""

                if result_text:
                    result_text = result_text.strip()

                    # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                    if result_text.startswith('```json'):
                        result_text = result_text[7:]
                    elif result_text.startswith('```'):
                        result_text = result_text[3:]
                    if result_text.endswith('```'):
                        result_text = result_text[:-3]
                    result_text = result_text.strip()

                try:
                    result = json.loads(result_text) if result_text else {'entities': [], 'relationships': []}
                    entities = result.get('entities', [])
                    relationships = result.get('relationships', [])

                    # éªŒè¯å’Œæ¸…ç†æ•°æ®
                    valid_entities = []
                    valid_relationships = []

                    for entity in entities:
                        if isinstance(entity, dict) and 'name' in entity and entity['name']:
                            name = str(entity['name']).strip() if entity['name'] else None
                            if name:
                                entity_type = entity.get('type', 'UNKNOWN') or 'UNKNOWN'
                                description = entity.get('description') or ''
                                valid_entities.append({
                                    'name': name,
                                    'type': str(entity_type).strip(),
                                    'description': str(description).strip(),
                                    'confidence': float(entity.get('confidence', 0.7))
                                })

                    for rel in relationships:
                        if isinstance(rel, dict):
                            # æ”¯æŒå¤šç§å­—æ®µåï¼šfrom_entity/to_entity æˆ– source/target
                            from_entity = rel.get('from_entity') or rel.get('source')
                            to_entity = rel.get('to_entity') or rel.get('target')
                            rel_type = rel.get('type') or rel.get('relation', 'RELATED_TO')

                            if from_entity and to_entity:
                                valid_relationships.append({
                                    'from_entity': str(from_entity).strip(),
                                    'to_entity': str(to_entity).strip(),
                                    'type': str(rel_type).strip(),
                                    'description': rel.get('description', ''),
                                    'confidence': float(rel.get('confidence', 0.7))
                                })

                    logger.info(f"âœ… æå–åˆ° {len(valid_entities)} ä¸ªå®ä½“å’Œ {len(valid_relationships)} ä¸ªå…³ç³»")

                    # ã€ä¼˜åŒ–3ã€‘ç¼“å­˜æå–ç»“æœ
                    try:
                        cache_data = {
                            'entities': valid_entities,
                            'relationships': valid_relationships,
                            'text_length': len(text_content),
                            'timestamp': datetime.now().isoformat()
                        }
                        await redis_client.setex(
                            cache_key,
                            86400,  # ç¼“å­˜24å°æ—¶
                            json.dumps(cache_data, ensure_ascii=False)
                        )
                        logger.info(f"âœ… å®ä½“å…³ç³»å·²ç¼“å­˜ (hash: {text_hash[:8]}...)")
                    except Exception as cache_err:
                        logger.warning(f"ç¼“å­˜å­˜å‚¨å¤±è´¥: {cache_err}")

                    return valid_entities, valid_relationships

                except json.JSONDecodeError as je:
                    logger.warning(f"JSONè§£æå¤±è´¥: {je}, å°è¯•å¤‡ç”¨æ–¹æ¡ˆ")
                    # å¤‡ç”¨æ–¹æ¡ˆ: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å®ä½“
                    return await self._extract_entities_fallback(text_to_process)

            else:
                logger.error(f"âŒ APIè°ƒç”¨å¤±è´¥: {response.status_code}, {response.message}")
                logger.error(f"è¯·æ±‚æ¨¡å‹: {self.config.multimodal_model}")
                logger.error(f"å¤±è´¥åŸå› å¯èƒ½: URLé…ç½®é”™è¯¯æˆ–æ¨¡å‹ä¸å¯ç”¨")
                logger.error(f"è¯¦ç»†é”™è¯¯: {response}")
                logger.warning("å°†ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæå–å®ä½“å’Œå…³ç³»...")
                return await self._extract_entities_fallback(text_to_process)

        except Exception as e:
            logger.error(f"å®ä½“å…³ç³»æå–å¤±è´¥: {e}")
            return await self._extract_entities_fallback(text_content)

    async def _extract_entities_fallback(self, text_content: str) -> tuple[List[Dict], List[Dict]]:
        """å¤‡ç”¨å®ä½“æå–æ–¹æ¡ˆ - å¢å¼ºç‰ˆï¼ŒåŒ…å«ç®€å•å…³ç³»æå–"""
        import re

        logger.info("ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆæå–å®ä½“å’Œå…³ç³»...")

        entities = []
        relationships = []

        try:
            # æå–å…¬å¸/æœºæ„åç§°
            company_patterns = [
                r'è‹±ä¼Ÿè¾¾|åä¸º|å¯’æ­¦çºª|æµ·å…‰|ä¸­ç§‘é™¢|é˜¿é‡Œ|è…¾è®¯|ç™¾åº¦|å­—èŠ‚|ç¾å›¢|äº¬ä¸œ|å°ç±³|OPPO|vivo',
                r'\w+[ç§‘æŠ€å…¬å¸|é›†å›¢|è¯åˆ¸|é“¶è¡Œ|ä¿é™©]{1,3}',
                r'OpenAI|Anthropic|Google|Microsoft|Apple|Meta|Amazon'
            ]

            # æå–äº§å“åç§°
            product_patterns = [
                r'[A-Z0-9]+-[A-Z0-9]+',  # å¦‚ A100, H100
                r'æ€å…ƒ\d+[A-Z]*',
                r'æ˜‡è…¾\d+[A-Z]*',
                r'AI\s*èŠ¯ç‰‡|CPU|GPU|DCU|CUDA|CANN|ChatGPT|GPT'
            ]

            # æå–æ•°å€¼æŒ‡æ ‡
            value_patterns = [
                r'\d+[.]\d+\s*(?:äº¿å…ƒ|ä¸‡å…ƒ|%|å€|TFlops)',
                r'\d{4}å¹´',
                r'\d+\s*[ä¸ªé¡¹å°]'
            ]

            all_patterns = company_patterns + product_patterns + value_patterns

            for pattern in all_patterns:
                matches = re.findall(pattern, text_content)
                for match in matches:
                    if len(str(match)) >= 2:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„
                        entities.append({
                            'name': str(match),
                            'type': self._guess_entity_type(str(match)),
                            'description': f"ä»æ–‡æœ¬ä¸­æå–",
                            'confidence': 0.6
                        })

            # å»é‡å®ä½“
            seen = set()
            unique_entities = []
            for entity in entities:
                if entity['name'] not in seen:
                    seen.add(entity['name'])
                    unique_entities.append(entity)

            # ç®€å•å…³ç³»æå–ï¼šåŸºäºå¥å­ä¸­å…±ç°çš„å®ä½“
            logger.info("å¼€å§‹æå–ç®€å•å…³ç³»...")

            # åˆ†å‰²æ–‡æœ¬ä¸ºå¥å­
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›;.\n]', text_content)

            # ä¸ºæ¯ä¸ªå¥å­ä¸­çš„å®ä½“åˆ›å»º"æåŠ"å…³ç³»
            entity_names = [e['name'] for e in unique_entities]

            for sentence in sentences:
                if len(sentence) < 10:  # è·³è¿‡å¤ªçŸ­çš„å¥å­
                    continue

                # æ‰¾å‡ºè¿™ä¸ªå¥å­ä¸­å‡ºç°çš„å®ä½“
                entities_in_sentence = []
                for entity_name in entity_names:
                    if entity_name in sentence:
                        entities_in_sentence.append(entity_name)

                # å¦‚æœå¥å­ä¸­æœ‰2ä¸ªæˆ–æ›´å¤šå®ä½“ï¼Œåˆ›å»ºå…³ç³»
                if len(entities_in_sentence) >= 2:
                    for i in range(len(entities_in_sentence) - 1):
                        from_entity = entities_in_sentence[i]
                        to_entity = entities_in_sentence[i + 1]

                        # é¿å…é‡å¤
                        rel_key = f"{from_entity}->{to_entity}"
                        if not any(r.get('relation_key') == rel_key for r in relationships):
                            relationships.append({
                                'from_entity': from_entity,
                                'to_entity': to_entity,
                                'type': 'æåŠ',
                                'description': f"åœ¨åŒä¸€å¥å­ä¸­å‡ºç°: {sentence[:50]}...",
                                'confidence': 0.5,
                                'relation_key': rel_key
                            })

            logger.info(f"âœ… å¤‡ç”¨æ–¹æ¡ˆæå–åˆ° {len(unique_entities)} ä¸ªå®ä½“å’Œ {len(relationships)} ä¸ªå…³ç³»")
            return unique_entities[:20], relationships[:15]  # é™åˆ¶æ•°é‡

        except Exception as e:
            logger.error(f"å¤‡ç”¨å®ä½“æå–å¤±è´¥: {e}")
            return [], []

    def _guess_entity_type(self, name: str) -> str:
        """æ ¹æ®åç§°çŒœæµ‹å®ä½“ç±»å‹"""
        import re
        if 'å…¬å¸' in name or 'é›†å›¢' in name or 'è¯åˆ¸' in name or 'é“¶è¡Œ' in name:
            return 'å…¬å¸'
        elif any(x in name for x in ['èŠ¯ç‰‡', 'CPU', 'GPU', 'DCU', 'GPT', 'CUDA', 'CANN']):
            return 'äº§å“'
        elif re.search(r'\d+[.]\d+|%|å¹´', name):
            return 'æ•°å€¼'
        else:
            return 'UNKNOWN'

    async def generate_embeddings_multimodal(self, texts: List[str], images: Optional[List[bytes]] = None) -> List[List[float]]:
        """ä½¿ç”¨qwen2.5-vl-embeddingç”Ÿæˆå¤šæ¨¡æ€åµŒå…¥"""
        logger.info(f"ä½¿ç”¨{self.config.embedding_model}ç”Ÿæˆå¤šæ¨¡æ€åµŒå…¥...")

        embeddings = []

        try:
            if DASHSCOPE_AVAILABLE and images:
                # å¤šæ¨¡æ€åµŒå…¥
                for i, text in enumerate(texts[:10]):  # é™åˆ¶å¤„ç†æ•°é‡
                    input_data = [{'text': text}]

                    # å¦‚æœæœ‰å›¾ç‰‡ï¼Œæ·»åŠ å›¾ç‰‡
                    if i < len(images):
                        # å°†å›¾ç‰‡è½¬æ¢ä¸ºbase64
                        img_base64 = base64.b64encode(images[i]).decode()
                        input_data.append({'image': img_base64})

                    resp = MultiModalEmbedding.call(
                        model=self.config.embedding_model,
                        input=input_data
                    )

                    if resp.status_code == HTTPStatus.OK:
                        embedding = resp.output['embeddings'][0]['embedding']
                        embeddings.append(embedding)
                    else:
                        logger.error(f"å¤šæ¨¡æ€åµŒå…¥ç”Ÿæˆå¤±è´¥: {resp}")
                        # ä½¿ç”¨æ–‡æœ¬åµŒå…¥ä½œä¸ºå›é€€
                        text_embedding = await self._generate_text_embedding(text)
                        embeddings.append(text_embedding)
            else:
                # çº¯æ–‡æœ¬åµŒå…¥
                for text in texts[:10]:
                    embedding = await self._generate_text_embedding(text)
                    embeddings.append(embedding)

        except Exception as e:
            logger.error(f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é›¶å‘é‡
            for _ in texts[:10]:
                embeddings.append([0.0] * 1024)  # Qwen2.5-VL-Embeddingç»´åº¦

        return embeddings

    async def _generate_text_embedding(self, text: str) -> List[float]:
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥ï¼ˆä½¿ç”¨text-embedding-v4ï¼‰"""
        try:
            if DASHSCOPE_AVAILABLE:
                resp = TextEmbedding.call(
                    model=self.config.text_embedding_model,
                    input=text
                )

                if resp.status_code == HTTPStatus.OK:
                    return resp.output['embeddings'][0]['embedding']

            # HTTP APIå›é€€
            url = f"{self.config.base_url}/embeddings"
            headers = {
                "Authorization": f"Bearer {self.config.api_key}",
                "Content-Type": "application/json"
            }
            data = {
                "model": self.config.text_embedding_model,
                "input": text
            }

            response = requests.post(url, headers=headers, json=data)
            if response.status_code == 200:
                result = response.json()
                return result['data'][0]['embedding']

        except Exception as e:
            logger.error(f"æ–‡æœ¬åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")

        # è¿”å›é›¶å‘é‡
        return [0.0] * 1024  # Qwen2.5-VL-Embeddingç»´åº¦

    async def rerank_documents(self, query: str, documents: List[str], top_n: int = 5) -> List[Dict]:
        """ä½¿ç”¨qwen3-rerankè¿›è¡Œæ–‡æ¡£é‡æ’åº"""
        logger.info(f"ä½¿ç”¨{self.config.rerank_model}è¿›è¡Œæ–‡æ¡£é‡æ’åº...")

        try:
            # å‡†å¤‡è¯·æ±‚
            url = f"https://dashscope.aliyuncs.com/api/v1/services/rerank/text-rerank/text-rerank"
            headers = {
                "Authorization": f"Bearer {self.config.api_key}",
                "Content-Type": "application/json"
            }

            data = {
                "model": self.config.rerank_model,
                "input": {
                    "query": query,
                    "documents": documents[:10]  # é™åˆ¶æ–‡æ¡£æ•°é‡
                },
                "parameters": {
                    "return_documents": True,
                    "top_n": top_n,
                    "instruct": "Given a query, retrieve relevant passages that answer the query."
                }
            }

            response = requests.post(url, headers=headers, json=data)

            if response.status_code == 200:
                result = response.json()
                if 'output' in result and 'results' in result['output']:
                    ranked_docs = result['output']['results']
                    logger.info(f"é‡æ’åºå®Œæˆï¼Œè¿”å› {len(ranked_docs)} ä¸ªæ–‡æ¡£")
                    return ranked_docs

            logger.error(f"é‡æ’åºå¤±è´¥: {response.status_code} - {response.text}")

        except Exception as e:
            logger.error(f"æ–‡æ¡£é‡æ’åºå¤±è´¥: {e}")

        # å›é€€æ–¹æ¡ˆï¼šè¿”å›åŸå§‹é¡ºåº
        return [{'index': i, 'document': doc, 'relevance_score': 1.0}
                for i, doc in enumerate(documents[:top_n])]

    async def extract_formulas(self, text: str) -> List[Dict]:
        """ä½¿ç”¨qwen-vl-plusæå–å’Œè§£é‡Šå…¬å¼"""
        logger.info("æå–æ•°å­¦å…¬å¼...")

        try:
            prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ‰€æœ‰æ•°å­¦å…¬å¼ï¼Œå¹¶è§£é‡Šå…¶å«ä¹‰ï¼š

{text_content[:1500]}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "formulas": [
    {{
      "formula": "å…¬å¼è¡¨è¾¾å¼",
      "explanation": "å…¬å¼å«ä¹‰è§£é‡Š",
      "variables": ["å˜é‡è¯´æ˜"],
      "context": "å…¬å¼ä¸Šä¸‹æ–‡"
    }}
  ]
}}"""

            messages = [
                {
                    "role": "user",
                    "content": prompt
                }
            ]

            response = Generation.call(
                model=self.config.text_model,
                messages=messages,
                temperature=0.1,
                max_tokens=1500
            )

            if response.status_code == 200:
                result_text = response.output.text
                try:
                    result = json.loads(result_text)
                    formulas = result.get('formulas', [])
                    logger.info(f"æå–åˆ° {len(formulas)} ä¸ªå…¬å¼")
                    return formulas
                except json.JSONDecodeError:
                    logger.error("å…¬å¼æå–ç»“æœè§£æå¤±è´¥")

        except Exception as e:
            logger.error(f"å…¬å¼æå–å¤±è´¥: {e}")

        return []

    async def analyze_images(self, image_data: bytes, context: str = "") -> Dict:
        """ä½¿ç”¨qwen-vl-ocr-lateståˆ†æå›¾ç‰‡"""
        logger.info("ä½¿ç”¨qwen-vl-ocr-lateståˆ†æå›¾ç‰‡...")

        try:
            # æ£€æŸ¥å›¾ç‰‡å¤§å°
            img_size_mb = len(image_data) / (1024 * 1024)
            if img_size_mb > 8:
                logger.error(f"Image too large ({img_size_mb:.2f}MB)")
                return {"description": "å›¾ç‰‡å¤ªå¤§ï¼Œæ— æ³•å¤„ç†"}

            # è½¬æ¢ä¸º Base64 æ ¼å¼
            img_base64 = base64.b64encode(image_data).decode()
            image_url = f"data:image/jpeg;base64,{img_base64}"

            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "image": image_url,  # ä½¿ç”¨ Base64 æ ¼å¼
                        },
                        {
                            "text": f"""è¯·åˆ†æè¿™å¼ å›¾ç‰‡å†…å®¹ï¼š
{context}

è¯·æè¿°ï¼š
1. å›¾ç‰‡çš„ä¸»è¦å†…å®¹
2. è¯†åˆ«çš„æ–‡å­—ä¿¡æ¯
3. å›¾ç‰‡ä¸­çš„æ•°æ®æˆ–å›¾è¡¨
4. å›¾ç‰‡çš„æ„ä¹‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š{{"description": "å›¾ç‰‡æè¿°", "text_content": "è¯†åˆ«çš„æ–‡å­—", "data_analysis": "æ•°æ®åˆ†æ", "significance": "å›¾ç‰‡æ„ä¹‰"}}"""
                        }
                    ]
                }
            ]

            response = MultiModalConversation.call(
                model=self.config.ocr_model,
                messages=messages,
                temperature=0.3,
                max_tokens=1000
            )

            if response.status_code == 200:
                result_text = response.output.text
                try:
                    return json.loads(result_text)
                except json.JSONDecodeError:
                    return {"description": result_text}

        except Exception as e:
            logger.error(f"å›¾ç‰‡åˆ†æå¤±è´¥: {e}")

        return {"description": "å›¾ç‰‡åˆ†æå¤±è´¥"}

    async def analyze_charts(self, text: str, page: int = 1) -> List[Dict]:
        """åˆ†æå›¾è¡¨æ•°æ®å’Œè¶‹åŠ¿"""
        logger.info("åˆ†æå›¾è¡¨æ•°æ®...")

        try:
            prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬ä¸­çš„å›¾è¡¨æ•°æ®ï¼š

{text[:1000]}

è¯·è¯†åˆ«ï¼š
1. å›¾è¡¨ç±»å‹ï¼ˆæŸ±çŠ¶å›¾ã€æŠ˜çº¿å›¾ã€é¥¼å›¾ç­‰ï¼‰
2. æ•°æ®è§„å¾‹
3. è¶‹åŠ¿åˆ†æ
4. ç»Ÿè®¡æ„ä¹‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "charts": [
    {{
      "type": "å›¾è¡¨ç±»å‹",
      "data_pattern": "æ•°æ®è§„å¾‹æè¿°",
      "trend": "è¶‹åŠ¿åˆ†æ",
      "statistical_significance": "ç»Ÿè®¡æ„ä¹‰",
      "insights": ["å…³é”®æ´å¯Ÿ"]
    }}
  ]
}}"""

            messages = [
                {
                    "role": "user",
                    "content": prompt
                }
            ]

            response = Generation.call(
                model=self.config.text_model,
                messages=messages,
                temperature=0.2,
                max_tokens=1200
            )

            if response.status_code == 200:
                result_text = response.output.text
                try:
                    result = json.loads(result_text)
                    charts = result.get('charts', [])
                    # æ·»åŠ é¡µç ä¿¡æ¯
                    for chart in charts:
                        chart['page'] = page
                    return charts
                except json.JSONDecodeError:
                    logger.error("å›¾è¡¨åˆ†æç»“æœè§£æå¤±è´¥")

        except Exception as e:
            logger.error(f"å›¾è¡¨åˆ†æå¤±è´¥: {e}")

        return []