"""
Markdownå’Œæ–‡æœ¬è§£æå™¨å¿«é€Ÿå¼€å§‹è„šæœ¬

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–°å¢çš„Markdownè§£æå™¨å’Œæ–‡æœ¬è§£æå™¨
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from app.services.parsers import MarkdownParser, TextParser
from app.services.parsers.register_parsers import get_default_registry


async def test_markdown_parser():
    """æµ‹è¯•Markdownè§£æå™¨"""
    print("="*60)
    print("æµ‹è¯• Markdown è§£æå™¨")
    print("="*60)

    # åˆ›å»ºä¸€ä¸ªç¤ºä¾‹Markdownæ–‡ä»¶
    sample_md = """---
title: æ— çº¿å……ç”µæŠ€æœ¯å‘å±•è¶‹åŠ¿
author: æŠ€æœ¯ç ”ç©¶éƒ¨
date: 2024-12-28
tags: [æ— çº¿å……ç”µ, æŠ€æœ¯è¶‹åŠ¿]
categories: [æŠ€æœ¯åˆ†æ]
---

# æ— çº¿å……ç”µæŠ€æœ¯å‘å±•è¶‹åŠ¿

## æ¦‚è¿°

æ— çº¿å……ç”µæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œé¢„è®¡æœªæ¥å‡ å¹´å°†è¿æ¥å¤§è§„æ¨¡å•†ä¸šåŒ–åº”ç”¨ã€‚

## æŠ€æœ¯å¯¹æ¯”

### ç£æ„Ÿåº”å……ç”µ

- **ä¼˜ç‚¹**: æŠ€æœ¯æˆç†Ÿï¼Œæˆæœ¬è¾ƒä½
- **ç¼ºç‚¹**: ä¼ è¾“è·ç¦»çŸ­

### ç£å…±æŒ¯å……ç”µ

- **ä¼˜ç‚¹**: ä¼ è¾“è·ç¦»è¾ƒè¿œ
- **ç¼ºç‚¹**: æˆæœ¬è¾ƒé«˜

## æŠ€æœ¯å‚æ•°

| å‚æ•° | ç£æ„Ÿåº” | ç£å…±æŒ¯ |
|------|--------|--------|
| ä¼ è¾“è·ç¦» | < 5mm | < 50mm |
| æ•ˆç‡ | 85% | 75% |
| æˆæœ¬ | ä½ | ä¸­ |

```python
# ç¤ºä¾‹ä»£ç 
def wireless_charge():
    return "charging..."
```

## æ€»ç»“

æ— çº¿å……ç”µæŠ€æœ¯å°†åœ¨æœªæ¥æ™ºèƒ½å®¶å±…é¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚
"""

    # ä¿å­˜ç¤ºä¾‹æ–‡ä»¶
    test_file = Path("test_document.md")
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(sample_md)

    # åˆ›å»ºè§£æå™¨
    parser = MarkdownParser({
        'extract_metadata': True,
        'preserve_html': False
    })

    # è§£ææ–‡ä»¶
    result = await parser.parse(str(test_file))

    print(f"\nâœ“ è§£ææˆåŠŸ: {result.success}")
    print(f"âœ“ è§£ææ—¶é—´: {result.parse_time:.3f}ç§’")
    print(f"âœ“ å†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")

    # æ˜¾ç¤ºæå–çš„å…ƒæ•°æ®
    metadata = result.metadata.get('metadata', {})
    print(f"\nğŸ“‹ æ–‡æ¡£å…ƒæ•°æ®:")
    print(f"  æ ‡é¢˜: {metadata.get('title')}")
    print(f"  ä½œè€…: {metadata.get('author')}")
    print(f"  æ—¥æœŸ: {metadata.get('date')}")
    print(f"  æ ‡ç­¾: {metadata.get('tags')}")
    print(f"  åˆ†ç±»: {metadata.get('categories')}")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = result.metadata.get('statistics', {})
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ ‡é¢˜æ•°: {stats.get('heading_count')}")
    print(f"  ä»£ç å—æ•°: {stats.get('code_block_count')}")
    print(f"  è¡¨æ ¼æ•°: {stats.get('table_count')}")
    print(f"  é“¾æ¥æ•°: {stats.get('link_count')}")
    print(f"  è¡Œæ•°: {stats.get('line_count')}")

    # æ˜¾ç¤ºæ ‡é¢˜ç»“æ„
    heading_structure = result.metadata.get('heading_structure', [])
    print(f"\nğŸ“‘ æ ‡é¢˜ç»“æ„:")
    for heading in heading_structure:
        indent = "  " * heading['level']
        print(f"{indent}- {heading['title']}")

    # æå–è¡¨æ ¼
    tables = parser.extract_tables(result.content)
    print(f"\nğŸ“Š æå–åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
    for i, table in enumerate(tables):
        print(f"  è¡¨æ ¼ {i+1}: {table['column_count']} åˆ— x {table['row_count']} è¡Œ")
        print(f"    è¡¨å¤´: {table['headers']}")

    # æå–ä»£ç å—
    code_blocks = parser.extract_code_blocks(result.content)
    print(f"\nğŸ’» æå–åˆ° {len(code_blocks)} ä¸ªä»£ç å—")
    for i, block in enumerate(code_blocks):
        print(f"  ä»£ç å— {i+1}: {block['language']} ({block['length']} å­—ç¬¦)")

    # æ™ºèƒ½åˆ†å—
    chunks = parser.chunk_content(
        result.content,
        chunk_size=500,
        chunk_overlap=50
    )

    print(f"\nğŸ”ª æ™ºèƒ½åˆ†å—: åˆ†ä¸º {len(chunks)} ä¸ªå—")
    for i, chunk in enumerate(chunks):
        title_path = chunk.metadata.get('title_path', 'æ ¹çº§åˆ«')
        print(f"  å— {i+1}: {title_path[:40]}... ({len(chunk.content)} å­—ç¬¦)")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    test_file.unlink()


async def test_text_parser():
    """æµ‹è¯•æ–‡æœ¬è§£æå™¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•æ–‡æœ¬è§£æå™¨")
    print("="*60)

    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    sample_txt = """æ— çº¿å……ç”µæŠ€æœ¯å‘å±•è¶‹åŠ¿

æ— çº¿å……ç”µæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œé¢„è®¡æœªæ¥å‡ å¹´å°†è¿æ¥å¤§è§„æ¨¡å•†ä¸šåŒ–åº”ç”¨ã€‚

æŠ€æœ¯å¯¹æ¯”ï¼š

ç£æ„Ÿåº”å……ç”µæŠ€æœ¯æˆç†Ÿï¼Œæˆæœ¬è¾ƒä½ï¼Œä½†ä¼ è¾“è·ç¦»çŸ­ã€‚ç£å…±æŒ¯å……ç”µä¼ è¾“è·ç¦»è¾ƒè¿œï¼Œä½†æˆæœ¬è¾ƒé«˜ã€‚

å¸‚åœºé¢„æµ‹ï¼š
2024å¹´å¸‚åœºè§„æ¨¡é¢„è®¡è¾¾åˆ°100äº¿ç¾å…ƒ
2025å¹´é¢„è®¡å¢é•¿åˆ°150äº¿ç¾å…ƒ
2026å¹´é¢„è®¡çªç ´200äº¿ç¾å…ƒ

ä¸»è¦åº”ç”¨é¢†åŸŸåŒ…æ‹¬ï¼šæ™ºèƒ½æ‰‹æœºã€ç”µåŠ¨æ±½è½¦ã€æ™ºèƒ½å®¶å±…è®¾å¤‡ç­‰ã€‚
"""

    # ä¿å­˜ç¤ºä¾‹æ–‡ä»¶
    test_file = Path("test_document.txt")
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(sample_txt)

    # åˆ›å»ºè§£æå™¨
    parser = TextParser({
        'detect_language': True,
        'chunk_by_paragraph': True
    })

    # è§£ææ–‡ä»¶
    result = await parser.parse(str(test_file))

    print(f"\nâœ“ è§£ææˆåŠŸ: {result.success}")
    print(f"âœ“ ç¼–ç : {result.encoding}")
    print(f"âœ“ è§£ææ—¶é—´: {result.parse_time:.3f}ç§’")
    print(f"âœ“ å†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = result.metadata.get('statistics', {})
    print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"  è¡Œæ•°: {stats.get('line_count')}")
    print(f"  è¯æ•°: {stats.get('word_count')}")
    print(f"  æ®µè½æ•°: {stats.get('paragraph_count')}")
    print(f"  å­—ç¬¦æ•°: {stats.get('char_count')}")

    # æ˜¾ç¤ºæ£€æµ‹ä¿¡æ¯
    print(f"\nğŸ” è‡ªåŠ¨æ£€æµ‹:")
    print(f"  è¯­è¨€: {result.metadata.get('detected_language', 'N/A')}")
    print(f"  å†…å®¹ç±»å‹: {result.metadata.get('content_type_hint', 'N/A')}")

    # æŒ‰æ®µè½åˆ†å—
    chunks = parser.chunk_content(
        result.content,
        chunk_size=200,
        chunk_overlap=20
    )

    print(f"\nğŸ”ª æ®µè½åˆ†å—: åˆ†ä¸º {len(chunks)} ä¸ªå—")
    for i, chunk in enumerate(chunks):
        para_count = chunk.metadata.get('paragraph_count', 0)
        method = chunk.metadata.get('chunking_method', 'N/A')
        print(f"  å— {i+1}: {para_count} ä¸ªæ®µè½ ({method})")
        print(f"    é¢„è§ˆ: {chunk.content[:60]}...")

    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    test_file.unlink()


async def test_parser_registry():
    """æµ‹è¯•è§£æå™¨æ³¨å†Œè¡¨"""
    print("\n" + "="*60)
    print("æµ‹è¯•è§£æå™¨æ³¨å†Œè¡¨")
    print("="*60)

    # è·å–æ³¨å†Œè¡¨
    registry = get_default_registry()

    print(f"\nğŸ“Š æ³¨å†Œè¡¨ç»Ÿè®¡:")
    print(f"  æ€»è§£æå™¨æ•°: {registry.get_parser_count()}")
    print(f"  æ”¯æŒçš„æ‰©å±•åæ•°: {registry.get_extension_count()}")

    print(f"\nğŸ“ æ”¯æŒçš„æ–‡ä»¶ç±»å‹:")
    for ext_info in registry.list_extensions():
        parsers = ", ".join(ext_info['parser_names'])
        print(f"  {ext_info['extension']:15} -> {parsers}")

    # æµ‹è¯•è‡ªåŠ¨é€‰æ‹©è§£æå™¨
    print(f"\nğŸ” æµ‹è¯•è‡ªåŠ¨é€‰æ‹©è§£æå™¨:")
    test_files = [
        "test.md",
        "test.txt",
        "test.docx",
        "test.xlsx",
        "test.pdf"
    ]

    for file_path in test_files:
        ext = Path(file_path).suffix
        parser = registry.get_parser_by_extension(ext)
        if parser:
            print(f"  {file_path:15} -> {parser.parser_name}")
        else:
            print(f"  {file_path:15} -> æœªæ‰¾åˆ°è§£æå™¨")


async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    try:
        await test_markdown_parser()
        await test_text_parser()
        await test_parser_registry()

        print("\n" + "="*60)
        print("âœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("="*60)

    except Exception as e:
        print(f"\nâœ— æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
