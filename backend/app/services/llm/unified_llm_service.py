"""
ç»Ÿä¸€LLMæœåŠ¡å±‚ - è§£å†³LLMæœåŠ¡é‡å¤å’Œåˆ†æ•£é—®é¢˜

æ•´åˆæ‰€æœ‰LLMè°ƒç”¨åˆ°ä¸€ä¸ªç»Ÿä¸€çš„æœåŠ¡å±‚ï¼Œæä¾›ï¼š
1. ç»Ÿä¸€çš„è°ƒç”¨æ¥å£
2. ç»Ÿä¸€çš„é”™è¯¯å¤„ç†
3. ç»Ÿä¸€çš„æ—¥å¿—è®°å½•
4. æ‰¹é‡è°ƒç”¨ä¼˜åŒ–
5. Promptæ¨¡æ¿ç®¡ç†
"""

import asyncio
from enum import Enum
from dataclasses import dataclass
from app.core.structured_logging import get_structured_logger
from datetime import datetime

logger = get_structured_logger(__name__)

class LLMModel(str, Enum):
    """æ”¯æŒçš„LLMæ¨¡å‹"""
    DEEPSEEK_CHAT = "deepseek-chat"
    DEEPSEEK_REASONER = "deepseek-reasoner"
    GLM_4_PLUS = "glm-4-plus"
    GLM_4_05_PLUS = "glm-4-0528-plus"  # GLM-4.7
    GLM_4V = "glm-4v"  # GLM-4.6V
    QWEN_VL_MAX = "qwen-vl-max"
    QWEN_VL_PLUS = "qwen-vl-plus"
    QWEN_VL_OCR = "qwen-vl-ocr"

@dataclass
class LLMResponse:
    """LLMå“åº”ç»Ÿä¸€æ ¼å¼"""
    content: str
    model: str
    usage: Optional[Dict[str, int]] = None
    latency_ms: Optional[int] = None
    success: bool = True
    error: Optional[str] = None

@dataclass
class LLMRequest:
    """LLMè¯·æ±‚ç»Ÿä¸€æ ¼å¼"""
    prompt: str
    model: LLMModel = LLMModel.DEEPSEEK_CHAT
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    top_p: float = 0.9
    stream: bool = False
    metadata: Optional[Dict[str, Any]] = None

class LLMError(Exception):
    """LLMè°ƒç”¨é”™è¯¯åŸºç±»"""
    def __init__(self, message: str, model: str, cause: Optional[Exception] = None):
        self.message = message
        self.model = model
        self.cause = cause
        super().__init__(f"[{model}] {message}")

class UnifiedLLMService:
    """
    ç»Ÿä¸€LLMæœåŠ¡

    æ•´åˆæ‰€æœ‰åˆ†æ•£çš„LLMè°ƒç”¨ï¼ŒåŒ…æ‹¬ï¼š
    - llm_service.py (DeepSeek)
    - financial_llm_service.py (é‡‘èä¸“ç”¨)
    - rag_glm_service.py (GLM)
    - ocr_service.py (Qwen-VL)
    """

    def __init__(self):
        self._service_cache = {}
        self._prompt_templates = {}
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰LLMæœåŠ¡"""
        if self._initialized:
            return

        logger.info("åˆå§‹åŒ–ç»Ÿä¸€LLMæœåŠ¡...")

        try:
            # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
            from app.services.llm.unified_llm_service import llm_service
            self._service_cache['deepseek'] = llm_service
            logger.info("âœ… DeepSeekæœåŠ¡å·²åŠ è½½")

        except Exception as e:
            logger.warning(f"âš ï¸ DeepSeekæœåŠ¡åŠ è½½å¤±è´¥: {e}")
            self._service_cache['deepseek'] = None

        try:
            from app.services.rag_glm_service import RAGGLMService
            self._service_cache['glm'] = RAGGLMService()
            await self._service_cache['glm'].initialize()
            logger.info("âœ… GLMæœåŠ¡å·²åŠ è½½")

        except Exception as e:
            logger.warning(f"âš ï¸ GLMæœåŠ¡åŠ è½½å¤±è´¥: {e}")
            self._service_cache['glm'] = None

        try:
            from app.services.ocr_service import OCRService
            self._service_cache['qwen_vl'] = OCRService()
            logger.info("âœ… Qwen-VLæœåŠ¡å·²åŠ è½½")

        except Exception as e:
            logger.warning(f"âš ï¸ Qwen-VLæœåŠ¡åŠ è½½å¤±è´¥: {e}")
            self._service_cache['qwen_vl'] = None

        # åŠ è½½Promptæ¨¡æ¿
        self._load_prompt_templates()

        self._initialized = True
        logger.info("âœ… ç»Ÿä¸€LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    def _load_prompt_templates(self):
        """åŠ è½½Promptæ¨¡æ¿"""
        self._prompt_templates = {
            'entity_extraction': """è¯·ä»ä»¥ä¸‹é‡‘èæ–‡æœ¬ä¸­æŠ½å–é‡è¦å®ä½“ï¼ŒåŒ…æ‹¬ï¼š
1. å…¬å¸åç§°
2. è‚¡ç¥¨ä»£ç 
3. è´¢åŠ¡æŒ‡æ ‡ï¼ˆæ”¶å…¥ã€åˆ©æ¶¦ã€èµ„äº§ã€è´Ÿå€ºç­‰ï¼‰
4. å…³é”®äººç‰©
5. é‡è¦æ—¥æœŸ
6. é‡‘é¢å’Œç™¾åˆ†æ¯”

æ–‡æœ¬ï¼š
{content}

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œåªè¿”å›é«˜ç½®ä¿¡åº¦çš„å®ä½“ï¼ˆconfidence > 0.7ï¼‰ï¼š
{{
    "entities": [
        {{"text": "å®ä½“æ–‡æœ¬", "type": "å®ä½“ç±»å‹", "confidence": 0.9}}
    ]
}}""",

            'document_summary': """è¯·ä¸ºä»¥ä¸‹é‡‘èæ–‡æ¡£ç”Ÿæˆæ‘˜è¦ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸»è¦è§‚ç‚¹ï¼ˆ3-5æ¡ï¼‰
2. æ ¸å¿ƒæ•°æ®
3. æŠ•èµ„å»ºè®®ï¼ˆå¦‚æœ‰ï¼‰

æ–‡æ¡£å†…å®¹ï¼š
{content}

è¯·ä»¥Markdownæ ¼å¼è¿”å›æ‘˜è¦ã€‚""",

            'qa_rag': """åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æä¾›å‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ï¼Œå¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜ã€‚""",

            'markdown_fusion': """è¯·èåˆä»¥ä¸‹ä¸¤éƒ¨åˆ†å†…å®¹ï¼Œç”Ÿæˆç»Ÿä¸€çš„æ–‡æ¡£ï¼š

1. PDFè§£æå†…å®¹ï¼š
{pdf_content}

2. Markdownè¡¥å……å†…å®¹ï¼š
{markdown_content}

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„ã€ç»“æ„åŒ–çš„Markdownæ–‡æ¡£ã€‚"""
        }

    # ========================================================================
    # æ ¸å¿ƒAPI
    # ========================================================================

    async def chat(
        self,
        prompt: str,
        model: LLMModel = LLMModel.DEEPSEEK_CHAT,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> LLMResponse:
        """
        ç»Ÿä¸€çš„èŠå¤©æ¥å£

        Args:
            prompt: æç¤ºè¯
            model: LLMæ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponse
        """
        start_time = datetime.now()

        try:
            # è·¯ç”±åˆ°å¯¹åº”çš„æœåŠ¡
            if model in [LLMModel.DEEPSEEK_CHAT, LLMModel.DEEPSEEK_REASONER]:
                response = await self._chat_deepseek(prompt, model, temperature, max_tokens, **kwargs)
            elif model in [LLMModel.GLM_4_PLUS, LLMModel.GLM_4_05_PLUS, LLMModel.GLM_4V]:
                response = await self._chat_glm(prompt, model, temperature, max_tokens, **kwargs)
            elif model in [LLMModel.QWEN_VL_MAX, LLMModel.QWEN_VL_PLUS, LLMModel.QWEN_VL_OCR]:
                response = await self._chat_qwen_vl(prompt, model, temperature, max_tokens, **kwargs)
            else:
                raise LLMError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}", model.value)

            latency_ms = int((datetime.now() - start_time).total_seconds() * 1000)

            logger.info(f"âœ… LLMè°ƒç”¨æˆåŠŸ: model={model.value}, latency={latency_ms}ms, "
                       f"tokens={response.usage.get('total_tokens', 'N/A') if response.usage else 'N/A'}")

            return LLMResponse(
                content=response.content,
                model=model.value,
                usage=response.usage,
                latency_ms=latency_ms,
                success=True
            )

        except Exception as e:
            latency_ms = int((datetime.now() - start_time).total_seconds() * 1000)

            logger.error(f"âŒ LLMè°ƒç”¨å¤±è´¥: model={model.value}, error={str(e)}, latency={latency_ms}ms")

            return LLMResponse(
                content="",
                model=model.value,
                latency_ms=latency_ms,
                success=False,
                error=str(e)
            )

    async def chat_with_template(
        self,
        template_name: str,
        template_vars: Dict[str, Any],
        model: LLMModel = LLMModel.DEEPSEEK_CHAT,
        **kwargs
    ) -> LLMResponse:
        """
        ä½¿ç”¨Promptæ¨¡æ¿è¿›è¡Œå¯¹è¯

        Args:
            template_name: æ¨¡æ¿åç§°
            template_vars: æ¨¡æ¿å˜é‡
            model: LLMæ¨¡å‹
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            LLMResponse
        """
        if template_name not in self._prompt_templates:
            raise ValueError(f"æœªæ‰¾åˆ°æ¨¡æ¿: {template_name}")

        prompt = self._prompt_templates[template_name].format(**template_vars)
        return await self.chat(prompt, model, **kwargs)

    async def batch_chat(
        self,
        prompts: List[str],
        model: LLMModel = LLMModel.DEEPSEEK_CHAT,
        temperature: float = 0.7,
        max_concurrent: int = 5,
        **kwargs
    ) -> List[LLMResponse]:
        """
        æ‰¹é‡èŠå¤©æ¥å£ï¼ˆå¹¶å‘æ‰§è¡Œï¼Œå‡å°‘æ€»è€—æ—¶ï¼‰

        Args:
            prompts: æç¤ºè¯åˆ—è¡¨
            model: LLMæ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            List[LLMResponse]
        """
        logger.info(f"ğŸš€ æ‰¹é‡LLMè°ƒç”¨: count={len(prompts)}, model={model.value}, max_concurrent={max_concurrent}")

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def chat_with_limit(prompt: str, index: int):
            async with semaphore:
                response = await self.chat(prompt, model, temperature, **kwargs)
                return index, response

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
        tasks = [chat_with_limit(prompt, i) for i, prompt in enumerate(prompts)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æŒ‰åŸå§‹é¡ºåºæ’åº
        sorted_responses = [None] * len(prompts)
        success_count = 0

        for result in results:
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡è°ƒç”¨å¼‚å¸¸: {result}")
                continue

            index, response = result
            sorted_responses[index] = response
            if response.success:
                success_count += 1

        logger.info(f"âœ… æ‰¹é‡LLMè°ƒç”¨å®Œæˆ: success={success_count}/{len(prompts)}")

        return sorted_responses

    # ========================================================================
    # ç§æœ‰æ–¹æ³• - è·¯ç”±åˆ°å…·ä½“æœåŠ¡
    # ========================================================================

    async def _chat_deepseek(
        self,
        prompt: str,
        model: LLMModel,
        temperature: float,
        max_tokens: Optional[int],
        **kwargs
    ) -> LLMResponse:
        """DeepSeekèŠå¤©"""
        service = self._service_cache.get('deepseek')
        if not service:
            raise LLMError("DeepSeekæœåŠ¡æœªåˆå§‹åŒ–", model.value)

        # è°ƒç”¨åŸå§‹æœåŠ¡
        response = await service.simple_chat(
            prompt,
            temperature=temperature,
            max_tokens=max_tokens
        )

        return LLMResponse(
            content=response,
            model=model.value
        )

    async def _chat_glm(
        self,
        prompt: str,
        model: LLMModel,
        temperature: float,
        max_tokens: Optional[int],
        **kwargs
    ) -> LLMResponse:
        """GLMèŠå¤©"""
        service = self._service_cache.get('glm')
        if not service:
            raise LLMError("GLMæœåŠ¡æœªåˆå§‹åŒ–", model.value)

        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ–¹æ³•
        if model == LLMModel.GLM_4V:
            # è§†è§‰æ¨¡å‹
            response = await service.analyze_image(
                prompt,
                kwargs.get('image')
            )
        else:
            # æ–‡æœ¬æ¨¡å‹
            response = await service.generate(
                prompt,
                temperature=temperature
            )

        return LLMResponse(
            content=response,
            model=model.value
        )

    async def _chat_qwen_vl(
        self,
        prompt: str,
        model: LLMModel,
        temperature: float,
        max_tokens: Optional[int],
        **kwargs
    ) -> LLMResponse:
        """Qwen-VLèŠå¤©"""
        service = self._service_cache.get('qwen_vl')
        if not service:
            raise LLMError("Qwen-VLæœåŠ¡æœªåˆå§‹åŒ–", model.value)

        # è°ƒç”¨OCR/è§†è§‰æœåŠ¡
        response = await service.analyze_document(
            kwargs.get('file_content'),
            kwargs.get('filename')
        )

        return LLMResponse(
            content=response.get('text', ''),
            model=model.value
        )

# å…¨å±€å•ä¾‹
_unified_llm_service: Optional[UnifiedLLMService] = None

def get_unified_llm_service() -> UnifiedLLMService:
    """è·å–ç»Ÿä¸€LLMæœåŠ¡å•ä¾‹"""
    global _unified_llm_service
    if _unified_llm_service is None:
        _unified_llm_service = UnifiedLLMService()
    return _unified_llm_service

async def get_unified_llm_service_initialized() -> UnifiedLLMService:
    """è·å–å·²åˆå§‹åŒ–çš„ç»Ÿä¸€LLMæœåŠ¡"""
    service = get_unified_llm_service()
    if not service._initialized:
        await service.initialize()
    return service
