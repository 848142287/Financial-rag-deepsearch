"""
ç»Ÿä¸€æœåŠ¡æ•´åˆå™¨ - æ•´åˆæ‰€æœ‰åˆ†æ•£çš„æœåŠ¡
é‡æ–°å®ç°ä»¥ä¿®å¤ç¼ºå¤±çš„æ–‡ä»¶
"""

import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
from dataclasses import dataclass
from app.core.structured_logging import get_structured_logger

logger = get_structured_logger(__name__)

@dataclass
class ServiceConfig:
    """æœåŠ¡é…ç½®"""
    enable_multimodal: bool = True
    enable_entity_extraction: bool = True
    enable_knowledge_graph: bool = True
    async_entity_extraction: bool = True
    async_knowledge_graph: bool = True
    enable_vector_storage: bool = True
    enable_mysql_storage: bool = True
    enable_chunking: bool = True

class CoreServiceIntegrator:
    """ç»Ÿä¸€æœåŠ¡æ•´åˆå™¨"""

    def __init__(self, config: Optional[ServiceConfig] = None):
        self.config = config or ServiceConfig()
        self._services = {}
        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        if self._initialized:
            return

        logger.info("ğŸš€ åˆå§‹åŒ–ç»Ÿä¸€æœåŠ¡æ•´åˆå™¨...")

        # å¯¼å…¥æœåŠ¡
        from app.services.minio_service import MinIOService
        from app.services.enhanced_milvus_service import EnhancedMilvusService  # ä½¿ç”¨å¢å¼ºç‰ˆMilvusæœåŠ¡
        # from app.services.neo4j_service import Neo4jService  # Neo4jæœåŠ¡æš‚æ—¶ç¦ç”¨ï¼ˆæ¨¡å—ä¸å­˜åœ¨ï¼‰
        from app.services.embeddings.unified_embedding_service import get_embedding_service
        # from app.services.unified_document_service import UnifiedDocumentService  # æ¨¡å—ä¸å­˜åœ¨ï¼Œå·²ç¦ç”¨
        # from app.services.knowledge.entity_extractor import FinancialEntityExtractor  # å®ä½“æå–æš‚æ—¶ç¦ç”¨

        # åˆå§‹åŒ–MinIO
        self._services['minio'] = MinIOService()
        # MinIOServiceæ²¡æœ‰initializeæ–¹æ³•ï¼Œä¸éœ€è¦è°ƒç”¨
        logger.info("âœ… MinIOæœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–Milvus (ä½¿ç”¨å¢å¼ºç‰ˆ)
        self._services['milvus'] = EnhancedMilvusService()
        # EnhancedMilvusServiceæ²¡æœ‰initializeæ–¹æ³•ï¼Œä¸éœ€è¦è°ƒç”¨
        logger.info("âœ… MilvusæœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–Neo4j (æš‚æ—¶ç¦ç”¨)
        # if self.config.enable_knowledge_graph:
        #     self._services['neo4j'] = Neo4jService()
        #     await self._services['neo4j'].initialize()
        #     logger.info("âœ… Neo4jæœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–EmbeddingæœåŠ¡
        self._services['embedding'] = get_embedding_service()
        logger.info("âœ… EmbeddingæœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–æ–‡æ¡£æœåŠ¡ (æš‚æ—¶ç¦ç”¨ - æ¨¡å—ä¸å­˜åœ¨)
        # self._services['document'] = UnifiedDocumentService()
        # logger.info("âœ… æ–‡æ¡£æœåŠ¡å·²åˆå§‹åŒ–")

        # åˆå§‹åŒ–å®ä½“æå–å™¨ (æš‚æ—¶ç¦ç”¨)
        # if self.config.enable_entity_extraction:
        #     self._services['entity_extractor'] = FinancialEntityExtractor()
        #     logger.info("âœ… å®ä½“æå–å™¨å·²åˆå§‹åŒ–")

        self._initialized = True
        logger.info("âœ… ç»Ÿä¸€æœåŠ¡æ•´åˆå™¨åˆå§‹åŒ–å®Œæˆ")

    async def process_document(
        self,
        file_content: bytes,
        filename: str,
        document_id: str
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆå®Œæ•´æµæ°´çº¿ï¼‰

        Returns:
            å¤„ç†ç»“æœ
        """
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {filename}")

        result = {
            'document_id': document_id,
            'filename': filename,
            'processing_start': datetime.now().isoformat(),
            'stages': {},
            'success': False
        }

        try:
            # é˜¶æ®µ1: è§£ææ–‡æ¡£
            logger.info("ğŸ“„ é˜¶æ®µ1: æ–‡æ¡£è§£æ...")
            parse_result = await self._parse_document(file_content, filename, document_id)
            result['stages']['parsing'] = parse_result

            if not parse_result.get('success'):
                return result

            # é˜¶æ®µ2: æ–‡æœ¬åˆ†å—
            logger.info("ğŸ”ª é˜¶æ®µ2: æ–‡æœ¬åˆ†å—...")
            chunks = await self._create_chunks(parse_result, document_id)
            result['stages']['chunking'] = {'status': 'completed', 'chunk_count': len(chunks)}

            # é˜¶æ®µ3: å‘é‡ç”Ÿæˆ
            logger.info("ğŸ”¢ é˜¶æ®µ3: å‘é‡ç”Ÿæˆ...")
            chunks_with_embeddings = await self._generate_embeddings(chunks, document_id)
            result['stages']['embeddings'] = {'status': 'completed', 'embedding_count': len(chunks_with_embeddings)}

            # é˜¶æ®µ4: å­˜å‚¨åˆ°MySQLå’ŒMilvus
            logger.info("ğŸ’¾ é˜¶æ®µ4: å­˜å‚¨æ•°æ®...")
            await self._store_document_data(document_id, parse_result, chunks_with_embeddings)
            result['stages']['storage'] = {'status': 'completed'}

            # ä¿å­˜chunksç”¨äºåå°enrichment
            result['chunks'] = chunks_with_embeddings

            # é˜¶æ®µ5: å®ä½“æå–ï¼ˆå¼‚æ­¥åå°ï¼‰
            result['stages']['entities'] = {
                'status': 'async_pending',
                'reason': 'å®ä½“æå–å·²è½¬ä¸ºåå°å¼‚æ­¥ä»»åŠ¡'
            }

            # é˜¶æ®µ6: çŸ¥è¯†å›¾è°±ï¼ˆå¼‚æ­¥åå°ï¼‰
            result['stages']['knowledge_graph'] = {
                'status': 'async_pending',
                'reason': 'çŸ¥è¯†å›¾è°±å·²è½¬ä¸ºåå°å¼‚æ­¥ä»»åŠ¡'
            }

            result['success'] = True
            result['processing_end'] = datetime.now().isoformat()

            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {document_id}")
            return result

        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {e}")
            result['error'] = str(e)
            result['success'] = False
            return result

    async def _parse_document(self, file_content: bytes, filename: str, document_id: str) -> Dict[str, Any]:
        """è§£ææ–‡æ¡£"""
        from app.services.advanced_pdf_parser import AdvancedPDFParser

        parser = AdvancedPDFParser()

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as f:
            f.write(file_content)
            temp_path = f.name

        try:
            # è§£æPDF
            parse_result = await parser.parse_pdf_async(temp_path)

            if not parse_result.success:
                return {
                    'success': False,
                    'error': parse_result.error or 'PDFè§£æå¤±è´¥',
                    'text': '',
                    'markdown': ''
                }

            return {
                'success': True,
                'text': parse_result.text,
                'markdown': parse_result.markdown,
                'tables': parse_result.tables,
                'images': parse_result.images,
                'formulas': parse_result.formulas,
                'charts': parse_result.charts,
                'metadata': parse_result.metadata,
                'parsing_stats': parse_result.parsing_stats,
                'method': parse_result.method
            }
        finally:
            import os
            if os.path.exists(temp_path):
                os.remove(temp_path)

    async def _create_chunks(self, parse_result: Dict, document_id: str) -> List[Dict]:
        """åˆ›å»ºæ–‡æ¡£å—"""
        text = parse_result.get('text', '')
        markdown = parse_result.get('markdown', '')
        content = markdown or text

        # ç®€å•åˆ†å—ç­–ç•¥
        chunk_size = 1000
        chunks = []

        for i in range(0, len(content), chunk_size):
            chunk_text = content[i:i + chunk_size]
            chunks.append({
                'chunk_id': f"{document_id}_{i // chunk_size}",
                'document_id': document_id,
                'chunk_index': i // chunk_size,
                'content': chunk_text,
                'metadata': {
                    'source': 'pdf_parser',
                    'chunk_size': chunk_size
                }
            })

        return chunks

    async def _generate_embeddings(self, chunks: List[Dict], document_id: str) -> List[Dict]:
        """ç”Ÿæˆå‘é‡"""
        chunks_with_embeddings = []
        semaphore = asyncio.Semaphore(1)  # é™åˆ¶å¹¶å‘

        async def generate_embedding(chunk: Dict):
            async with semaphore:
                embedding = await self._services['embedding'].embed(chunk['content'])
                chunk['embedding'] = embedding
                chunk['embedding_id'] = None  # Will be set after storage
                return chunk

        tasks = [generate_embedding(chunk) for chunk in chunks]
        chunks_with_embeddings = await asyncio.gather(*tasks)

        return chunks_with_embeddings

    async def _store_document_data(self, document_id: str, parse_result: Dict, chunks: List[Dict]):
        """å­˜å‚¨æ–‡æ¡£æ•°æ®åˆ°MySQLå’ŒMilvus"""
        from app.core.database import SessionLocal
        from sqlalchemy import text as sql_text

        db = SessionLocal()
        try:
            # æ›´æ–°æ–‡æ¡£çš„parsed_content - å­˜å‚¨ä¸ºJSONå¯¹è±¡
            markdown = parse_result.get('markdown', '')
            parsed_text = parse_result.get('text', '')

            # æ„å»ºJSONæ ¼å¼çš„parsed_content
            import json
            content_json = json.dumps({
                'text': parsed_text,
                'markdown': markdown
            }, ensure_ascii=False)

            db.execute(
                sql_text("UPDATE documents SET parsed_content=:content WHERE id=:id"),
                {'content': content_json, 'id': document_id}
            )
            db.commit()

            # å­˜å‚¨chunksåˆ°MySQLå’ŒMilvus
            embedding_ids = []
            milvus_chunks = []

            for chunk in chunks:
                # ä¿å­˜åˆ°MySQL
                chunk_record = DocumentChunk(
                    document_id=int(document_id),
                    chunk_index=chunk['chunk_index'],
                    content=chunk['content'],
                    metadata=chunk.get('metadata', {})
                )
                db.add(chunk_record)
                db.flush()

                # å‡†å¤‡Milvusæ•°æ®
                if chunk.get('embedding') is not None:
                    milvus_chunks.append({
                        'chunk_id': chunk['chunk_id'],
                        'content': chunk['content'],
                        'embedding': chunk['embedding'].tolist() if hasattr(chunk['embedding'], 'tolist') else chunk['embedding'],
                        'chunk_index': chunk['chunk_index'],
                        'page_number': chunk.get('metadata', {}).get('page', 0),
                        'chunk_type': 'text'
                    })

            # æ‰¹é‡æ’å…¥åˆ°Milvus
            if milvus_chunks:
                try:
                    milvus_ids = await self._services['milvus'].insert_chunks_with_full_metadata(
                        document_id=str(document_id),
                        chunks_data=milvus_chunks
                    )
                    embedding_ids = [str(mid) for mid in milvus_ids]
                    logger.info(f"âœ… æ’å…¥äº† {len(milvus_ids)} ä¸ªå‘é‡åˆ°Milvus")
                except Exception as e:
                    logger.warning(f"Milvuså­˜å‚¨å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")

            db.commit()
            logger.info(f"âœ… å­˜å‚¨äº† {len(chunks)} ä¸ªæ–‡æ¡£å—")

        finally:
            db.close()

    async def search_documents(self, query: str, top_k: int = 10, filters: Optional[Dict] = None) -> List[Dict]:
        """æœç´¢æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await self._services['embedding'].get_embedding(query)

        # å‘é‡æœç´¢
        results = await self._services['milvus'].search_vectors(
            collection_name='document_chunks',
            query_vector=query_embedding,
            limit=top_k
        )

        return results

    async def get_service_status(self) -> Dict[str, Any]:
        """è·å–æœåŠ¡çŠ¶æ€"""
        return {
            'services': {
                'minio': 'healthy' if 'minio' in self._services else 'disabled',
                'milvus': 'healthy' if 'milvus' in self._services else 'disabled',
                'neo4j': 'healthy' if 'neo4j' in self._services else 'disabled',
                'embedding': 'healthy' if 'embedding' in self._services else 'disabled',
            },
            'initialized': self._initialized
        }

    def get_config_summary(self) -> Dict[str, Any]:
        """è·å–é…ç½®æ‘˜è¦"""
        return {
            'enable_entity_extraction': self.config.enable_entity_extraction,
            'enable_knowledge_graph': self.config.enable_knowledge_graph,
            'async_entity_extraction': self.config.async_entity_extraction,
            'async_knowledge_graph': self.config.async_knowledge_graph,
        }

# å…¨å±€å®ä¾‹
_integrator: Optional[CoreServiceIntegrator] = None

def get_service_integrator() -> CoreServiceIntegrator:
    """è·å–æœåŠ¡æ•´åˆå™¨å®ä¾‹"""
    global _integrator
    if _integrator is None:
        _integrator = CoreServiceIntegrator()
    return _integrator
