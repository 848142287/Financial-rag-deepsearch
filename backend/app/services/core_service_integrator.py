"""
æ ¸å¿ƒæœåŠ¡æ•´åˆå™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰åˆ†æ•£çš„æœåŠ¡åŠŸèƒ½
æ•´åˆæ–‡æ¡£å¤„ç†ã€å¤šæ¨¡æ€åˆ†æã€åµŒå…¥ç”Ÿæˆã€çŸ¥è¯†å›¾è°±ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional, Union
from dataclasses import dataclass
from datetime import datetime
import json

# æ ¸å¿ƒæœåŠ¡å¯¼å…¥
from app.services.real_qwen_service import RealQwenService, RealQwenConfig
from app.services.qwen_embedding_service import QwenEmbeddingService
from app.services.minio_service import MinIOService
from app.services.milvus_service import MilvusService
from app.services.neo4j_service import Neo4jService
from app.services.document_deduplication import DocumentDeduplicationService
from app.services.ocr_service import get_ocr_service
from app.services.advanced_pdf_parser import get_pdf_parser

logger = logging.getLogger(__name__)


@dataclass
class ServiceConfig:
    """ç»Ÿä¸€æœåŠ¡é…ç½®"""
    qwen_api_key: str = "sk-5233a3a4b1a24426b6846a432794bbe2"
    qwen_base_url: str = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    enable_multimodal: bool = True
    enable_entity_extraction: bool = True
    enable_knowledge_graph: bool = True
    max_workers: int = 4
    timeout: int = 120
    # æ€§èƒ½ä¼˜åŒ–é…ç½®
    skip_ocr_if_text_exists: bool = True  # å¦‚æœPDFå·²æœ‰æ–‡æœ¬ï¼Œè·³è¿‡OCR
    simplify_entity_extraction: bool = False  # ç®€åŒ–å®ä½“æå–ï¼ˆå‡å°‘æ–‡æœ¬é•¿åº¦å’Œç±»å‹ï¼‰- å·²ç¦ç”¨ä»¥ç¡®ä¿æ•°æ®å‡†ç¡®æ€§


class CoreServiceIntegrator:
    """
    æ ¸å¿ƒæœåŠ¡æ•´åˆå™¨
    ç»Ÿä¸€ç®¡ç†æ–‡æ¡£å¤„ç†ã€å¤šæ¨¡æ€åˆ†æã€åµŒå…¥ç”Ÿæˆç­‰åŠŸèƒ½
    é¿å…ä»£ç åˆ†æ•£ï¼Œæä¾›ç»Ÿä¸€çš„æœåŠ¡å…¥å£
    """

    def __init__(self, config: Optional[ServiceConfig] = None):
        self.config = config or ServiceConfig()
        self._services = {}
        self._initialized = False

    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ‰€æœ‰æœåŠ¡"""
        if self._initialized:
            return

        logger.info("åˆå§‹åŒ–æ ¸å¿ƒæœåŠ¡æ•´åˆå™¨...")

        try:
            # åˆå§‹åŒ–Qwenå¤šæ¨¡æ€æœåŠ¡
            qwen_config = RealQwenConfig(
                api_key=self.config.qwen_api_key,
                base_url=self.config.qwen_base_url,
                enable_image_analysis=self.config.enable_multimodal,
                enable_chart_analysis=self.config.enable_multimodal,
                enable_formula_extraction=self.config.enable_multimodal,
                enable_entity_extraction=self.config.enable_entity_extraction,
                timeout=self.config.timeout
            )
            self._services['qwen'] = RealQwenService(qwen_config)

            # åˆå§‹åŒ–åµŒå…¥æœåŠ¡
            self._services['embedding'] = QwenEmbeddingService()

            # åˆå§‹åŒ–å­˜å‚¨æœåŠ¡
            self._services['minio'] = MinIOService()

            # åˆå§‹åŒ–MilvusæœåŠ¡ - å®¹è®¸è¿æ¥å¤±è´¥
            self._services['milvus'] = None
            try:
                milvus_service = MilvusService(embedding_model="qwen2.5-vl-embedding")
                await milvus_service.init_collections()  # åˆå§‹åŒ–Milvusé›†åˆ
                self._services['milvus'] = milvus_service
                logger.info("âœ… MilvusæœåŠ¡å·²è¿æ¥")
            except Exception as milvus_error:
                logger.warning(f"âš ï¸ Milvusè¿æ¥å¤±è´¥: {milvus_error}")
                logger.warning("âš ï¸ å‘é‡å­˜å‚¨åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œä½†æ–‡æ¡£å¤„ç†å°†ç»§ç»­")
                self._services['milvus'] = None

            # åˆå§‹åŒ–Neo4jæœåŠ¡ - å®¹è®¸è¿æ¥å¤±è´¥
            self._services['neo4j'] = None
            try:
                self._services['neo4j'] = Neo4jService()
                await self._services['neo4j'].connect()  # è¿æ¥åˆ°Neo4j
                logger.info("âœ… Neo4jæœåŠ¡å·²è¿æ¥")
            except Exception as neo4j_error:
                logger.warning(f"âš ï¸ Neo4jè¿æ¥å¤±è´¥: {neo4j_error}")
                logger.warning("âš ï¸ çŸ¥è¯†å›¾è°±åŠŸèƒ½å°†è¢«ç¦ç”¨ï¼Œä½†æ–‡æ¡£å¤„ç†å°†ç»§ç»­")
                self._services['neo4j'] = None

            # åˆå§‹åŒ–æ–‡æ¡£å»é‡æœåŠ¡
            self._services['deduplication'] = DocumentDeduplicationService()

            self._initialized = True
            logger.info("âœ… æ ¸å¿ƒæœåŠ¡æ•´åˆå™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ ¸å¿ƒæœåŠ¡æ•´åˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _ensure_initialized(self):
        """ç¡®ä¿æœåŠ¡å·²åˆå§‹åŒ–"""
        if not self._initialized:
            raise RuntimeError("æœåŠ¡æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ initialize()")

    async def process_document_complete(self,
                                     file_content: bytes,
                                     filename: str,
                                     document_id: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿
        æ•´åˆæ‰€æœ‰åˆ†æ•£çš„åŠŸèƒ½ï¼šä¸Šä¼ ã€è§£æã€åˆ†æã€å­˜å‚¨
        """
        self._ensure_initialized()

        logger.info(f"ğŸš€ å¼€å§‹å®Œæ•´æ–‡æ¡£å¤„ç†: {filename}")

        try:
            result = {
                'document_id': document_id,
                'filename': filename,
                'processing_start': datetime.now().isoformat(),
                'stages': {}
            }

            # é˜¶æ®µ1: æ–‡æ¡£ä¸Šä¼ å’Œå­˜å‚¨
            logger.info("ğŸ“¤ é˜¶æ®µ1: æ–‡æ¡£ä¸Šä¼ ...")
            file_path = f"documents/{datetime.now().strftime('%Y/%m/%d')}/{filename}"
            await self._services['minio'].upload_file(file_path, file_content)
            result['stages']['upload'] = {'status': 'completed', 'path': file_path}

            # é˜¶æ®µ2: é«˜çº§PDFè§£æ
            logger.info("ğŸ“„ é˜¶æ®µ2: é«˜çº§PDFè§£æ...")
            pdf_parser = get_pdf_parser()
            pdf_result = await pdf_parser.parse_pdf(file_content, filename)

            # ä½¿ç”¨è§£æç»“æœ - å®‰å…¨åœ°è·å–å†…å®¹
            content_data = pdf_result.get('content', {})
            text_content = content_data.get('raw_text') or ''
            markdown_content = content_data.get('markdown') or ''
            structured_content = content_data.get('structured') or {}

            # æ£€æŸ¥è§£ææ˜¯å¦æˆåŠŸ
            if not text_content and not markdown_content:
                error_msg = f"PDFè§£æå¤±è´¥: æœªæå–åˆ°ä»»ä½•å†…å®¹"
                logger.error(error_msg)
                result['stages']['parsing'] = {
                    'status': 'failed',
                    'error': error_msg
                }
                return {**result, 'success': False, 'error': error_msg}

            result['stages']['parsing'] = {
                'status': 'completed',
                'method': pdf_result.get('method', 'unknown'),
                'pages_processed': pdf_result.get('pages_processed', 0),
                'text_length': len(text_content) if text_content else 0,
                'has_markdown': bool(markdown_content),
                'has_structured': bool(structured_content)
            }

            # é˜¶æ®µ3: å¤šæ¨¡æ€åˆ†æ(ä½¿ç”¨Qwen VLå¢å¼º)
            logger.info("ğŸ§  é˜¶æ®µ3: å¤šæ¨¡æ€AIåˆ†æ...")

            # æ€§èƒ½ä¼˜åŒ–: å¦‚æœPDFå·²æœ‰è¶³å¤Ÿæ–‡æœ¬ï¼Œè·³è¿‡è€—æ—¶çš„OCRå¤„ç†
            text_length = len(text_content) if text_content else 0
            skip_multimodal = (
                self.config.skip_ocr_if_text_exists and
                text_length > 1000 and  # æ–‡æœ¬é•¿åº¦è¶…è¿‡1000å­—ç¬¦
                pdf_result.get('method') in ['PyPDF2', 'pymupdf4llm']  # å·²æˆåŠŸæå–æ–‡æœ¬
            )

            # é¢å¤–ä¼˜åŒ–: æ€»æ˜¯è·³è¿‡å¤šæ¨¡æ€åˆ†æï¼Œå› ä¸º PyMuPDF4LLM å·²ç»è¶³å¤Ÿ
            # å¦‚æœç¡®å®éœ€è¦å¤šæ¨¡æ€åˆ†æï¼Œå¯ä»¥å•ç‹¬å¯ç”¨
            if True or skip_multimodal:  # å¼ºåˆ¶è·³è¿‡å¤šæ¨¡æ€åˆ†æä»¥é¿å…APIé”™è¯¯
                logger.info(f"âš¡ PDFå·²æœ‰{text_length}å­—ç¬¦æ–‡æœ¬ï¼Œè·³è¿‡å¤šæ¨¡æ€OCRåˆ†æä»¥æå‡é€Ÿåº¦")
                analysis_result = {
                    'summary': text_content[:500] if text_content else '',  # ä½¿ç”¨å‰500å­—ç¬¦ä½œä¸ºæ‘˜è¦
                    'images_found': [],
                    'charts_found': [],
                    'formulas_found': [],
                    'tables_found': [],
                    'ocr_skipped': True,
                    'reason': 'PDFå·²æœ‰è¶³å¤Ÿæ–‡æœ¬å†…å®¹ï¼Œä½¿ç”¨PyMuPDF4LLMè§£æ'
                }
            else:
                analysis_result = await self._services['qwen'].analyze_document_multimodal(
                    file_content, filename, []
                )

            # åˆå¹¶è§£æç»“æœå’ŒAIåˆ†æç»“æœ
            analysis_result['parsed_text'] = text_content
            analysis_result['markdown'] = markdown_content
            analysis_result['structured_content'] = structured_content
            analysis_result['parsing_method'] = pdf_result.get('method')

            # è®°å½•ä½¿ç”¨çš„æ¨¡å‹
            if not skip_multimodal:
                models_used = ['qwen-vl-max', pdf_result.get('method', 'pymupdf4llm')]
            else:
                models_used = [pdf_result.get('method')]

            result['stages']['analysis'] = {
                'status': 'completed',
                'models_used': models_used,
                'sections': len(structured_content.get('titles') or []),
                'images_found': len(analysis_result.get('images_found') or []),
                'charts_found': len(analysis_result.get('charts_found') or []),
                'formulas_found': len(analysis_result.get('formulas_found') or []),
                'ocr_skipped': skip_multimodal
            }

            # é˜¶æ®µ4: å®ä½“å…³ç³»æŠ½å–
            logger.info("ğŸ”— é˜¶æ®µ4: å®ä½“å…³ç³»æŠ½å–...")
            # ä¼˜å…ˆä½¿ç”¨summaryï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨markdownæˆ–textå†…å®¹
            summary_text = (
                analysis_result.get('summary') or
                analysis_result.get('markdown') or
                text_content or
                ''
            )

            # æ€§èƒ½ä¼˜åŒ–: ç®€åŒ–å®ä½“æå–
            if self.config.simplify_entity_extraction:
                # å‡å°‘æ–‡æœ¬é•¿åº¦åˆ°3000å­—ç¬¦ï¼ˆåŸæ¥8000ï¼‰
                if len(summary_text) > 3000:
                    summary_text = summary_text[:3000]
                logger.info(f"âš¡ ç®€åŒ–æ¨¡å¼ï¼šå®ä½“æŠ½å–è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(summary_text)} å­—ç¬¦")
            else:
                # åŸå§‹é•¿åº¦é™åˆ¶
                if len(summary_text) > 8000:
                    summary_text = summary_text[:8000]
                logger.info(f"å®ä½“æŠ½å–è¾“å…¥æ–‡æœ¬é•¿åº¦: {len(summary_text)} å­—ç¬¦")

            entities, relationships = await self._services['qwen'].extract_entities_relationships(summary_text)
            entities = entities or []
            relationships = relationships or []
            result['stages']['entities'] = {
                'status': 'completed',
                'entity_count': len(entities),
                'relationship_count': len(relationships),
                'simplified': self.config.simplify_entity_extraction
            }

            # é˜¶æ®µ5: å‘é‡åµŒå…¥ç”Ÿæˆä¸æ–‡æ¡£å—å­˜å‚¨
            logger.info("ğŸ”¢ é˜¶æ®µ5: æ–‡æ¡£åˆ†å‰²ä¸å‘é‡åµŒå…¥ç”Ÿæˆ...")

            # 5a. å…ˆåˆ†å‰²æ–‡æ¡£æˆchunks
            chunks_data = await self._smart_chunk_text(text_content, analysis_result)
            logger.info(f"æ–‡æ¡£å·²åˆ†å‰²ä¸º {len(chunks_data)} ä¸ªchunks")

            # 5b. ä¸ºæ¯ä¸ªchunkç”Ÿæˆå‘é‡
            chunks_with_embeddings = []
            for chunk in chunks_data:
                try:
                    chunk_embedding = await self._services['embedding'].generate_embeddings([chunk['content']])
                    chunks_with_embeddings.append({
                        **chunk,
                        'embedding': chunk_embedding[0] if chunk_embedding else None
                    })
                except Exception as e:
                    logger.error(f"Chunk {chunk.get('chunk_index')} å‘é‡ç”Ÿæˆå¤±è´¥: {e}")
                    chunks_with_embeddings.append({**chunk, 'embedding': None})

            result['stages']['embeddings'] = {
                'status': 'completed',
                'chunks_processed': len(chunks_with_embeddings),
                'dimension': 1024,  # text-embedding-v4 ç»´åº¦
                'model': 'text-embedding-v4'
            }

            # é˜¶æ®µ6: æ•°æ®æŒä¹…åŒ–
            logger.info("ğŸ’¾ é˜¶æ®µ6: æ•°æ®æŒä¹…åŒ–...")

            # 6a. å­˜å‚¨æ–‡æ¡£å—åˆ°MySQL (åŒ…å«å‘é‡)
            await self._store_document_chunks_with_embeddings(
                document_id, chunks_with_embeddings
            )

            # 6b. å­˜å‚¨å‘é‡åˆ°Milvus - ä»…åœ¨æœåŠ¡å¯ç”¨æ—¶æ‰§è¡Œ
            if chunks_with_embeddings and self._services['milvus']:
                try:
                    # å‡†å¤‡Milvusæ ¼å¼çš„chunks
                    milvus_chunks = []
                    for chunk in chunks_with_embeddings:
                        if chunk.get('embedding'):
                            milvus_chunks.append({
                                'chunk_index': chunk.get('chunk_index', 0),
                                'content': chunk['content'],
                                'embedding': chunk['embedding'],
                                'metadata': {
                                    'page': chunk.get('page', 0),
                                    'section': chunk.get('section', ''),
                                    'title_path': chunk.get('title_path', []),
                                    'token_count': chunk.get('token_count', 0)
                                }
                            })

                    if milvus_chunks:
                        embedding_ids = await self._services['milvus'].insert_embeddings(
                            document_id=int(document_id),
                            chunks=milvus_chunks
                        )

                        # æ›´æ–°document_chunksè¡¨çš„embedding_id
                        await self._update_chunk_embedding_ids(document_id, embedding_ids)

                        result['stages']['storage'] = {
                            'status': 'completed',
                            'chunks_stored': len(milvus_chunks),
                            'embedding_ids': embedding_ids
                        }
                        logger.info(f"âœ… æˆåŠŸå­˜å‚¨ {len(milvus_chunks)} ä¸ªå‘é‡åˆ°Milvus")
                except Exception as e:
                    logger.error(f"Milvuså‘é‡å­˜å‚¨å¤±è´¥: {e}")
                    result['stages']['storage'] = {
                        'status': 'partial',
                        'mysql': 'completed',
                        'milvus': 'failed',
                        'error': str(e)
                    }
            elif chunks_with_embeddings and not self._services['milvus']:
                logger.warning("âš ï¸ MilvusæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡å‘é‡å­˜å‚¨")
                result['stages']['storage'] = {
                    'status': 'partial',
                    'mysql': 'completed',
                    'milvus': 'skipped',
                    'reason': 'MilvusæœåŠ¡ä¸å¯ç”¨'
                }

            # 5c. å­˜å‚¨çŸ¥è¯†å›¾è°±åˆ°Neo4jå’ŒMySQL
            if entities and self.config.enable_knowledge_graph:
                # MySQLå­˜å‚¨
                await self._store_knowledge_graph_to_mysql(
                    document_id, entities, relationships
                )

                # Neo4jå­˜å‚¨ - ä»…åœ¨æœåŠ¡å¯ç”¨æ—¶æ‰§è¡Œ
                if self._services['neo4j']:
                    for idx, entity in enumerate(entities[:10]):  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤š
                        entity_id = f"{document_id}_{entity.get('name', '')}_{idx}"
                        await self._services['neo4j'].create_knowledge_graph_node(
                            node_id=entity_id,
                            node_name=entity.get('name', ''),
                            node_type=entity.get('type', 'UNKNOWN'),
                            properties=entity,
                            document_id=int(document_id)
                        )

                    # å­˜å‚¨å…³ç³»åˆ°Neo4j
                    for idx, rel in enumerate(relationships[:10]):
                        rel_id = f"{document_id}_rel_{idx}"
                        source_id = f"{document_id}_{rel.get('from_entity', '')}"
                        target_id = f"{document_id}_{rel.get('to_entity', '')}"
                        await self._services['neo4j'].create_knowledge_graph_relation(
                            relation_id=rel_id,
                            source_node_id=source_id,
                            target_node_id=target_id,
                            relation_type=rel.get('type', 'RELATED_TO'),
                            properties=rel,
                            document_id=int(document_id)
                        )
                else:
                    logger.warning("âš ï¸ Neo4jæœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡çŸ¥è¯†å›¾è°±å­˜å‚¨åˆ°Neo4j")

            result['stages']['storage'] = {'status': 'completed'}

            # é˜¶æ®µ6: ä¿å­˜è§£æåçš„æ–‡æ¡£åˆ°æœ¬åœ°å­˜å‚¨
            logger.info("ğŸ’¾ é˜¶æ®µ6: ä¿å­˜è§£æåçš„æ–‡æ¡£åˆ°æœ¬åœ°...")
            await self._save_parsed_document_to_local(
                document_id=document_id,
                filename=filename,
                text_content=text_content,
                markdown_content=markdown_content,
                structured_content=structured_content,
                analysis_result=analysis_result
            )
            result['stages']['local_storage'] = {'status': 'completed'}

            result['processing_end'] = datetime.now().isoformat()
            result['status'] = 'completed'
            result['success'] = True

            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {filename}")
            return result

        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ {filename}: {e}")
            result['status'] = 'failed'
            result['error'] = str(e)
            result['success'] = False
            return result

    async def search_documents(self,
                            query: str,
                            top_k: int = 10,
                            filters: Optional[Dict] = None) -> List[Dict[str, Any]]:
        """
        ç»Ÿä¸€çš„æ–‡æ¡£æœç´¢æ¥å£
        æ•´åˆå‘é‡æœç´¢å’ŒçŸ¥è¯†å›¾è°±æœç´¢
        """
        self._ensure_initialized()

        try:
            # æ£€æŸ¥MilvusæœåŠ¡æ˜¯å¦å¯ç”¨
            if not self._services['milvus']:
                logger.warning("âš ï¸ MilvusæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œå‘é‡æœç´¢")
                return []

            # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
            query_embeddings = await self._services['embedding'].generate_embeddings([query])

            # å‘é‡æœç´¢
            search_results = await self._services['milvus'].search(
                collection_name="document_embeddings",
                query_vectors=query_embeddings,
                limit=top_k
            )

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for result in search_results:
                formatted_results.append({
                    'id': result.get('id', ''),
                    'filename': result.get('filename', ''),
                    'content': result.get('content', ''),
                    'score': result.get('score', 0.0),
                    'metadata': json.loads(result.get('metadata', '{}'))
                })

            return formatted_results

        except Exception as e:
            logger.error(f"æ–‡æ¡£æœç´¢å¤±è´¥: {e}")
            return []

    async def _store_document_chunks_to_mysql(self,
                                            document_id: str,
                                            text_content: str,
                                            analysis_result: Dict[str, Any]):
        """å­˜å‚¨æ–‡æ¡£åˆ†å—åˆ°MySQL"""
        from app.core.database import SessionLocal
        from app.models.document import DocumentChunk
        from sqlalchemy import func

        db = SessionLocal()
        try:
            # ä½¿ç”¨é«˜çº§åˆ†å‰²ç­–ç•¥
            chunks = await self._smart_chunk_text(text_content, analysis_result)

            for idx, chunk in enumerate(chunks):
                doc_chunk = DocumentChunk(
                    document_id=int(document_id),
                    chunk_index=idx,
                    content=chunk['content'],
                    chunk_metadata={
                        'page': chunk.get('page', 0),
                        'section': chunk.get('section', ''),
                        'title_path': chunk.get('title_path', []),
                        'chunk_type': chunk.get('type', 'text'),
                        'token_count': chunk.get('token_count', 0)
                    }
                )
                db.add(doc_chunk)

            db.commit()
            logger.info(f"âœ… ä¿å­˜ {len(chunks)} ä¸ªæ–‡æ¡£å—åˆ°MySQL")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡æ¡£å—å¤±è´¥: {e}")
            db.rollback()
        finally:
            db.close()

    async def _store_document_chunks_with_embeddings(self,
                                                   document_id: str,
                                                   chunks_with_embeddings: List[Dict[str, Any]]):
        """å­˜å‚¨æ–‡æ¡£å—åˆ°MySQL (ä¸åŒ…å«embeddingå‘é‡ï¼Œåªå­˜å‚¨å†…å®¹)"""
        from app.core.database import SessionLocal
        from app.models.document import DocumentChunk

        db = SessionLocal()
        try:
            for chunk in chunks_with_embeddings:
                doc_chunk = DocumentChunk(
                    document_id=int(document_id),
                    chunk_index=chunk.get('chunk_index', 0),
                    content=chunk['content'],
                    embedding_id=None,  # ç¨åæ›´æ–°
                    chunk_metadata={
                        'page': chunk.get('page', 0),
                        'section': chunk.get('section', ''),
                        'title_path': chunk.get('title_path', []),
                        'chunk_type': chunk.get('type', 'text'),
                        'token_count': chunk.get('token_count', 0)
                    }
                )
                db.add(doc_chunk)

            db.commit()
            logger.info(f"âœ… ä¿å­˜ {len(chunks_with_embeddings)} ä¸ªæ–‡æ¡£å—åˆ°MySQL")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ–‡æ¡£å—å¤±è´¥: {e}")
            db.rollback()
            raise
        finally:
            db.close()

    async def _update_chunk_embedding_ids(self,
                                         document_id: str,
                                         embedding_ids: List[int]):
        """æ›´æ–°æ–‡æ¡£å—çš„embedding_idå¹¶åˆ›å»ºVectorStorageè®°å½•"""
        from app.core.database import SessionLocal
        from app.models.document import DocumentChunk, VectorStorage

        if not embedding_ids:
            return

        db = SessionLocal()
        try:
            # è·å–æ‰€æœ‰chunksï¼ŒæŒ‰chunk_indexæ’åº
            chunks = db.query(DocumentChunk).filter(
                DocumentChunk.document_id == int(document_id)
            ).order_by(DocumentChunk.chunk_index).all()

            # æ›´æ–°embedding_idå¹¶åˆ›å»ºVectorStorageè®°å½•
            for i, chunk in enumerate(chunks):
                if i < len(embedding_ids):
                    embedding_id = embedding_ids[i]
                    chunk.embedding_id = embedding_id

                    # åˆ›å»ºVectorStorageè®°å½•
                    vector_record = VectorStorage(
                        document_id=int(document_id),
                        chunk_id=chunk.id,
                        vector_id=str(embedding_id),
                        model_provider='dashscope',
                        model_name='text-embedding-v4',
                        embedding_dimension=1024
                    )
                    db.add(vector_record)

            db.commit()
            logger.info(f"âœ… æ›´æ–° {len(chunks)} ä¸ªæ–‡æ¡£å—çš„embedding_idå¹¶åˆ›å»ºVectorStorageè®°å½•")

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°embedding_idå¤±è´¥: {e}")
            db.rollback()
        finally:
            db.close()

    async def _smart_chunk_text(self,
                               text_content: str,
                               analysis_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å‰² - ä¿ç•™æ ‡é¢˜ä¸Šä¸‹æ–‡"""
        import re

        chunks = []
        current_section = ""
        title_path = []
        chunk_index = 0  # æ·»åŠ chunkç´¢å¼•

        # è·å–ç« èŠ‚ç»“æ„
        sections = analysis_result.get('sections_analysis', [])

        # åŸºç¡€åˆ†å‰²: æŒ‰æ®µè½åˆ†å‰²ï¼Œä¿ç•™æ ‡é¢˜ä¸Šä¸‹æ–‡
        paragraphs = text_content.split('\n\n')
        current_chunk = ""
        chunk_size = 0
        max_chunk_size = 1000  # å­—ç¬¦æ•°
        page_num = 1

        for para in paragraphs:
            para = para.strip()
            if not para:
                continue

            # æ£€æµ‹æ ‡é¢˜
            if self._is_heading(para):
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append({
                        'chunk_index': chunk_index,  # æ·»åŠ ç´¢å¼•
                        'content': current_chunk.strip(),
                        'section': current_section,
                        'title_path': title_path.copy(),
                        'page': page_num,
                        'type': 'text',
                        'token_count': len(current_chunk.split())
                    })
                    chunk_index += 1

                # æ›´æ–°æ ‡é¢˜è·¯å¾„
                title_path.append(para)
                current_section = para
                current_chunk = ""
                chunk_size = 0
            else:
                # æ·»åŠ åˆ°å½“å‰å—
                if chunk_size + len(para) > max_chunk_size and current_chunk:
                    # ä¿å­˜å½“å‰å—
                    chunks.append({
                        'chunk_index': chunk_index,  # æ·»åŠ ç´¢å¼•
                        'content': current_chunk.strip(),
                        'section': current_section,
                        'title_path': title_path.copy(),
                        'page': page_num,
                        'type': 'text',
                        'token_count': len(current_chunk.split())
                    })
                    chunk_index += 1
                    current_chunk = para
                    chunk_size = len(para)
                    page_num += 1
                else:
                    current_chunk += "\n\n" + para if current_chunk else para
                    chunk_size += len(para)

        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append({
                'chunk_index': chunk_index,  # æ·»åŠ ç´¢å¼•
                'content': current_chunk.strip(),
                'section': current_section,
                'title_path': title_path.copy(),
                'page': page_num,
                'type': 'text',
                'token_count': len(current_chunk.split())
            })

        logger.info(f"æ™ºèƒ½åˆ†å‰²äº§ç”Ÿ {len(chunks)} ä¸ªå—")
        return chunks

    def _is_heading(self, text: str) -> bool:
        """æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜"""
        import re
        # æ£€æµ‹ä¸­æ–‡æ ‡é¢˜æ¨¡å¼: ä¸€ã€äºŒã€ä¸‰ã€æˆ– 1.1ã€1.2ç­‰
        heading_patterns = [
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ç« èŠ‚ç¯‡]',
            r'^\d+\.\d+\s+\S',
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.]',
            r'^\d{1,2}[ã€.]',
            r'^[A-Z][A-Z\s]+$'  # å…¨å¤§å†™è‹±æ–‡æ ‡é¢˜
        ]
        return any(re.match(pattern, text) for pattern in heading_patterns)

    def _map_entity_type_to_db(self, entity_type: str) -> str:
        """å°†æå–çš„å®ä½“ç±»å‹æ˜ å°„åˆ°æ•°æ®åº“ NodeType æšä¸¾å€¼"""
        from app.models.knowledge_graph import NodeType

        type_mapping = {
            'å…¬å¸': NodeType.ORGANIZATION,
            'é›†å›¢': NodeType.ORGANIZATION,
            'ä¼ä¸š': NodeType.ORGANIZATION,
            'é“¶è¡Œ': NodeType.ORGANIZATION,
            'è¯åˆ¸': NodeType.ORGANIZATION,
            'æœºæ„': NodeType.ORGANIZATION,
            'äº§å“': NodeType.CONCEPT,
            'èŠ¯ç‰‡': NodeType.CONCEPT,
            'æ•°å€¼': NodeType.AMOUNT,
            'UNKNOWN': NodeType.ENTITY,
            'Person': NodeType.PERSON,
            'Location': NodeType.LOCATION,
            'Date': NodeType.DATE,
            'Event': NodeType.EVENT
        }

        return type_mapping.get(entity_type, NodeType.ENTITY)

    def _map_relation_type_to_db(self, relation_type: str) -> str:
        """å°†æå–çš„å…³ç³»ç±»å‹æ˜ å°„åˆ°æ•°æ®åº“ RelationType æšä¸¾å€¼"""
        from app.models.knowledge_graph import RelationType

        # å°†å…³ç³»ç±»å‹è½¬æ¢ä¸ºå°å†™å¹¶æ ‡å‡†åŒ–
        rel_type_lower = relation_type.lower().replace('-', '_').replace(' ', '_')

        # ç›´æ¥æ˜ å°„è¡¨
        direct_mapping = {
            'owns': RelationType.OWNS,
            'work_for': RelationType.WORKS_FOR,
            'works_for': RelationType.WORKS_FOR,
            'located_in': RelationType.LOCATED_IN,
            'part_of': RelationType.PART_OF,
            'related_to': RelationType.RELATED_TO,
            'invests_in': RelationType.INVESTS_IN,
            'acquires': RelationType.ACQUIRES,
            'merges_with': RelationType.MERGES_WITH,
            'collaborates_with': RelationType.COLLABORATES_WITH,
            'reports_to': RelationType.REPORTS_TO,
            'regulated_by': RelationType.REGULATED_BY
        }

        # ä¸­æ–‡å…³ç³»æ˜ å°„
        chinese_mapping = {
            'æ‹¥æœ‰': RelationType.OWNS,
            'éš¶å±äº': RelationType.PART_OF,
            'ä½äº': RelationType.LOCATED_IN,
            'æŠ•èµ„': RelationType.INVESTS_IN,
            'æ”¶è´­': RelationType.ACQUIRES,
            'åˆä½œ': RelationType.COLLABORATES_WITH,
            'æŠ¥å‘Šç»™': RelationType.REPORTS_TO,
            'å—ç›‘ç®¡': RelationType.REGULATED_BY
        }

        if rel_type_lower in direct_mapping:
            return direct_mapping[rel_type_lower]

        if relation_type in chinese_mapping:
            return chinese_mapping[relation_type]

        # é»˜è®¤è¿”å› RELATED_TO
        return RelationType.RELATED_TO

    async def _store_knowledge_graph_to_mysql(self,
                                            document_id: str,
                                            entities: List[Dict],
                                            relationships: List[Dict]):
        """å­˜å‚¨çŸ¥è¯†å›¾è°±æ•°æ®åˆ°MySQL"""
        from app.core.database import SessionLocal
        from app.models.knowledge_graph import KnowledgeGraphNode, KnowledgeGraphRelation
        import uuid

        db = SessionLocal()
        try:
            # å­˜å‚¨å®ä½“èŠ‚ç‚¹ï¼Œå¹¶å»ºç«‹å®ä½“åç§°åˆ°node_idçš„æ˜ å°„
            entity_name_to_node_id = {}
            for entity in entities[:50]:  # é™åˆ¶æ•°é‡
                node_id = f"{document_id}_{entity.get('name', '')}_{uuid.uuid4().hex[:8]}"
                entity_name = entity.get('name', '')
                entity_name_to_node_id[entity_name] = node_id  # å»ºç«‹æ˜ å°„

                entity_type = entity.get('type', 'UNKNOWN')
                mapped_type = self._map_entity_type_to_db(entity_type)

                kg_node = KnowledgeGraphNode(
                    document_id=int(document_id),
                    node_id=node_id,
                    node_type=mapped_type,
                    node_name=entity_name,
                    properties=entity
                )
                db.add(kg_node)

            # å­˜å‚¨å…³ç³»
            for rel in relationships[:50]:
                rel_id = f"{document_id}_{rel.get('from_entity', '')}_{rel.get('to_entity', '')}_{uuid.uuid4().hex[:8]}"
                relation_type = rel.get('type', 'RELATED_TO')
                mapped_rel_type = self._map_relation_type_to_db(relation_type)

                # ä¿®å¤ï¼šä½¿ç”¨æ˜ å°„è·å–æ­£ç¡®çš„node_id
                from_entity_name = rel.get('from_entity', '')
                to_entity_name = rel.get('to_entity', '')

                # ä»æ˜ å°„ä¸­è·å–node_idï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™ä½¿ç”¨å®ä½“åç§°ä½œä¸ºfallback
                source_node_id = entity_name_to_node_id.get(from_entity_name, from_entity_name)
                target_node_id = entity_name_to_node_id.get(to_entity_name, to_entity_name)

                kg_rel = KnowledgeGraphRelation(
                    document_id=int(document_id),
                    relation_id=rel_id,
                    relation_type=mapped_rel_type,
                    source_node_id=source_node_id,
                    target_node_id=target_node_id,
                    relation_label=rel.get('description', ''),
                    properties=rel
                )
                db.add(kg_rel)

            db.commit()
            logger.info(f"âœ… ä¿å­˜ {len(entities)} ä¸ªèŠ‚ç‚¹å’Œ {len(relationships)} ä¸ªå…³ç³»åˆ°MySQL")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
            db.rollback()
        finally:
            db.close()

    async def get_service_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æœåŠ¡çš„çŠ¶æ€"""
        self._ensure_initialized()

        status = {
            'integrator': 'initialized',
            'services': {}
        }

        for name, service in self._services.items():
            try:
                # ç®€å•çš„å¥åº·æ£€æŸ¥
                status['services'][name] = 'healthy'
            except Exception as e:
                status['services'][name] = f'unhealthy: {e}'

        return status

    async def _save_parsed_document_to_local(self,
                                           document_id: str,
                                           filename: str,
                                           text_content: str,
                                           markdown_content: str,
                                           structured_content: dict,
                                           analysis_result: dict):
        """ä¿å­˜è§£æåçš„æ–‡æ¡£åˆ°æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ"""
        import os
        import json
        from pathlib import Path

        try:
            # åˆ›å»ºå­˜å‚¨ç›®å½•
            storage_base = Path('/app/storage/parsed_docs')
            storage_base.mkdir(parents=True, exist_ok=True)

            # ä¸ºæ¯ä¸ªæ–‡æ¡£åˆ›å»ºå­ç›®å½•
            doc_dir = storage_base / str(document_id)
            doc_dir.mkdir(exist_ok=True)

            # ä¿å­˜åŸå§‹æ–‡æœ¬
            if text_content:
                text_file = doc_dir / 'content.txt'
                text_file.write_text(text_content, encoding='utf-8')
                logger.info(f"  âœ… ä¿å­˜æ–‡æœ¬: {text_file}")

            # ä¿å­˜Markdown
            if markdown_content:
                md_file = doc_dir / 'content.md'
                md_file.write_text(markdown_content, encoding='utf-8')
                logger.info(f"  âœ… ä¿å­˜Markdown: {md_file}")

            # ä¿å­˜ç»“æ„åŒ–å†…å®¹ï¼ˆJSONï¼‰
            if structured_content:
                structured_file = doc_dir / 'structured.json'
                structured_file.write_text(
                    json.dumps(structured_content, ensure_ascii=False, indent=2),
                    encoding='utf-8'
                )
                logger.info(f"  âœ… ä¿å­˜ç»“æ„åŒ–æ•°æ®: {structured_file}")

            # ä¿å­˜å®Œæ•´çš„åˆ†æç»“æœï¼ˆJSONï¼‰
            analysis_file = doc_dir / 'analysis.json'
            # æ¸…ç†ä¸èƒ½åºåˆ—åŒ–çš„å¯¹è±¡
            clean_analysis = {}
            for key, value in analysis_result.items():
                if isinstance(value, (str, int, float, bool, list, dict, type(None))):
                    clean_analysis[key] = value
                else:
                    clean_analysis[key] = str(value)

            analysis_file.write_text(
                json.dumps(clean_analysis, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            logger.info(f"  âœ… ä¿å­˜åˆ†æç»“æœ: {analysis_file}")

            # ä¿å­˜å…ƒæ•°æ®
            metadata = {
                'document_id': document_id,
                'filename': filename,
                'saved_at': datetime.now().isoformat(),
                'text_length': len(text_content) if text_content else 0,
                'markdown_length': len(markdown_content) if markdown_content else 0,
                'has_structured': bool(structured_content),
                'files_created': [
                    'content.txt' if text_content else None,
                    'content.md' if markdown_content else None,
                    'structured.json' if structured_content else None,
                    'analysis.json',
                    'metadata.json'
                ]
            }
            metadata_file = doc_dir / 'metadata.json'
            metadata_file.write_text(
                json.dumps(metadata, ensure_ascii=False, indent=2),
                encoding='utf-8'
            )
            logger.info(f"  âœ… ä¿å­˜å…ƒæ•°æ®: {metadata_file}")

            logger.info(f"ğŸ’¾ è§£ææ–‡æ¡£å·²ä¿å­˜åˆ°æœ¬åœ°: {doc_dir}")

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è§£ææ–‡æ¡£åˆ°æœ¬åœ°å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ä¸»æµç¨‹ç»§ç»­

    def get_config_summary(self) -> Dict[str, Any]:
        """è·å–é…ç½®æ‘˜è¦"""
        return {
            'multimodal_enabled': self.config.enable_multimodal,
            'entity_extraction_enabled': self.config.enable_entity_extraction,
            'knowledge_graph_enabled': self.config.enable_knowledge_graph,
            'max_workers': self.config.max_workers,
            'timeout': self.config.timeout
        }


# å…¨å±€æœåŠ¡æ•´åˆå™¨å®ä¾‹
_service_integrator = None


def get_service_integrator() -> CoreServiceIntegrator:
    """è·å–å…¨å±€æœåŠ¡æ•´åˆå™¨å®ä¾‹"""
    global _service_integrator
    if _service_integrator is None:
        _service_integrator = CoreServiceIntegrator()
    return _service_integrator