"""
ç‹¬ç«‹çš„å‘é‡ç”ŸæˆæœåŠ¡ - ä»ŽCoreServiceIntegratoræ‹†åˆ†
è´Ÿè´£æ‰¹é‡å¹¶è¡Œå‘é‡ç”Ÿæˆ
"""

import asyncio
from typing import List, Dict, Any
from datetime import datetime
from app.core.structured_logging import get_structured_logger

logger = get_structured_logger(__name__)


class VectorGenerationService:
    """
    å‘é‡ç”ŸæˆæœåŠ¡

    åŠŸèƒ½ï¼š
    - æ‰¹é‡å¹¶è¡Œå‘é‡ç”Ÿæˆ
    - å‘é‡è´¨é‡éªŒè¯
    - æ€§èƒ½ç›‘æŽ§
    """

    def __init__(self, embedding_service, batch_size: int = 50, max_concurrent: int = 10):
        self.embedding_service = embedding_service
        self.batch_size = batch_size
        self.max_concurrent = max_concurrent

    async def generate_vectors_batch(
        self,
        chunks: List[Dict[str, Any]],
        document_id: str
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¹¶è¡Œç”Ÿæˆå‘é‡

        Args:
            chunks: chunkåˆ—è¡¨
            document_id: æ–‡æ¡£ID

        Returns:
            å¸¦æœ‰å‘é‡çš„chunkåˆ—è¡¨
        """
        if not chunks:
            return []

        total_chunks = len(chunks)
        logger.info(f"ðŸ”¢ æ‰¹é‡å¹¶è¡Œå‘é‡ç”Ÿæˆ: {total_chunks} chunks, batch_size={self.batch_size}")

        start_time = datetime.now()

        # åˆ†æ‰¹
        batches = [
            chunks[i:i + self.batch_size]
            for i in range(0, total_chunks, self.batch_size)
        ]

        logger.info(f"ðŸ“¦ åˆ†ä¸º {len(batches)} ä¸ªbatch")

        # ä½¿ç”¨ä¿¡å·é‡æŽ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_batch(batch: List[Dict], batch_idx: int) -> List[Dict]:
            """å¤„ç†ä¸€ä¸ªbatch"""
            async with semaphore:
                try:
                    texts = [chunk['content'] for chunk in batch]
                    embeddings = await self.embedding_service.embed_batch(texts)

                    result_batch = []
                    for chunk, embedding in zip(batch, embeddings):
                        result_batch.append({
                            **chunk,
                            'embedding': embedding if embedding else None
                        })

                    logger.info(f"âœ… Batch {batch_idx + 1}/{len(batches)} å®Œæˆ: {len(batch)} chunks")
                    return result_batch

                except Exception as e:
                    logger.error(f"âŒ Batch {batch_idx + 1} å¤±è´¥: {e}")
                    return [{**chunk, 'embedding': None} for chunk in batch]

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰batch
        batch_tasks = [process_batch(batch, i) for i, batch in enumerate(batches)]
        batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)

        # åˆå¹¶ç»“æžœ
        chunks_with_vectors = []
        for result in batch_results:
            if isinstance(result, Exception):
                logger.error(f"Batchå¼‚å¸¸: {result}")
                continue
            chunks_with_vectors.extend(result)

        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"âœ… å‘é‡ç”Ÿæˆå®Œæˆ: {len(chunks_with_vectors)} chunks, è€—æ—¶ {duration:.2f}ç§’")

        return chunks_with_vectors

    def validate_embeddings(self, chunks_with_vectors: List[Dict]) -> Dict[str, Any]:
        """éªŒè¯å‘é‡è´¨é‡"""
        valid_count = 0
        invalid_count = 0
        total_dim = 0

        for chunk in chunks_with_vectors:
            embedding = chunk.get('embedding')
            if embedding is not None and hasattr(embedding, '__len__') and len(embedding) > 0:
                valid_count += 1
                if total_dim == 0:
                    total_dim = len(embedding)
            else:
                invalid_count += 1

        return {
            'total': len(chunks_with_vectors),
            'valid': valid_count,
            'invalid': invalid_count,
            'dimension': total_dim,
            'valid_rate': valid_count / len(chunks_with_vectors) if chunks_with_vectors else 0
        }
