"""
ç»Ÿä¸€æ–‡æ¡£å¤„ç†ç¼–æ’å™¨ï¼ˆé‡æ„ç‰ˆï¼‰
æ•´åˆorchestratorå’Œcore_service_integratorçš„åŠŸèƒ½

ä¼˜åŒ–ç‚¹ï¼š
- æ¸…æ™°çš„èŒè´£åˆ†ç¦»
- æ’ä»¶åŒ–çš„æœåŠ¡æ¶æ„
- ç»Ÿä¸€çš„æµç¨‹æ§åˆ¶
- å®Œå–„çš„é”™è¯¯å¤„ç†
- è¯¦ç»†çš„è¿›åº¦è·Ÿè¸ª
"""

import asyncio
from typing import Dict, Any, Optional, List, Callable
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum

from app.core.structured_logging import get_structured_logger

logger = get_structured_logger(__name__)


class ProcessingStage(Enum):
    """å¤„ç†é˜¶æ®µ"""
    VALIDATION = "validation"           # æ–‡ä»¶éªŒè¯
    PARSING = "parsing"                 # æ–‡æ¡£è§£æ
    CHUNKING = "chunking"               # æ–‡æ¡£åˆ†å—
    ENTITY_EXTRACTION = "entity_extraction"  # å®ä½“æå–
    EMBEDDING = "embedding"             # å‘é‡ç”Ÿæˆ
    STORAGE = "storage"                 # å­˜å‚¨å…¥åº“
    INDEXING = "indexing"               # ç´¢å¼•æ„å»º


@dataclass
class StageResult:
    """é˜¶æ®µç»“æœ"""
    stage: ProcessingStage
    status: str  # 'pending', 'running', 'completed', 'failed'
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    duration: float = 0.0
    data: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'stage': self.stage.value,
            'status': self.status,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None,
            'duration': self.duration,
            'data': self.data,
            'error': self.error
        }


@dataclass
class ProcessingResult:
    """å¤„ç†ç»“æœ"""
    success: bool
    document_id: str
    filename: str
    stages: List[StageResult] = field(default_factory=list)
    metrics: Dict[str, Any] = field(default_factory=dict)
    error: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'success': self.success,
            'document_id': self.document_id,
            'filename': self.filename,
            'stages': [stage.to_dict() for stage in self.stages],
            'metrics': self.metrics,
            'error': self.error
        }


class ProcessingPipeline:
    """
    å¤„ç†æµæ°´çº¿

    ç‰¹ç‚¹ï¼š
    - æ’ä»¶åŒ–çš„é˜¶æ®µå¤„ç†å™¨
    - å¯é…ç½®çš„æµç¨‹æ§åˆ¶
    - å®Œå–„çš„é”™è¯¯å¤„ç†
    """

    def __init__(self):
        self.handlers: Dict[ProcessingStage, Callable] = {}
        self.middleware: List[Callable] = []

    def register_handler(
        self,
        stage: ProcessingStage,
        handler: Callable
    ):
        """æ³¨å†Œé˜¶æ®µå¤„ç†å™¨"""
        self.handlers[stage] = handler
        logger.debug(f"æ³¨å†Œå¤„ç†å™¨: {stage.value}")

    def register_middleware(self, middleware: Callable):
        """æ³¨å†Œä¸­é—´ä»¶"""
        self.middleware.append(middleware)
        logger.debug(f"æ³¨å†Œä¸­é—´ä»¶: {middleware.__name__}")

    async def execute(
        self,
        context: Dict[str, Any],
        stages: List[ProcessingStage]
    ) -> ProcessingResult:
        """
        æ‰§è¡Œæµæ°´çº¿

        Args:
            context: å¤„ç†ä¸Šä¸‹æ–‡
            stages: è¦æ‰§è¡Œçš„é˜¶æ®µåˆ—è¡¨

        Returns:
            ProcessingResult
        """
        document_id = context.get('document_id', 'unknown')
        filename = context.get('filename', 'unknown')

        result = ProcessingResult(
            success=True,
            document_id=document_id,
            filename=filename
        )

        start_time = datetime.now()

        try:
            # æ‰§è¡Œå‰ç½®ä¸­é—´ä»¶
            for middleware in self.middleware:
                await middleware(context, 'before')

            # æ‰§è¡Œå„ä¸ªé˜¶æ®µ
            for stage in stages:
                stage_result = await self._execute_stage(stage, context)
                result.stages.append(stage_result)

                if stage_result.status == 'failed':
                    # é˜¶æ®µå¤±è´¥ï¼Œåœæ­¢åç»­å¤„ç†
                    result.success = False
                    result.error = f"é˜¶æ®µ {stage.value} å¤±è´¥: {stage_result.error}"
                    logger.error(result.error)
                    break

                # å°†é˜¶æ®µç»“æœä¼ é€’ç»™ä¸‹ä¸€ä¸ªé˜¶æ®µ
                context[f'{stage.value}_result'] = stage_result

            # æ‰§è¡Œåç½®ä¸­é—´ä»¶
            for middleware in self.middleware:
                await middleware(context, 'after')

            # è®¡ç®—æ€»è€—æ—¶
            total_duration = (datetime.now() - start_time).total_seconds()
            result.metrics['total_duration'] = total_duration

            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {filename} (è€—æ—¶: {total_duration:.2f}ç§’)")

        except Exception as e:
            result.success = False
            result.error = str(e)
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¼‚å¸¸: {e}", exc_info=True)

        return result

    async def _execute_stage(
        self,
        stage: ProcessingStage,
        context: Dict[str, Any]
    ) -> StageResult:
        """
        æ‰§è¡Œå•ä¸ªé˜¶æ®µ

        Args:
            stage: å¤„ç†é˜¶æ®µ
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            StageResult
        """
        stage_result = StageResult(
            stage=stage,
            status='pending'
        )

        start_time = datetime.now()

        try:
            logger.info(f"ğŸ”„ é˜¶æ®µå¼€å§‹: {stage.value}")
            stage_result.status = 'running'
            stage_result.start_time = start_time

            # æ£€æŸ¥æ˜¯å¦æœ‰æ³¨å†Œçš„å¤„ç†å™¨
            if stage not in self.handlers:
                logger.warning(f"âš ï¸  é˜¶æ®µ {stage.value} æ²¡æœ‰æ³¨å†Œå¤„ç†å™¨ï¼Œè·³è¿‡")
                stage_result.status = 'completed'
                return stage_result

            # æ‰§è¡Œå¤„ç†å™¨
            handler = self.handlers[stage]
            result_data = await handler(context)

            stage_result.status = 'completed'
            stage_result.data = result_data

            # è®°å½•è€—æ—¶
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            stage_result.end_time = end_time
            stage_result.duration = duration

            logger.info(f"âœ… é˜¶æ®µå®Œæˆ: {stage.value} (è€—æ—¶: {duration:.2f}ç§’)")

        except Exception as e:
            stage_result.status = 'failed'
            stage_result.error = str(e)
            stage_result.end_time = datetime.now()
            stage_result.duration = (stage_result.end_time - start_time).total_seconds()

            logger.error(f"âŒ é˜¶æ®µå¤±è´¥: {stage.value} - {e}")

        return stage_result


class UnifiedOrchestrator:
    """
    ç»Ÿä¸€æ–‡æ¡£å¤„ç†ç¼–æ’å™¨ï¼ˆé‡æ„ç‰ˆï¼‰

    æ•´åˆäº†orchestratorå’Œcore_service_integratorçš„åŠŸèƒ½

    ç‰¹ç‚¹ï¼š
    - æ¸…æ™°çš„èŒè´£åˆ†ç¦»ï¼ˆåªè´Ÿè´£ç¼–æ’ï¼Œä¸è´Ÿè´£ä¸šåŠ¡é€»è¾‘ï¼‰
    - æ’ä»¶åŒ–çš„æœåŠ¡æ¶æ„
    - ç»Ÿä¸€çš„æµç¨‹æ§åˆ¶
    - å®Œå–„çš„é”™è¯¯å¤„ç†
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self.pipeline = ProcessingPipeline()
        self.services: Dict[str, Any] = {}

        # æµæ°´çº¿é…ç½®
        self.enable_validation = self.config.get('enable_validation', True)
        self.enable_parsing = self.config.get('enable_parsing', True)
        self.enable_chunking = self.config.get('enable_chunking', True)
        self.enable_entity_extraction = self.config.get('enable_entity_extraction', False)
        self.enable_embedding = self.config.get('enable_embedding', True)
        self.enable_storage = self.config.get('enable_storage', True)

        self._initialized = False

    async def initialize(self):
        """åˆå§‹åŒ–ç¼–æ’å™¨å’ŒæœåŠ¡"""
        if self._initialized:
            return

        logger.info("ğŸ”§ åˆå§‹åŒ–ç»Ÿä¸€ç¼–æ’å™¨...")

        # åˆå§‹åŒ–æœåŠ¡
        await self._initialize_services()

        # æ³¨å†Œé˜¶æ®µå¤„ç†å™¨
        self._register_handlers()

        # æ³¨å†Œä¸­é—´ä»¶
        self._register_middleware()

        self._initialized = True
        logger.info("âœ… ç»Ÿä¸€ç¼–æ’å™¨åˆå§‹åŒ–å®Œæˆ")

    async def _initialize_services(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
        from app.services.parsers.parser_factory import get_parser_factory
        from app.services.embeddings.unified_embedding_service import get_embedding_service
        from app.services.unified_chunker import UnifiedChunker

        # è§£æå™¨å·¥å‚
        self.services['parser_factory'] = get_parser_factory()
        logger.info("âœ… è§£æå™¨å·¥å‚å·²åŠ è½½")

        # EmbeddingæœåŠ¡
        self.services['embedding'] = get_embedding_service()
        logger.info("âœ… EmbeddingæœåŠ¡å·²åŠ è½½")

        # Chunker
        self.services['chunker'] = UnifiedChunker(config=self.config)
        logger.info("âœ… Chunkerå·²åŠ è½½")

        # å¯é€‰æœåŠ¡
        if self.enable_entity_extraction:
            try:
                from app.services.financial_entity_extractor import get_financial_entity_extractor
                self.services['entity_extractor'] = get_financial_entity_extractor()
                logger.info("âœ… å®ä½“æå–å™¨å·²åŠ è½½")
            except ImportError as e:
                logger.warning(f"âš ï¸  å®ä½“æå–å™¨åŠ è½½å¤±è´¥: {e}")

    def _register_handlers(self):
        """æ³¨å†Œé˜¶æ®µå¤„ç†å™¨"""
        # æ–‡æ¡£è§£æ
        if self.enable_parsing:
            self.pipeline.register_handler(
                ProcessingStage.PARSING,
                self._handle_parsing
            )

        # æ–‡æ¡£åˆ†å—
        if self.enable_chunking:
            self.pipeline.register_handler(
                ProcessingStage.CHUNKING,
                self._handle_chunking
            )

        # å‘é‡ç”Ÿæˆ
        if self.enable_embedding:
            self.pipeline.register_handler(
                ProcessingStage.EMBEDDING,
                self._handle_embedding
            )

        # å­˜å‚¨
        if self.enable_storage:
            self.pipeline.register_handler(
                ProcessingStage.STORAGE,
                self._handle_storage
            )

    def _register_middleware(self):
        """æ³¨å†Œä¸­é—´ä»¶"""
        # æ—¥å¿—ä¸­é—´ä»¶
        async def logging_middleware(context, position):
            if position == 'before':
                logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {context.get('filename')}")
            elif position == 'after':
                logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: {context.get('metrics', {})}")

        self.pipeline.register_middleware(logging_middleware)

    # ========================================================================
    # é˜¶æ®µå¤„ç†å™¨
    # ========================================================================

    async def _handle_parsing(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£è§£æ

        Args:
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            è§£æç»“æœ
        """
        file_path = context['file_path']

        # ä½¿ç”¨è§£æå™¨å·¥å‚è‡ªåŠ¨è§£æ
        parse_result = await self.services['parser_factory'].parse_document(file_path)

        return {
            'success': parse_result.success,
            'content': parse_result.content,
            'markdown': parse_result.markdown,
            'metadata': parse_result.metadata.to_dict() if parse_result.metadata else {},
            'sections_count': len(parse_result.sections),
            'tables_count': len(parse_result.tables),
            'images_count': len(parse_result.images)
        }

    async def _handle_chunking(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡æ¡£åˆ†å—

        Args:
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            åˆ†å—ç»“æœ
        """
        parsing_result = context['parsing_result']
        content = parsing_result['content']
        metadata = parsing_result['metadata']

        # ä½¿ç”¨chunkerè¿›è¡Œåˆ†å—
        chunks = await self.services['chunker'].chunk([content], metadata)

        return {
            'chunks_count': len(chunks),
            'avg_chunk_size': sum(len(c.content) for c in chunks) / len(chunks) if chunks else 0
        }

    async def _handle_embedding(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å‘é‡ç”Ÿæˆ

        Args:
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            Embeddingç»“æœ
        """
        parsing_result = context['parsing_result']
        content = parsing_result['content']

        # ç”Ÿæˆembedding
        embedding = await self.services['embedding'].embed(content)

        return {
            'embedding_dimension': len(embedding),
            'embedding_norm': float(abs(sum(embedding)))  # L1èŒƒæ•°
        }

    async def _handle_storage(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """
        å¤„ç†å­˜å‚¨

        Args:
            context: å¤„ç†ä¸Šä¸‹æ–‡

        Returns:
            å­˜å‚¨ç»“æœ
        """
        # TODO: å®ç°å®é™…çš„å­˜å‚¨é€»è¾‘
        document_id = context.get('document_id')

        return {
            'stored': True,
            'document_id': document_id
        }

    # ========================================================================
    # å…¬å…±æ¥å£
    # ========================================================================

    async def process_document(
        self,
        file_path: str,
        document_id: Optional[str] = None,
        **kwargs
    ) -> ProcessingResult:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆä¸»å…¥å£ï¼‰

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ID
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ProcessingResult
        """
        if not self._initialized:
            await self.initialize()

        # å‡†å¤‡å¤„ç†ä¸Šä¸‹æ–‡
        context = {
            'file_path': file_path,
            'filename': kwargs.get('filename', file_path),
            'document_id': document_id,
            'config': kwargs
        }

        # å®šä¹‰å¤„ç†æµç¨‹
        stages = [
            ProcessingStage.PARSING,
            ProcessingStage.CHUNKING,
            ProcessingStage.EMBEDDING,
            ProcessingStage.STORAGE,
        ]

        # æ‰§è¡Œæµæ°´çº¿
        result = await self.pipeline.execute(context, stages)

        return result

    async def batch_process_documents(
        self,
        file_paths: List[str],
        **kwargs
    ) -> List[ProcessingResult]:
        """
        æ‰¹é‡å¤„ç†æ–‡æ¡£

        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            ProcessingResultåˆ—è¡¨
        """
        tasks = []

        for i, file_path in enumerate(file_paths):
            document_id = kwargs.get(f'document_id_{i}') or f'doc_{i}'

            task = self.process_document(
                file_path=file_path,
                document_id=document_id,
                **kwargs
            )

            tasks.append(task)

        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                processed_results.append(
                    ProcessingResult(
                        success=False,
                        document_id=f'doc_{i}',
                        filename=file_paths[i],
                        error=str(result)
                    )
                )
            else:
                processed_results.append(result)

        return processed_results

    def get_metrics(self) -> Dict[str, Any]:
        """
        è·å–ç¼–æ’å™¨æŒ‡æ ‡

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        return {
            'initialized': self._initialized,
            'registered_handlers': list(self.pipeline.handlers.keys()),
            'middleware_count': len(self.pipeline.middleware),
            'services': list(self.services.keys())
        }


# ============================================================================
# å…¨å±€å®ä¾‹
# ============================================================================

_global_orchestrator: Optional[UnifiedOrchestrator] = None


def get_orchestrator(config: Optional[Dict[str, Any]] = None) -> UnifiedOrchestrator:
    """
    è·å–å…¨å±€ç¼–æ’å™¨å®ä¾‹

    Args:
        config: é…ç½®å‚æ•°

    Returns:
        UnifiedOrchestratorå®ä¾‹
    """
    global _global_orchestrator

    if _global_orchestrator is None:
        _global_orchestrator = UnifiedOrchestrator(config)
        logger.info("å…¨å±€ç¼–æ’å™¨å·²åˆ›å»º")

    return _global_orchestrator


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

async def process_document(
    file_path: str,
    config: Optional[Dict[str, Any]] = None
) -> ProcessingResult:
    """
    å¤„ç†æ–‡æ¡£ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        config: é…ç½®å‚æ•°

    Returns:
        ProcessingResult
    """
    orchestrator = get_orchestrator(config)
    return await orchestrator.process_document(file_path)


async def batch_process_documents(
    file_paths: List[str],
    config: Optional[Dict[str, Any]] = None
) -> List[ProcessingResult]:
    """
    æ‰¹é‡å¤„ç†æ–‡æ¡£ï¼ˆä¾¿æ·å‡½æ•°ï¼‰

    Args:
        file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        config: é…ç½®å‚æ•°

    Returns:
        ProcessingResultåˆ—è¡¨
    """
    orchestrator = get_orchestrator(config)
    return await orchestrator.batch_process_documents(file_paths)


__all__ = [
    'UnifiedOrchestrator',
    'get_orchestrator',
    'process_document',
    'batch_process_documents',
    'ProcessingStage',
    'StageResult',
    'ProcessingResult',
    'ProcessingPipeline'
]
