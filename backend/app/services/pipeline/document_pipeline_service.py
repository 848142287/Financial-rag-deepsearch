"""
æ–‡æ¡£å¤„ç†æµæ°´çº¿æœåŠ¡ - å®Œæ•´ç‰ˆ
æ•´åˆæ‰€æœ‰æ­¥éª¤ï¼šè§£æ -> å¤šæ¨¡æ€åˆ†æ -> æ·±åº¦æ±‡æ€» -> å¢å¼ºMarkdown -> çŸ¥è¯†å›¾è°± -> å‘é‡å­˜å‚¨ -> æœ¬åœ°å­˜å‚¨
"""

import asyncio
import json
from datetime import datetime
from pathlib import Path
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
import uuid

from app.core.structured_logging import get_structured_logger
from app.core.config import settings
from app.services.llm_service import LLMService
from app.services.multimodal.engines.qwen_vl_engine import QwenVLEngine
from app.services.enhanced_milvus_service import EnhancedMilvusService
from app.services.minio_service import MinioService

logger = get_structured_logger(__name__)

@dataclass
class PipelineResult:
    """æµæ°´çº¿å¤„ç†ç»“æœ"""
    document_id: str
    filename: str
    file_type: str
    success: bool
    parsing_result: Dict[str, Any] = field(default_factory=dict)
    multimodal_analysis: Dict[str, Any] = field(default_factory=dict)
    deepseek_summary: Dict[str, Any] = field(default_factory=dict)
    enhanced_markdown: str = ""
    knowledge_graph: Dict[str, Any] = field(default_factory=dict)
    vector_storage: Dict[str, Any] = field(default_factory=dict)
    file_storage: Dict[str, Any] = field(default_factory=dict)
    processing_time: float = 0.0
    error_message: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

class DocumentPipelineService:
    """
    æ–‡æ¡£å¤„ç†æµæ°´çº¿æœåŠ¡

    å®Œæ•´æµç¨‹ï¼š
    1. æ–‡æ¡£ä¸Šä¼ å’ŒåŸºç¡€è§£æ (openpyxl/python-pptx)
    2. å¢å¼ºè§£æ (EnhancedExcelParser/PPTParserWrapper)
    3. å¤šæ¨¡æ€åˆ†æ (qwen-vl-plus)
    4. æ·±åº¦æ±‡æ€» (deepseek)
    5. å¢å¼ºMarkdownç”Ÿæˆ
    6. çŸ¥è¯†å›¾è°±æŠ½å– (neo4j)
    7. å‘é‡å­˜å‚¨ (milvus)
    8. æœ¬åœ°æ–‡ä»¶å­˜å‚¨
    """

    def __init__(
        self,
        llm_service: LLMService = None,
        milvus_service: EnhancedMilvusService = None,
        minio_service: MinioService = None
    ):
        self.llm_service = llm_service or LLMService()
        self.milvus_service = milvus_service
        self.minio_service = minio_service

        # åˆå§‹åŒ–å¤šæ¨¡æ€å¼•æ“
        self.qwen_vl_engine = QwenVLEngine()

        # å¤„ç†é…ç½®
        self.enable_multimodal = True
        self.enable_deepseek_summary = True
        self.enable_knowledge_graph = True
        self.enable_vector_storage = True
        self.enable_file_storage = True

        # æœ¬åœ°å­˜å‚¨è·¯å¾„
        self.local_storage_path = Path(settings.file_storage_path) / "processed"
        self.local_storage_path.mkdir(parents=True, exist_ok=True)

        logger.info("=" * 80)
        logger.info("ğŸš€ æ–‡æ¡£å¤„ç†æµæ°´çº¿æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - å¤šæ¨¡æ€åˆ†æ: {'å¯ç”¨' if self.enable_multimodal else 'ç¦ç”¨'}")
        logger.info(f"  - Deepseekæ±‡æ€»: {'å¯ç”¨' if self.enable_deepseek_summary else 'ç¦ç”¨'}")
        logger.info(f"  - çŸ¥è¯†å›¾è°±: {'å¯ç”¨' if self.enable_knowledge_graph else 'ç¦ç”¨'}")
        logger.info(f"  - å‘é‡å­˜å‚¨: {'å¯ç”¨' if self.enable_vector_storage else 'ç¦ç”¨'}")
        logger.info(f"  - æœ¬åœ°å­˜å‚¨: {self.local_storage_path}")
        logger.info("=" * 80)

    async def process_document(
        self,
        file_content: bytes,
        filename: str,
        document_id: str = None,
        options: Dict[str, Any] = None
    ) -> PipelineResult:
        """
        å¤„ç†æ–‡æ¡£ï¼ˆå®Œæ•´æµæ°´çº¿ï¼‰

        Args:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆå­—èŠ‚ï¼‰
            filename: æ–‡ä»¶å
            document_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼‰
            options: å¤„ç†é€‰é¡¹

        Returns:
            PipelineResult: å¤„ç†ç»“æœ
        """
        start_time = asyncio.get_event_loop().time()

        # ç”Ÿæˆæ–‡æ¡£ID
        if not document_id:
            document_id = str(uuid.uuid4())

        # æ–‡ä»¶ç±»å‹
        file_ext = Path(filename).suffix.lower()
        file_type = file_ext[1:] if file_ext else "unknown"

        logger.info("=" * 80)
        logger.info(f"ğŸ“„ å¼€å§‹å¤„ç†æ–‡æ¡£: {filename} (ID: {document_id})")
        logger.info("=" * 80)

        result = PipelineResult(
            document_id=document_id,
            filename=filename,
            file_type=file_type,
            success=False
        )

        try:
            # æ­¥éª¤1: æ–‡æ¡£è§£æ
            logger.info("ğŸ“– [æ­¥éª¤ 1/8] æ–‡æ¡£åŸºç¡€è§£æ...")
            parsing_result = await self._parse_document(file_content, filename, document_id)
            result.parsing_result = parsing_result

            # æ­¥éª¤2: å¤šæ¨¡æ€åˆ†æï¼ˆå¦‚æœæœ‰å›¾ç‰‡ï¼‰
            if self.enable_multimodal and parsing_result.get("images"):
                logger.info("ğŸ¨ [æ­¥éª¤ 2/8] å¤šæ¨¡æ€åˆ†æ...")
                multimodal_result = await self._analyze_multimodal(
                    parsing_result["images"],
                    document_id
                )
                result.multimodal_analysis = multimodal_result
            else:
                logger.info("â­ï¸  [æ­¥éª¤ 2/8] è·³è¿‡å¤šæ¨¡æ€åˆ†æï¼ˆæ— å›¾ç‰‡ï¼‰")
                result.multimodal_analysis = {"status": "skipped", "reason": "no_images"}

            # æ­¥éª¤3: Deepseekæ·±åº¦æ±‡æ€»
            if self.enable_deepseek_summary:
                logger.info("ğŸ§  [æ­¥éª¤ 3/8] Deepseekæ·±åº¦æ±‡æ€»...")
                summary_result = await self._deepseek_summary(
                    parsing_result,
                    result.multimodal_analysis,
                    filename
                )
                result.deepseek_summary = summary_result
            else:
                logger.info("â­ï¸  [æ­¥éª¤ 3/8] è·³è¿‡æ·±åº¦æ±‡æ€»")
                result.deepseek_summary = {"status": "skipped"}

            # æ­¥éª¤4: ç”Ÿæˆå¢å¼ºMarkdown
            logger.info("ğŸ“ [æ­¥éª¤ 4/8] ç”Ÿæˆå¢å¼ºMarkdown...")
            enhanced_markdown = await self._generate_enhanced_markdown(
                parsing_result,
                result.multimodal_analysis,
                result.deepseek_summary,
                filename
            )
            result.enhanced_markdown = enhanced_markdown

            # æ­¥éª¤5: çŸ¥è¯†å›¾è°±æŠ½å–
            if self.enable_knowledge_graph:
                logger.info("ğŸ•¸ï¸  [æ­¥éª¤ 5/8] çŸ¥è¯†å›¾è°±æŠ½å–...")
                kg_result = await self._extract_knowledge_graph(
                    enhanced_markdown,
                    document_id,
                    filename
                )
                result.knowledge_graph = kg_result
            else:
                logger.info("â­ï¸  [æ­¥éª¤ 5/8] è·³è¿‡çŸ¥è¯†å›¾è°±æŠ½å–")
                result.knowledge_graph = {"status": "skipped"}

            # æ­¥éª¤6: å‘é‡å­˜å‚¨ï¼ˆåŸæœ‰ï¼‰
            if self.enable_vector_storage and self.milvus_service:
                logger.info("ğŸ” [æ­¥éª¤ 6/8] å‘é‡å­˜å‚¨...")
                vector_result = await self._store_vectors(
                    enhanced_markdown,
                    document_id,
                    filename,
                    parsing_result
                )
                result.vector_storage = vector_result
            else:
                logger.info("â­ï¸  [æ­¥éª¤ 6/8] è·³è¿‡å‘é‡å­˜å‚¨")
                result.vector_storage = {"status": "skipped"}

            # æ­¥éª¤7: åˆ†å±‚ç´¢å¼•æ„å»ºï¼ˆæ–°å¢ï¼‰
            logger.info("ğŸ“š [æ­¥éª¤ 7/8] åˆ†å±‚ç´¢å¼•æ„å»º...")
            try:
                from app.services.hierarchical_index import get_hierarchical_index_pipeline_integration
                pipeline_integration = get_hierarchical_index_pipeline_integration()

                hierarchical_index = await pipeline_integration.build_index_from_pipeline(
                    document_id=document_id,
                    markdown_content=parsing_result.get("markdown_content", ""),
                    deepseek_summary=result.deepseek_summary
                )

                result.hierarchical_index = {
                    "status": "success",
                    "total_chapters": len(hierarchical_index.chapters),
                    "total_chunks": len(hierarchical_index.chunks),
                    "processing_time": hierarchical_index.processing_time
                }

                logger.info(f"  âœ… åˆ†å±‚ç´¢å¼•æ„å»ºæˆåŠŸ: ç« èŠ‚={len(hierarchical_index.chapters)}, ç‰‡æ®µ={len(hierarchical_index.chunks)}")

            except Exception as e:
                logger.warning(f"âš ï¸ åˆ†å±‚ç´¢å¼•æ„å»ºå¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {str(e)}")
                result.hierarchical_index = {
                    "status": "failed",
                    "error": str(e)
                }

            # æ­¥éª¤8: æœ¬åœ°æ–‡ä»¶å­˜å‚¨
            if self.enable_file_storage:
                logger.info("ğŸ’¾ [æ­¥éª¤ 8/8] æœ¬åœ°æ–‡ä»¶å­˜å‚¨...")
                file_result = await self._store_locally(
                    result,
                    document_id,
                    filename
                )
                result.file_storage = file_result
            else:
                logger.info("â­ï¸  [æ­¥éª¤ 8/8] è·³è¿‡æœ¬åœ°æ–‡ä»¶å­˜å‚¨")
                result.file_storage = {"status": "skipped"}

            # è®¡ç®—å¤„ç†æ—¶é—´
            result.processing_time = asyncio.get_event_loop().time() - start_time
            result.success = True

            logger.info("=" * 80)
            logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {result.processing_time:.2f}ç§’")
            logger.info("=" * 80)

            return result

        except Exception as e:
            result.processing_time = asyncio.get_event_loop().time() - start_time
            result.error_message = str(e)
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
            return result

    async def _parse_document(
        self,
        file_content: bytes,
        filename: str,
        document_id: str
    ) -> Dict[str, Any]:
        """æ­¥éª¤1: æ–‡æ¡£è§£æ"""
        from app.services.parsing.document_parsing_service import DocumentParsingService

        # åˆ›å»ºè§£ææœåŠ¡å®ä¾‹
        parsing_service = DocumentParsingService(services={})

        # è§£ææ–‡æ¡£
        text_content, markdown_content, parse_result = await parsing_service.parse_document(
            file_content=file_content,
            filename=filename,
            document_id=document_id
        )

        # æå–å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        images = parse_result.get("images", [])

        return {
            "text_content": text_content,
            "markdown_content": markdown_content,
            "parse_result": parse_result,
            "images": images,
            "metadata": {
                "filename": filename,
                "document_id": document_id,
                "parsed_at": datetime.now().isoformat()
            }
        }

    async def _analyze_multimodal(
        self,
        images: List[str],
        document_id: str
    ) -> Dict[str, Any]:
        """æ­¥éª¤2: å¤šæ¨¡æ€åˆ†æ"""
        try:
            # æ‰¹é‡åˆ†æå›¾ç‰‡
            results = await self.qwen_vl_engine.analyze_images_batch(
                image_paths=images,
                document_id=document_id,
                analysis_type="general"
            )

            # æ•´åˆç»“æœ
            all_analyses = []
            for i, result in enumerate(results):
                if "error" not in result:
                    all_analyses.append({
                        "image_path": images[i],
                        "analysis": result.get("full_analysis", ""),
                        "metadata": result.get("metadata", {})
                    })

            return {
                "status": "success",
                "analyzed_count": len(all_analyses),
                "total_images": len(images),
                "analyses": all_analyses
            }

        except Exception as e:
            logger.error(f"å¤šæ¨¡æ€åˆ†æå¤±è´¥: {str(e)}")
            return {
                "status": "failed",
                "error": str(e),
                "analyzed_count": 0,
                "total_images": len(images)
            }

    async def _deepseek_summary(
        self,
        parsing_result: Dict[str, Any],
        multimodal_analysis: Dict[str, Any],
        filename: str
    ) -> Dict[str, Any]:
        """æ­¥éª¤3: Deepseekæ·±åº¦æ±‡æ€»ï¼ˆåŸºäºè§„åˆ™æ±‡æ€» + Deepseekæ£€æŸ¥ï¼‰"""
        try:
            # æ­¥éª¤3.1: åŸºäºè§„åˆ™çš„ç« èŠ‚æ±‡æ€»
            logger.info("  ğŸ“‹ åŸºäºè§„åˆ™çš„ç« èŠ‚æ±‡æ€»...")
            rule_based_summary = self._rule_based_summary(
                parsing_result,
                multimodal_analysis
            )

            # æ­¥éª¤3.2: Deepseekæ£€æŸ¥å’Œå¢å¼º
            logger.info("  ğŸ¤– Deepseekæ£€æŸ¥å’Œå¢å¼º...")
            enhanced_summary = await self._deepseek_enhance(
                rule_based_summary,
                parsing_result
            )

            return {
                "status": "success",
                "rule_based_summary": rule_based_summary,
                "enhanced_summary": enhanced_summary,
                "model": "deepseek-chat (enhancement)",
                "created_at": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"Deepseekæ±‡æ€»å¤±è´¥: {str(e)}")
            return {
                "status": "failed",
                "error": str(e)
            }

    def _rule_based_summary(
        self,
        parsing_result: Dict[str, Any],
        multimodal_analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        åŸºäºè§„åˆ™çš„ç« èŠ‚æ±‡æ€»
        å¿«é€Ÿæå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå‡å°‘å¯¹LLMçš„ä¾èµ–
        """
        markdown_content = parsing_result.get("markdown_content", "")

        summary = {
            "sections": [],
            "key_points": [],
            "statistics": {},
            "entities": [],
            "images_analysis": []
        }

        # 1. æŒ‰ç« èŠ‚æ‹†åˆ†
        sections = self._extract_sections(markdown_content)
        summary["sections"] = sections

        # 2. æå–å…³é”®ä¿¡æ¯
        for section in sections:
            # æå–æ•°å­—å’Œç»Ÿè®¡æ•°æ®
            numbers = self._extract_numbers(section["content"])
            if numbers:
                summary["key_points"].extend(numbers)

            # æå–æ—¥æœŸ
            dates = self._extract_dates(section["content"])
            if dates:
                summary["key_points"].extend(dates)

            # æå–å…³é”®è¯
            keywords = self._extract_keywords(section["content"])
            if keywords:
                summary["entities"].extend(keywords)

        # 3. ç»Ÿè®¡ä¿¡æ¯
        summary["statistics"] = {
            "total_sections": len(sections),
            "total_words": len(markdown_content),
            "has_images": multimodal_analysis.get("status") == "success",
            "image_count": multimodal_analysis.get("analyzed_count", 0)
        }

        # 4. å›¾ç‰‡åˆ†ææ±‡æ€»
        if multimodal_analysis.get("status") == "success":
            for analysis in multimodal_analysis.get("analyses", []):
                summary["images_analysis"].append({
                    "image": analysis.get("image_path", ""),
                    "summary": analysis.get("analysis", "")[:200]
                })

        return summary

    def _extract_sections(self, markdown_content: str) -> List[Dict[str, Any]]:
        """æå–ç« èŠ‚ç»“æ„"""
        import re

        sections = []
        lines = markdown_content.split('\n')

        current_section = {"title": "æ¦‚è¿°", "level": 0, "content": []}

        for line in lines:
            # æ£€æµ‹æ ‡é¢˜
            if line.startswith('#'):
                # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                if current_section["content"]:
                    current_section["content"] = '\n'.join(current_section["content"])
                    sections.append(current_section)

                # åˆ›å»ºæ–°ç« èŠ‚
                level = len(line) - len(line.lstrip('#'))
                title = line.strip('#').strip()
                current_section = {
                    "title": title,
                    "level": level,
                    "content": []
                }
            else:
                current_section["content"].append(line)

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section["content"]:
            current_section["content"] = '\n'.join(current_section["content"])
            sections.append(current_section)

        return sections

    def _extract_numbers(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ•°å­—å’Œç»Ÿè®¡ä¿¡æ¯"""
        import re

        # åŒ¹é…æ•°å­—ï¼ˆåŒ…æ‹¬ç™¾åˆ†æ¯”ã€é‡‘é¢ç­‰ï¼‰
        patterns = [
            r'(\d+\.?\d*)%',  # ç™¾åˆ†æ¯”
            r'(\d+\.?\d*)\s*(ä¸‡|äº¿|åƒ)ä¸‡?',  # ä¸­æ–‡å•ä½
            r'\$?(\d{1,3}(,\d{3})*(\.\d+)?)',  # é‡‘é¢
            r'(\d{4}å¹´?\d{1,2}æœˆ?\d{1,2}æ—¥?)'  # æ—¥æœŸ
        ]

        numbers = []
        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                numbers.append({
                    "type": "number",
                    "value": str(match[0] if isinstance(match, tuple) else match),
                    "context": text[max(0, text.find(str(match))-20):text.find(str(match))+50]
                })

        return numbers[:10]  # é™åˆ¶è¿”å›æ•°é‡

    def _extract_dates(self, text: str) -> List[Dict[str, Any]]:
        """æå–æ—¥æœŸ"""
        import re

        date_patterns = [
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',
            r'(\d{4})-(\d{1,2})-(\d{1,2})',
            r'(\d{1,2})/(\d{1,2})/(\d{4})'
        ]

        dates = []
        for pattern in date_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                dates.append({
                    "type": "date",
                    "value": str(match),
                    "context": text[max(0, text.find(str(match))-20):text.find(str(match))+50]
                })

        return dates[:5]

    def _extract_keywords(self, text: str) -> List[Dict[str, Any]]:
        """æå–å…³é”®è¯ï¼ˆç®€å•çš„è¯é¢‘ç»Ÿè®¡ï¼‰"""
        import re
        from collections import Counter

        # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)

        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(chinese_words)

        # è¿”å›é«˜é¢‘è¯
        keywords = []
        for word, freq in word_freq.most_common(10):
            if freq >= 2:  # è‡³å°‘å‡ºç°2æ¬¡
                keywords.append({
                    "type": "keyword",
                    "word": word,
                    "frequency": freq
                })

        return keywords

    async def _deepseek_enhance(
        self,
        rule_based_summary: Dict[str, Any],
        parsing_result: Dict[str, Any]
    ) -> str:
        """
        Deepseekæ£€æŸ¥å’Œå¢å¼º
        ä¸»è¦ä»»åŠ¡ï¼š
        1. æ£€æŸ¥è§„åˆ™æ±‡æ€»çš„å‡†ç¡®æ€§
        2. è¯†åˆ«å¤æ‚çš„å…³ç³»å’Œæ¨¡å¼
        3. æä¾›æ´å¯Ÿå’Œå»ºè®®
        """
        prompt = f"""è¯·æ£€æŸ¥å’Œå¢å¼ºä»¥ä¸‹åŸºäºè§„åˆ™æå–çš„æ–‡æ¡£æ‘˜è¦ï¼š

## è§„åˆ™æå–ç»“æœ
ç« èŠ‚æ•°é‡: {rule_based_summary['statistics'].get('total_sections', 0)}
å›¾ç‰‡æ•°é‡: {rule_based_summary['statistics'].get('image_count', 0)}

### ä¸»è¦ç« èŠ‚
{chr(10).join([f"- {s['title']}" for s in rule_based_summary.get('sections', [])[:5]])}

### å…³é”®ä¿¡æ¯æ ·æœ¬
{json.dumps(rule_based_summary.get('key_points', [])[:5], ensure_ascii=False)}

## è¦æ±‚
è¯·æä¾›ï¼š
1. **å‡†ç¡®æ€§æ£€æŸ¥**ï¼šä¸Šè¿°æå–ä¿¡æ¯æ˜¯å¦æœ‰æ˜æ˜¾é”™è¯¯ï¼Ÿ
2. **å…³é”®æ´å¯Ÿ**ï¼šä»è¿™äº›ä¿¡æ¯ä¸­èƒ½å‘ç°ä»€ä¹ˆé‡è¦è¶‹åŠ¿æˆ–é—®é¢˜ï¼Ÿ
3. **æ”¹è¿›å»ºè®®**ï¼šè¿˜éœ€è¦è¡¥å……å“ªäº›å…³é”®ä¿¡æ¯ï¼Ÿ
4. **é£é™©æç¤º**ï¼šæœ‰ä»€ä¹ˆéœ€è¦ç‰¹åˆ«æ³¨æ„çš„é£é™©ç‚¹ï¼Ÿ

è¯·ç”¨ç®€æ´çš„Markdownæ ¼å¼è¾“å‡ºï¼Œé‡ç‚¹çªå‡ºé—®é¢˜å’Œå»ºè®®ã€‚"""

        enhanced = await self.llm_service.simple_chat(
            prompt=prompt,
            system_prompt="ä½ æ˜¯æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œè´Ÿè´£æ£€æŸ¥å’Œä¼˜åŒ–åŸºäºè§„åˆ™æå–çš„ä¿¡æ¯ã€‚",
            temperature=0.3
        )

        return enhanced

    def _get_summary_system_prompt(self) -> str:
        """è·å–æ±‡æ€»ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ–‡æ¡£åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œæ·±åº¦åˆ†æå’Œæ±‡æ€»ã€‚

è¯·æä¾›ï¼š
1. æ–‡æ¡£æ ¸å¿ƒå†…å®¹æ¦‚è¿°
2. å…³é”®æ•°æ®å’Œä¿¡æ¯æå–
3. é‡è¦ç»“è®ºå’Œè§‚ç‚¹
4. é£é™©æç¤ºå’Œæ³¨æ„äº‹é¡¹

è¾“å‡ºæ ¼å¼ï¼š
- ä½¿ç”¨æ¸…æ™°çš„æ ‡é¢˜å’Œåˆ†æ®µ
- çªå‡ºå…³é”®ä¿¡æ¯
- ä¿æŒå®¢è§‚å’Œä¸“ä¸š"""

    def _build_summary_prompt(
        self,
        parsing_result: Dict[str, Any],
        multimodal_analysis: Dict[str, Any]
    ) -> str:
        """æ„é€ æ±‡æ€»æç¤ºè¯"""
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡æ¡£å†…å®¹è¿›è¡Œæ·±åº¦åˆ†ææ±‡æ€»ï¼š

## æ–‡æ¡£å†…å®¹
{parsing_result.get('markdown_content', '')[:3000]}

## å›¾ç‰‡åˆ†æ
"""

        if multimodal_analysis.get("status") == "success":
            for analysis in multimodal_analysis.get("analyses", [])[:5]:
                prompt += f"\n{analysis.get('analysis', '')}\n"

        prompt += """

## è¦æ±‚
è¯·æä¾›ï¼š
1. æ–‡æ¡£æ¦‚è¿°ï¼ˆä¸»è¦å†…å®¹å’Œç›®çš„ï¼‰
2. å…³é”®ä¿¡æ¯æå–ï¼ˆé‡è¦æ•°æ®ã€æ—¶é—´ã€äººç‰©ã€äº‹ä»¶ï¼‰
3. æ ¸å¿ƒè§‚ç‚¹å’Œç»“è®º
4. éœ€è¦å…³æ³¨çš„é‡ç‚¹

è¯·ç”¨æ¸…æ™°çš„Markdownæ ¼å¼è¾“å‡ºã€‚"""

        return prompt

    async def _generate_enhanced_markdown(
        self,
        parsing_result: Dict[str, Any],
        multimodal_analysis: Dict[str, Any],
        deepseek_summary: Dict[str, Any],
        filename: str
    ) -> str:
        """æ­¥éª¤4: ç”Ÿæˆå¢å¼ºMarkdown"""
        markdown_parts = []

        # æ ‡é¢˜
        markdown_parts.append(f"# {filename}\n")
        markdown_parts.append(f"**å¤„ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        markdown_parts.append("---\n\n")

        # åŸå§‹å†…å®¹
        markdown_parts.append("## åŸå§‹å†…å®¹\n\n")
        markdown_parts.append(parsing_result.get("markdown_content", "")[:5000])
        markdown_parts.append("\n\n---\n\n")

        # å¤šæ¨¡æ€åˆ†æ
        if multimodal_analysis.get("status") == "success":
            markdown_parts.append("## å›¾ç‰‡åˆ†æ\n\n")
            for i, analysis in enumerate(multimodal_analysis.get("analyses", []), 1):
                markdown_parts.append(f"### å›¾ç‰‡ {i}\n\n")
                markdown_parts.append(f"{analysis.get('analysis', '')}\n\n")

        # Deepseekæ±‡æ€»
        if deepseek_summary.get("status") == "success":
            markdown_parts.append("## AIæ·±åº¦æ±‡æ€»\n\n")
            markdown_parts.append(deepseek_summary.get("summary", ""))
            markdown_parts.append("\n\n")

        return "\n".join(markdown_parts)

    async def _extract_knowledge_graph(
        self,
        enhanced_markdown: str,
        document_id: str,
        filename: str
    ) -> Dict[str, Any]:
        """æ­¥éª¤5: çŸ¥è¯†å›¾è°±æŠ½å–"""
        try:
            # ä½¿ç”¨DeepseekæŠ½å–å®ä½“å’Œå…³ç³»
            prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æ¡£ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

## æ–‡æ¡£å†…å®¹
{enhanced_markdown[:4000]}

## è¦æ±‚
è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
  "entities": [
    {{"name": "å®ä½“åç§°", "type": "ç±»å‹ï¼ˆCompany/Person/Stockç­‰ï¼‰", "confidence": 0.9}}
  ],
  "relations": [
    {{"source": "å®ä½“1", "target": "å®ä½“2", "type": "å…³ç³»ç±»å‹", "confidence": 0.8}}
  ]
}}"""

            kg_data = await self.llm_service.structured_completion(
                prompt=prompt,
                system_prompt="ä½ æ˜¯çŸ¥è¯†å›¾è°±æŠ½å–ä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«å®ä½“å’Œå…³ç³»ã€‚",
                schema={
                    "entities": "å®ä½“åˆ—è¡¨",
                    "relations": "å…³ç³»åˆ—è¡¨"
                }
            )

            # TODO: å­˜å‚¨åˆ°Neo4j
            # ç›®å‰Neo4jæœåŠ¡è¢«ç¦ç”¨ï¼Œå…ˆè¿”å›æ•°æ®

            return {
                "status": "success",
                "entities": kg_data.get("entities", []),
                "relations": kg_data.get("relations", []),
                "entity_count": len(kg_data.get("entities", [])),
                "relation_count": len(kg_data.get("relations", [])),
                "note": "Neo4jå­˜å‚¨æœªå¯ç”¨ï¼Œä»…è¿”å›æŠ½å–ç»“æœ"
            }

        except Exception as e:
            logger.error(f"çŸ¥è¯†å›¾è°±æŠ½å–å¤±è´¥: {str(e)}")
            return {
                "status": "failed",
                "error": str(e)
            }

    async def _store_vectors(
        self,
        enhanced_markdown: str,
        document_id: str,
        filename: str,
        parsing_result: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ­¥éª¤6: å‘é‡å­˜å‚¨"""
        try:
            # TODO: å®ç°å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°Milvus
            # å½“å‰MilvusæœåŠ¡å­˜åœ¨ï¼Œä½†éœ€è¦å®ç°embedding

            return {
                "status": "success",
                "note": "å‘é‡å­˜å‚¨åŠŸèƒ½å¾…å®Œå–„",
                "document_id": document_id
            }

        except Exception as e:
            logger.error(f"å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")
            return {
                "status": "failed",
                "error": str(e)
            }

    async def _store_locally(
        self,
        result: PipelineResult,
        document_id: str,
        filename: str
    ) -> Dict[str, Any]:
        """æ­¥éª¤7: æœ¬åœ°æ–‡ä»¶å­˜å‚¨"""
        try:
            # åˆ›å»ºæ–‡æ¡£ç›®å½•
            doc_dir = self.local_storage_path / document_id
            doc_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜å¢å¼ºMarkdown
            markdown_path = doc_dir / f"{filename}.md"
            with open(markdown_path, 'w', encoding='utf-8') as f:
                f.write(result.enhanced_markdown)

            # ä¿å­˜å®Œæ•´ç»“æœJSON
            json_path = doc_dir / f"{filename}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "document_id": document_id,
                    "filename": filename,
                    "processing_time": result.processing_time,
                    "parsing_result": result.parsing_result,
                    "multimodal_analysis": result.multimodal_analysis,
                    "deepseek_summary": result.deepseek_summary,
                    "knowledge_graph": result.knowledge_graph,
                    "vector_storage": result.vector_storage,
                    "processed_at": datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)

            return {
                "status": "success",
                "markdown_path": str(markdown_path),
                "json_path": str(json_path),
                "document_dir": str(doc_dir)
            }

        except Exception as e:
            logger.error(f"æœ¬åœ°å­˜å‚¨å¤±è´¥: {str(e)}")
            return {
                "status": "failed",
                "error": str(e)
            }

# å…¨å±€å•ä¾‹
_document_pipeline_service = None

def get_document_pipeline_service() -> DocumentPipelineService:
    """è·å–æ–‡æ¡£æµæ°´çº¿æœåŠ¡å•ä¾‹"""
    global _document_pipeline_service
    if _document_pipeline_service is None:
        _document_pipeline_service = DocumentPipelineService()
    return _document_pipeline_service
