"""
å¢å¼ºæŸ¥è¯¢å¤„ç†å™¨
å€Ÿé‰´DocMindé¡¹ç›®çš„æŸ¥è¯¢å¤„ç†ç­–ç•¥ï¼ŒåŒ…æ‹¬ï¼š
1. ä¸Šä¸‹æ–‡é‡å†™ï¼ˆè§£å†³æŒ‡ä»£é—®é¢˜ï¼‰
2. åŒè½¨æŸ¥è¯¢é‡å†™ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
3. HyDEç”Ÿæˆï¼ˆå‡è®¾æ–‡æ¡£ï¼‰
4. å…ƒé—®é¢˜è¯†åˆ«
"""

import re
from dataclasses import dataclass
from app.core.structured_logging import get_structured_logger
from app.services.llm.unified_llm_service import get_unified_llm_service_initialized, LLMModel

logger = get_structured_logger(__name__)

@dataclass
class ProcessedQuery:
    """å¤„ç†åçš„æŸ¥è¯¢"""
    original_query: str
    standalone_query: str  # ç‹¬ç«‹æŸ¥è¯¢ï¼ˆä¸Šä¸‹æ–‡é‡å†™åï¼‰
    vector_query: str  # å‘é‡æ£€ç´¢æŸ¥è¯¢
    keywords: List[str]  # BM25å…³é”®è¯
    hyde_doc: str  # HyDEç”Ÿæˆçš„å‡è®¾æ–‡æ¡£
    is_meta_question: bool  # æ˜¯å¦æ˜¯å…ƒé—®é¢˜
    direct_answer: Optional[str]  # å…ƒé—®é¢˜çš„ç›´æ¥å›ç­”

class EnhancedQueryProcessor:
    """
    å¢å¼ºæŸ¥è¯¢å¤„ç†å™¨

    åŠŸèƒ½ï¼š
    1. å…ƒé—®é¢˜è¯†åˆ«ä¸ç›´æ¥å›ç­”
    2. ä¸Šä¸‹æ–‡é‡å†™ï¼ˆå¤„ç†å¤šè½®å¯¹è¯çš„æŒ‡ä»£ï¼‰
    3. åŒè½¨æŸ¥è¯¢é‡å†™ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
    4. HyDEç”Ÿæˆï¼ˆå‡è®¾æ€§æ–‡æ¡£ï¼‰
    """

    def __init__(
        self,
        enable_context_rewrite: bool = True,
        enable_query_rewrite: bool = True,
        enable_hyde: bool = True,
        max_history_turns: int = 3
    ):
        """
        åˆå§‹åŒ–æŸ¥è¯¢å¤„ç†å™¨

        Args:
            enable_context_rewrite: æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡é‡å†™
            enable_query_rewrite: æ˜¯å¦å¯ç”¨æŸ¥è¯¢é‡å†™
            enable_hyde: æ˜¯å¦å¯ç”¨HyDE
            max_history_turns: æœ€å¤§å†å²è½®æ•°
        """
        self.enable_context_rewrite = enable_context_rewrite
        self.enable_query_rewrite = enable_query_rewrite
        self.enable_hyde = enable_hyde
        self.max_history_turns = max_history_turns

        logger.info(
            f"EnhancedQueryProcessoråˆå§‹åŒ–: "
            f"context_rewrite={enable_context_rewrite}, "
            f"query_rewrite={enable_query_rewrite}, "
            f"hyde={enable_hyde}"
        )

    async def process(
        self,
        query: str,
        history: List[Dict[str, str]] = None
    ) -> ProcessedQuery:
        """
        å¤„ç†æŸ¥è¯¢

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            history: å¯¹è¯å†å² [{"role": "user", "content": "...}, ...]

        Returns:
            ProcessedQuery
        """
        history = history or []

        # 1. å…ƒé—®é¢˜è¯†åˆ«
        is_meta, direct_answer = self._identify_meta_question(query, history)

        if is_meta:
            logger.info(f"âœ… è¯†åˆ«ä¸ºå…ƒé—®é¢˜ï¼Œç›´æ¥å›ç­”")
            return ProcessedQuery(
                original_query=query,
                standalone_query=query,
                vector_query=query,
                keywords=[],
                hyde_doc="",
                is_meta_question=True,
                direct_answer=direct_answer
            )

        # 2. ä¸Šä¸‹æ–‡é‡å†™
        standalone_query = query
        if self.enable_context_rewrite and history:
            standalone_query = await self._rewrite_context(query, history)
            logger.debug(f"ä¸Šä¸‹æ–‡é‡å†™: {query} -> {standalone_query}")

        # 3. åŒè½¨æŸ¥è¯¢é‡å†™
        vector_query, keywords = standalone_query, []
        if self.enable_query_rewrite:
            vector_query, keywords = await self._rewrite_query(standalone_query)
            logger.debug(f"æŸ¥è¯¢é‡å†™: vector={vector_query}, keywords={keywords}")

        # 4. HyDEç”Ÿæˆ
        hyde_doc = ""
        if self.enable_hyde:
            hyde_doc = await self._generate_hyde(standalone_query)
            logger.debug(f"HyDEç”Ÿæˆ: {hyde_doc[:100]}...")

        return ProcessedQuery(
            original_query=query,
            standalone_query=standalone_query,
            vector_query=vector_query,
            keywords=keywords,
            hyde_doc=hyde_doc,
            is_meta_question=False,
            direct_answer=None
        )

    def _identify_meta_question(
        self,
        query: str,
        history: List[Dict[str, str]]
    ) -> Tuple[bool, Optional[str]]:
        """
        è¯†åˆ«å…ƒé—®é¢˜

        å…ƒé—®é¢˜åŒ…æ‹¬ï¼š
        - èº«ä»½è¯¢é—®ï¼š"ä½ æ˜¯è°"ã€"ä½ å«ä»€ä¹ˆ"
        - å†å²è¯¢é—®ï¼š"æˆ‘åˆšåˆšé—®äº†ä»€ä¹ˆ"
        - ç³»ç»Ÿè¯¢é—®ï¼š"ä½ èƒ½åšä»€ä¹ˆ"
        """
        query_lower = query.lower()

        # èº«ä»½è¯¢é—®
        identity_patterns = [
            "ä½ æ˜¯è°", "ä½ å«ä»€ä¹ˆ", "ä½ çš„åå­—", "self intro"
        ]
        if any(p in query_lower for p in identity_patterns):
            return True, (
                "æˆ‘æ˜¯é‡‘èRAGæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“æ³¨äºé‡‘èæ–‡æ¡£åˆ†æå’Œé—®ç­”ã€‚\n"
                "æˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n"
                "â€¢ åˆ†æè´¢åŠ¡æŠ¥å‘Šå’Œè´¢æŠ¥\n"
                "â€¢ æœç´¢é‡‘èçŸ¥è¯†\n"
                "â€¢ å›ç­”æŠ•èµ„ç›¸å…³é—®é¢˜\n"
                "â€¢ æä¾›å¸‚åœºæ•°æ®åˆ†æ"
            )

        # å†å²è¯¢é—®
        history_patterns = ["æˆ‘åˆšåˆšé—®", "ä¸Šä¸€ä¸ªé—®é¢˜", "åˆšæ‰è¯´çš„"]
        if any(p in query_lower for p in history_patterns):
            if history:
                last_question = next(
                    (h["content"] for h in reversed(history) if h["role"] == "user"),
                    "æ²¡æœ‰æ‰¾åˆ°ä¹‹å‰çš„é—®é¢˜"
                )
                return True, f"ä½ åˆšåˆšé—®çš„æ˜¯ï¼š{last_question}"
            else:
                return True, "è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬ä¸€æ¬¡å¯¹è¯"

        # èƒ½åŠ›è¯¢é—®
        capability_patterns = ["ä½ èƒ½åšä»€ä¹ˆ", "æœ‰ä»€ä¹ˆåŠŸèƒ½", "help"]
        if any(p in query_lower for p in capability_patterns):
            return True, (
                "æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š\n"
                "1. ğŸ“Š åˆ†æè´¢åŠ¡æŠ¥å‘Šå’Œè´¢æŠ¥æ•°æ®\n"
                "2. ğŸ” æœç´¢é‡‘èçŸ¥è¯†å’Œç›‘ç®¡æ–‡ä»¶\n"
                "3. ğŸ“ˆ æä¾›å¸‚åœºè¶‹åŠ¿åˆ†æ\n"
                "4. ğŸ’¡ å›ç­”æŠ•èµ„ç›¸å…³é—®é¢˜\n"
                "5. ğŸ“ ç”Ÿæˆé‡‘èæ‘˜è¦å’ŒæŠ¥å‘Š\n\n"
                "è¯·ä¸Šä¼ æ‚¨çš„é‡‘èæ–‡æ¡£æˆ–ç›´æ¥æé—®ï¼"
            )

        return False, None

    async def _rewrite_context(
        self,
        query: str,
        history: List[Dict[str, str]]
    ) -> str:
        """
        ä¸Šä¸‹æ–‡é‡å†™

        å°†åŒ…å«æŒ‡ä»£çš„æŸ¥è¯¢é‡å†™ä¸ºç‹¬ç«‹å®Œæ•´çš„æŸ¥è¯¢
        ä¾‹å¦‚ï¼š"å®ƒçš„å¢é•¿ç‡æ˜¯å¤šå°‘ï¼Ÿ" -> "è‹¹æœå…¬å¸çš„å¢é•¿ç‡æ˜¯å¤šå°‘ï¼Ÿ"
        """
        # è·å–æœ€è¿‘Nè½®å¯¹è¯
        recent_history = history[-self.max_history_turns:]

        # æ„å»ºå†å²æ–‡æœ¬
        history_text = "\n".join([
            f"{h['role']}: {h['content']}"
            for h in recent_history
        ])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢é‡å†™ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„æœ€æ–°é—®é¢˜é‡å†™ä¸ºç‹¬ç«‹å®Œæ•´çš„æŸ¥è¯¢ã€‚

**è¦æ±‚**ï¼š
1. å¦‚æœé—®é¢˜ä¸­åŒ…å«ä»£è¯ï¼ˆ"å®ƒ"ã€"è¿™ä¸ª"ã€"é‚£ä¸ª"ï¼‰ï¼Œè¯·æ ¹æ®å¯¹è¯å†å²æ›¿æ¢ä¸ºå…·ä½“çš„åè¯
2. ä¿æŒé—®é¢˜çš„åŸæ„
3. ä½¿é—®é¢˜å¯ä»¥ç‹¬ç«‹ç†è§£ï¼Œä¸ä¾èµ–å†å²å¯¹è¯

**å¯¹è¯å†å²**ï¼š
{history_text}

**ç”¨æˆ·æœ€æ–°é—®é¢˜**ï¼š{query}

**é‡å†™åçš„ç‹¬ç«‹æŸ¥è¯¢**ï¼š"""

        try:
            llm = await get_unified_llm_service_initialized()
            response = await llm.chat(
                prompt=prompt,
                model=LLMModel.DEEPSEEK_CHAT,
                temperature=0.3,
                max_tokens=200
            )

            rewritten = response.content.strip()
            # å¦‚æœé‡å†™å¤±è´¥æˆ–ä¸ºç©ºï¼Œè¿”å›åŸæŸ¥è¯¢
            if not rewritten or len(rewritten) < len(query) // 2:
                return query

            return rewritten

        except Exception as e:
            logger.warning(f"ä¸Šä¸‹æ–‡é‡å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæŸ¥è¯¢")
            return query

    async def _rewrite_query(self, query: str) -> Tuple[str, List[str]]:
        """
        åŒè½¨æŸ¥è¯¢é‡å†™

        ç”Ÿæˆä¸¤éƒ¨åˆ†å†…å®¹ï¼š
        1. Vector Query: é€‚åˆå‘é‡æ£€ç´¢çš„æŸ¥è¯¢ï¼ˆé€»è¾‘å®Œæ•´ã€å»å£è¯­åŒ–ï¼‰
        2. Keywords: é€‚åˆBM25çš„å…³é”®è¯ï¼ˆ3-5ä¸ªæ ¸å¿ƒè¯ï¼‰
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªé‡‘èé¢†åŸŸçš„æŸ¥è¯¢ä¼˜åŒ–ä¸“å®¶ã€‚è¯·å°†ç”¨æˆ·çš„æŸ¥è¯¢é‡å†™ä¸ºä¸¤éƒ¨åˆ†ã€‚

**è¦æ±‚**ï¼š
1. [Vector] é€»è¾‘å®Œæ•´ã€å»å£è¯­åŒ–çš„ä¸“ä¸šé™ˆè¿°å¥
2. [Keywords] æå–3-5ä¸ªæ ¸å¿ƒå…³é”®è¯ï¼Œç”¨äºç²¾ç¡®åŒ¹é…ï¼ˆå…³é”®è¯ä¹‹é—´ç”¨é€—å·åˆ†éš”ï¼‰

**ç”¨æˆ·æŸ¥è¯¢**ï¼š{query}

**è¾“å‡ºæ ¼å¼**ï¼š
[Vector] è¥ä¸šæ”¶å…¥ã€å‡€åˆ©æ¶¦çš„åŒæ¯”å¢é•¿ç‡
[Keywords] è¥æ”¶, å‡€åˆ©æ¶¦, åŒæ¯”å¢é•¿

**è¯·å¼€å§‹é‡å†™**ï¼š"""

        try:
            llm = await get_unified_llm_service_initialized()
            response = await llm.chat(
                prompt=prompt,
                model=LLMModel.DEEPSEEK_CHAT,
                temperature=0.3,
                max_tokens=200
            )

            result = response.content.strip()

            # è§£æè¾“å‡º
            vector_query = query  # é»˜è®¤å€¼
            keywords = []

            # æå– [Vector] éƒ¨åˆ†
            vector_match = re.search(r'\[Vector\]\s*(.+?)(?:\[Keywords\]|$)', result, re.DOTALL)
            if vector_match:
                vector_query = vector_match.group(1).strip()

            # æå– [Keywords] éƒ¨åˆ†
            keywords_match = re.search(r'\[Keywords\]\s*(.+)', result)
            if keywords_match:
                keywords_str = keywords_match.group(1).strip()
                keywords = [k.strip() for k in re.split(r'[,ï¼Œã€]', keywords_str) if k.strip()]

            # å¦‚æœæ²¡æœ‰æå–åˆ°å…³é”®è¯ï¼Œä½¿ç”¨åŸæŸ¥è¯¢çš„åˆ†è¯
            if not keywords:
                keywords = self._extract_keywords(query)

            logger.debug(f"æŸ¥è¯¢é‡å†™ç»“æœ: vector={vector_query}, keywords={keywords}")

            return vector_query, keywords

        except Exception as e:
            logger.warning(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæŸ¥è¯¢")
            return query, self._extract_keywords(query)

    def _extract_keywords(self, query: str) -> List[str]:
        """
        ç®€å•çš„å…³é”®è¯æå–

        ä½¿ç”¨jiebaåˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
        """
        try:
            import jieba

            # é‡‘èé¢†åŸŸåœç”¨è¯
            stopwords = {
                "å¦‚ä½•", "æ€ä¹ˆ", "ä»€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å“ªäº›", "å¤šå°‘",
                "çš„", "äº†", "å—", "å‘¢", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸",
                "æˆ‘", "ä½ ", "ä»–", "å®ƒ", "æˆ‘ä»¬", "ä½ ä»¬"
            }

            # åˆ†è¯
            words = jieba.cut(query)

            # è¿‡æ»¤åœç”¨è¯å’ŒçŸ­è¯
            keywords = [
                w for w in words
                if len(w) >= 2 and w not in stopwords
            ]

            # å–å‰5ä¸ª
            return keywords[:5]

        except Exception as e:
            logger.warning(f"å…³é”®è¯æå–å¤±è´¥: {e}")
            return []

    async def _generate_hyde(self, query: str) -> str:
        """
        HyDE (Hypothetical Document Embeddings)

        ç”Ÿæˆä¸€ä¸ªå‡è®¾æ€§çš„å›ç­”ï¼Œç”¨äºå‘é‡æ£€ç´¢
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªé‡‘èçŸ¥è¯†ä¸“å®¶ã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œå†™ä¸€æ®µç®€çŸ­ã€ä¸“ä¸šçš„å‡è®¾æ€§å›ç­”ã€‚

**è¦æ±‚**ï¼š
1. å›ç­”è¦åŒ…å«ç›¸å…³çš„é‡‘èæœ¯è¯­å’Œæ¦‚å¿µ
2. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®
3. é•¿åº¦æ§åˆ¶åœ¨150-300å­—
4. è¿™ä¸ªå›ç­”å°†ç”¨äºæ£€ç´¢ç›¸å…³æ–‡æ¡£

**é—®é¢˜**ï¼š{query}

**å‡è®¾æ€§å›ç­”**ï¼š"""

        try:
            llm = await get_unified_llm_service_initialized()
            response = await llm.chat(
                prompt=prompt,
                model=LLMModel.DEEPSEEK_CHAT,
                temperature=0.5,
                max_tokens=500
            )

            hyde_doc = response.content.strip()

            # é™åˆ¶é•¿åº¦
            if len(hyde_doc) > 500:
                hyde_doc = hyde_doc[:500] + "..."

            return hyde_doc

        except Exception as e:
            logger.warning(f"HyDEç”Ÿæˆå¤±è´¥: {e}")
            return ""  # è¿”å›ç©ºï¼Œä¸å½±å“å…¶ä»–æ£€ç´¢è·¯å¾„

# å…¨å±€å®ä¾‹
_query_processor = None

def get_query_processor() -> EnhancedQueryProcessor:
    """è·å–æŸ¥è¯¢å¤„ç†å™¨å•ä¾‹"""
    global _query_processor
    if _query_processor is None:
        _query_processor = EnhancedQueryProcessor()
    return _query_processor
