"""
å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
æ”¯æŒä¸»æ¨¡å‹å’Œå¤‡ä»½æ¨¡å‹çš„è‡ªåŠ¨åˆ‡æ¢

é…ç½®è¯´æ˜ï¼š
- ä¸»æ¨¡å‹ï¼šDeepseekï¼ˆdeepseek-chatï¼‰
- å¤‡ä»½æ¨¡å‹ï¼šGLM-4.7ï¼ˆæ™ºè°±AIï¼‰
"""

import openai
from typing import List, Dict, Any, Optional
from app.core.structured_logging import get_structured_logger

from app.core.config import settings

logger = get_structured_logger(__name__)

class LLMService:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡ - æ”¯æŒä¸»æ¨¡å‹å’Œå¤‡ä»½æ¨¡å‹è‡ªåŠ¨åˆ‡æ¢

    é…ç½®è¯´æ˜ï¼š
    - ä¸»æ¨¡å‹ï¼šDeepseekï¼ˆdeepseek-chatï¼‰
    - å¤‡ä»½æ¨¡å‹ï¼šGLM-4.7ï¼ˆæ™ºè°±AIï¼‰

    è‡ªåŠ¨åˆ‡æ¢é€»è¾‘ï¼š
    1. ä¼˜å…ˆä½¿ç”¨ä¸»æ¨¡å‹ï¼ˆDeepseekï¼‰
    2. ä¸»æ¨¡å‹å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ä»½æ¨¡å‹ï¼ˆGLM-4.7ï¼‰
    3. å¯é€šè¿‡é…ç½®ç¦ç”¨è‡ªåŠ¨åˆ‡æ¢
    """

    def __init__(self):
        # ä»settingsä¸­è¯»å–é…ç½®
        self.primary_model = settings.primary_llm_model  # "deepseek"
        self.fallback_model = settings.fallback_llm_model  # "glm"
        self.fallback_enabled = True  # é»˜è®¤å¯ç”¨è‡ªåŠ¨åˆ‡æ¢

        # åˆå§‹åŒ–Deepseekå®¢æˆ·ç«¯ï¼ˆä¸»æ¨¡å‹ï¼‰
        deepseek_api_key = getattr(settings, 'deepseek_api_key', None)
        deepseek_base_url = getattr(settings, 'deepseek_base_url', None)

        if deepseek_api_key:
            self.deepseek_client = openai.OpenAI(
                api_key=deepseek_api_key,
                base_url=deepseek_base_url
            )
            logger.info("âœ“ Deepseek (ä¸»æ¨¡å‹) å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.deepseek_client = None
            logger.warning("âœ— Deepseek API Keyæœªé…ç½®")

        # åˆå§‹åŒ–GLM-4.7å®¢æˆ·ç«¯ï¼ˆå¤‡ä»½æ¨¡å‹ï¼‰
        glm_api_key = getattr(settings, 'glm_api_key', None)
        glm_base_url = getattr(settings, 'glm_base_url', None)

        if glm_api_key:
            self.glm_client = openai.OpenAI(
                api_key=glm_api_key,
                base_url=glm_base_url
            )
            logger.info("âœ“ GLM-4.7 (å¤‡ä»½æ¨¡å‹) å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        else:
            self.glm_client = None
            logger.warning("âœ— GLM-4.7 API Keyæœªé…ç½®")

        logger.info("="*80)
        logger.info(f"ğŸ¯ LLMæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"  - ä¸»æ¨¡å‹: {self._get_model_name(self.primary_model)}")
        logger.info(f"  - å¤‡ä»½æ¨¡å‹: {self._get_model_name(self.fallback_model)}")
        logger.info(f"  - è‡ªåŠ¨åˆ‡æ¢: {'å¯ç”¨' if self.fallback_enabled else 'ç¦ç”¨'}")
        logger.info("="*80)

        self.current_model = self.primary_model  # å½“å‰ä½¿ç”¨çš„æ¨¡å‹

    def _get_model_name(self, model_key: str) -> str:
        """è·å–æ¨¡å‹çš„æ˜¾ç¤ºåç§°"""
        model_names = {
            "deepseek": "Deepseek (deepseek-chat)",
            "glm": "GLM-4.7 (æ™ºè°±AI)",
            "qwen": "Qwen (é€šä¹‰åƒé—®)"
        }
        return model_names.get(model_key, model_key)

    def _get_client_and_model(self, model_key: str):
        """è·å–æ¨¡å‹å¯¹åº”çš„å®¢æˆ·ç«¯å’Œæ¨¡å‹åç§°"""
        if model_key == "deepseek":
            return self.deepseek_client, settings.deepseek_model
        elif model_key == "glm":
            return self.glm_client, settings.glm_model
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_key}")

    # ä½¿ç”¨ç»Ÿä¸€çš„é‡è¯•æœºåˆ¶ (app.core.retry)
    # å¦‚éœ€é‡è¯•åŠŸèƒ½ï¼Œä½¿ç”¨ @retry_on_failure è£…é¥°å™¨
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 4000,
        stream: bool = False
    ) -> Dict[str, Any]:
        """
        èŠå¤©å®Œæˆæ¥å£ - æ”¯æŒè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ä»½æ¨¡å‹

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰ï¼Œä¸æŒ‡å®šåˆ™ä½¿ç”¨ä¸»æ¨¡å‹
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¿”å›

        Returns:
            æ¨¡å‹å“åº”ç»“æœ
        """
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
        model_key = model or self.current_model

        # éªŒè¯æ¨¡å‹æ˜¯å¦å¯ç”¨
        if model_key not in [self.primary_model, self.fallback_model]:
            logger.warning(f"âš  æŒ‡å®šçš„æ¨¡å‹ {model_key} ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¸»æ¨¡å‹")
            model_key = self.primary_model

        try:
            # è·å–å®¢æˆ·ç«¯å’Œæ¨¡å‹åç§°
            client, model_name = self._get_client_and_model(model_key)

            if client is None:
                raise ValueError(f"æ¨¡å‹ {model_key} çš„å®¢æˆ·ç«¯æœªåˆå§‹åŒ–")

            logger.info("="*80)
            logger.info(f"ğŸš€ è°ƒç”¨LLMæ¨¡å‹: {self._get_model_name(model_key)}")
            logger.info(f"  æ¨¡å‹: {model_name}")
            logger.info(f"  Temperature: {temperature}")
            logger.info(f"  Max Tokens: {max_tokens}")
            logger.info(f"  æ¶ˆæ¯æ•°é‡: {len(messages)}")
            logger.info(f"  Stream: {stream}")

            # æ‰“å°æ¶ˆæ¯å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰
            for i, msg in enumerate(messages):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')[:200]
                logger.info(f"  æ¶ˆæ¯{i+1} [{role}]: {content}...")

            logger.info("="*80)

            response = client.chat.completions.create(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )

            if stream:
                logger.info("âœ“ è¿”å›æµå¼å“åº”")
                return response  # æµå¼å“åº”
            else:
                result = {
                    "content": response.choices[0].message.content,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "model": model_name,
                    "model_key": model_key,
                    "finish_reason": response.choices[0].finish_reason
                }

                logger.info("="*80)
                logger.info(f"âœ“ LLMè°ƒç”¨æˆåŠŸ: {self._get_model_name(model_key)}")
                logger.info(f"  æ¨¡å‹: {result['model']}")
                logger.info(f"  Tokenä½¿ç”¨: {result['usage']['total_tokens']}")
                logger.info(f"    - Prompt: {result['usage']['prompt_tokens']}")
                logger.info(f"    - Completion: {result['usage']['completion_tokens']}")
                logger.info(f"  å®ŒæˆåŸå› : {result['finish_reason']}")
                logger.info(f"  å“åº”å†…å®¹: {result['content'][:200]}...")
                logger.info("="*80)

                return result

        except Exception as e:
            logger.error("="*80)
            logger.error(f"âœ— æ¨¡å‹ {self._get_model_name(model_key)} è°ƒç”¨å¤±è´¥")
            logger.error(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error("="*80)

            # å°è¯•ä½¿ç”¨å¤‡ä»½æ¨¡å‹
            if self.fallback_enabled and model_key != self.fallback_model:
                logger.info(f"ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ä»½æ¨¡å‹: {self._get_model_name(self.fallback_model)}")
                self.current_model = self.fallback_model
                return await self.chat_completion(
                    messages=messages,
                    model=self.fallback_model,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    stream=stream
                )

            raise

    async def simple_chat(
        self,
        prompt: str,
        system_prompt: str = None,
        model: str = None,
        temperature: float = 0.7
    ) -> str:
        """
        ç®€å•èŠå¤©æ¥å£

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°

        Returns:
            æ¨¡å‹å›å¤æ–‡æœ¬
        """
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": prompt})

        response = await self.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature
        )

        return response["content"]

    async def structured_completion(
        self,
        prompt: str,
        schema: Dict[str, Any],
        system_prompt: str = None,
        model: str = None
    ) -> Dict[str, Any]:
        """
        ç»“æ„åŒ–è¾“å‡ºå®Œæˆ

        Args:
            prompt: è¾“å…¥æç¤º
            schema: è¾“å‡ºç»“æ„schema
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°

        Returns:
            ç»“æ„åŒ–è¾“å‡ºç»“æœ
        """
        # æ·»åŠ JSONæ ¼å¼åŒ–æŒ‡ä»¤
        if system_prompt:
            system_prompt += f"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š\n{schema}"
        else:
            system_prompt = f"è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š\n{schema}"

        response = await self.simple_chat(
            prompt=prompt,
            system_prompt=system_prompt,
            model=model
        )

        try:
            import json
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥: {response}")
            raise ValueError("æ¨¡å‹è¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®")

# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()