"""
å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
æ”¯æŒDeepSeekå’ŒQwenç­‰å¤šç§æ¨¡å‹
"""

from typing import List, Dict, Any, Optional
import openai
import logging
# from tenacity import retry, stop_after_attempt, wait_exponential  # TODO: å®‰è£…tenacityä¾èµ–

from app.core.config import settings

logger = logging.getLogger(__name__)


class LLMService:
    """å¤§è¯­è¨€æ¨¡å‹æœåŠ¡"""

    def __init__(self):
        # DeepSeekå®¢æˆ·ç«¯é…ç½® - ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
        api_key = getattr(settings, 'deepseek_api_key', None) or settings.openai_api_key
        base_url = getattr(settings, 'deepseek_base_url', None) or settings.openai_base_url

        logger.info(f"åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯:")
        logger.info(f"  API Key: {api_key[:20]}...{api_key[-5:] if api_key else 'None'}")
        logger.info(f"  Base URL: {base_url}")
        logger.info(f"  Model: {settings.llm_model}")

        self.deepseek_client = openai.OpenAI(
            api_key=api_key,
            base_url=base_url
        )

        # Qwenå®¢æˆ·ç«¯é…ç½®
        logger.info(f"åˆå§‹åŒ–Qwenå®¢æˆ·ç«¯:")
        if settings.qwen_api_key:
            logger.info(f"  API Key: {settings.qwen_api_key[:20]}...{settings.qwen_api_key[-5:]}")
        else:
            logger.warning("  API Key: Not configured (Qwen API will not be available)")
        logger.info(f"  Base URL: {settings.qwen_base_url}")

        self.qwen_client = openai.OpenAI(
            api_key=settings.qwen_api_key,
            base_url=settings.qwen_base_url
        )

    # @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))  # TODO: å®‰è£…tenacityä¾èµ–
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 4000,
        stream: bool = False,
        use_qwen: bool = False
    ) -> Dict[str, Any]:
        """
        èŠå¤©å®Œæˆæ¥å£

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            stream: æ˜¯å¦æµå¼è¿”å›
            use_qwen: æ˜¯å¦ä½¿ç”¨Qwenæ¨¡å‹

        Returns:
            æ¨¡å‹å“åº”ç»“æœ
        """
        try:
            # é€‰æ‹©å®¢æˆ·ç«¯å’Œæ¨¡å‹
            client = self.qwen_client if use_qwen else self.deepseek_client
            model = model or (settings.qwen_multimodal_model if use_qwen else settings.llm_model)

            logger.info("="*60)
            logger.info(f"ğŸš€ è°ƒç”¨LLMæ¨¡å‹")
            logger.info(f"  æ¨¡å‹: {model}")
            logger.info(f"  ä½¿ç”¨Qwen: {use_qwen}")
            logger.info(f"  Temperature: {temperature}")
            logger.info(f"  Max Tokens: {max_tokens}")
            logger.info(f"  æ¶ˆæ¯æ•°é‡: {len(messages)}")
            logger.info(f"  Stream: {stream}")

            # æ‰“å°æ¶ˆæ¯å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰
            for i, msg in enumerate(messages):
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')[:200]
                logger.info(f"  æ¶ˆæ¯{i+1} [{role}]: {content}...")

            logger.info("="*60)

            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=stream
            )

            if stream:
                logger.info("âœ“ è¿”å›æµå¼å“åº”")
                return response  # æµå¼å“åº”
            else:
                result = {
                    "content": response.choices[0].message.content,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens
                    },
                    "model": model,
                    "finish_reason": response.choices[0].finish_reason
                }

                logger.info("="*60)
                logger.info(f"âœ“ LLMè°ƒç”¨æˆåŠŸ")
                logger.info(f"  æ¨¡å‹: {result['model']}")
                logger.info(f"  Tokenä½¿ç”¨: {result['usage']['total_tokens']}")
                logger.info(f"    - Prompt: {result['usage']['prompt_tokens']}")
                logger.info(f"    - Completion: {result['usage']['completion_tokens']}")
                logger.info(f"  å®ŒæˆåŸå› : {result['finish_reason']}")
                logger.info(f"  å“åº”å†…å®¹: {result['content'][:200]}...")
                logger.info("="*60)

                return result

        except Exception as e:
            logger.error("="*60)
            logger.error(f"âœ— LLMè°ƒç”¨å¤±è´¥")
            logger.error(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"  é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error("="*60)
            raise

    async def simple_chat(
        self,
        prompt: str,
        system_prompt: str = None,
        model: str = None,
        temperature: float = 0.7,
        use_qwen: bool = False
    ) -> str:
        """
        ç®€å•èŠå¤©æ¥å£

        Args:
            prompt: ç”¨æˆ·æç¤º
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            use_qwen: æ˜¯å¦ä½¿ç”¨Qwenæ¨¡å‹

        Returns:
            æ¨¡å‹å›å¤æ–‡æœ¬
        """
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        messages.append({"role": "user", "content": prompt})

        response = await self.chat_completion(
            messages=messages,
            model=model,
            temperature=temperature,
            use_qwen=use_qwen
        )

        return response["content"]

    async def structured_completion(
        self,
        prompt: str,
        schema: Dict[str, Any],
        system_prompt: str = None,
        model: str = None,
        use_qwen: bool = False
    ) -> Dict[str, Any]:
        """
        ç»“æ„åŒ–è¾“å‡ºå®Œæˆ

        Args:
            prompt: è¾“å…¥æç¤º
            schema: è¾“å‡ºç»“æ„schema
            system_prompt: ç³»ç»Ÿæç¤º
            model: æ¨¡å‹åç§°
            use_qwen: æ˜¯å¦ä½¿ç”¨Qwenæ¨¡å‹

        Returns:
            ç»“æ„åŒ–è¾“å‡ºç»“æœ
        """
        # æ·»åŠ JSONæ ¼å¼åŒ–æŒ‡ä»¤
        if system_prompt:
            system_prompt += f"\n\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š\n{schema}"
        else:
            system_prompt = f"è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š\n{schema}"

        response = await self.simple_chat(
            prompt=prompt,
            system_prompt=system_prompt,
            model=model,
            use_qwen=use_qwen
        )

        try:
            import json
            return json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"ç»“æ„åŒ–è¾“å‡ºè§£æå¤±è´¥: {response}")
            raise ValueError("æ¨¡å‹è¿”å›çš„JSONæ ¼å¼ä¸æ­£ç¡®")


# å…¨å±€LLMæœåŠ¡å®ä¾‹
llm_service = LLMService()