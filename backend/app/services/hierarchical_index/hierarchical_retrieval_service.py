"""
åˆ†å±‚æ£€ç´¢æœåŠ¡
ä½¿ç”¨ä¸‰å±‚ç´¢å¼•ï¼ˆæ–‡æ¡£æ‘˜è¦ã€ç« èŠ‚ã€ç‰‡æ®µï¼‰è¿›è¡Œæ™ºèƒ½æ£€ç´¢
"""

import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.structured_logging import get_structured_logger
from app.schemas.hierarchical_index import (
    HierarchicalRetrievalRequest,
    HierarchicalRetrievalResult,
    RetrievedDocument,
    RetrievedChapter,
    RetrievedChunk
)
from app.services.llm_service import LLMService
from app.services.embeddings.unified_embedding_service import UnifiedEmbeddingService

logger = get_structured_logger(__name__)


class HierarchicalRetrievalService:
    """
    åˆ†å±‚æ£€ç´¢æœåŠ¡

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. å…ˆåœ¨æ–‡æ¡£æ‘˜è¦å±‚è¿›è¡Œç²—ç²’åº¦æ£€ç´¢ï¼Œç­›é€‰å‡ºç›¸å…³æ–‡æ¡£
    2. åœ¨ç­›é€‰å‡ºçš„æ–‡æ¡£çš„ç« èŠ‚å±‚è¿›è¡Œä¸­ç²’åº¦æ£€ç´¢ï¼Œå®šä½ç›¸å…³ç« èŠ‚
    3. åœ¨ç­›é€‰å‡ºçš„ç« èŠ‚çš„ç‰‡æ®µå±‚è¿›è¡Œç»†ç²’åº¦æ£€ç´¢ï¼Œè·å–ç²¾ç¡®å†…å®¹

    ä¼˜åŠ¿ï¼š
    - å‡å°‘æ£€ç´¢èŒƒå›´ï¼Œæé«˜æ•ˆç‡
    - å±‚å±‚ç¼©å°èŒƒå›´ï¼Œæé«˜å‡†ç¡®ç‡
    - æä¾›å¤šç²’åº¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    """

    def __init__(
        self,
        embedding_service: UnifiedEmbeddingService = None,
        llm_service: LLMService = None
    ):
        """
        åˆå§‹åŒ–åˆ†å±‚æ£€ç´¢æœåŠ¡

        Args:
            embedding_service: åµŒå…¥æœåŠ¡
            llm_service: LLMæœåŠ¡
        """
        self.embedding_service = embedding_service or UnifiedEmbeddingService()
        self.llm_service = llm_service or LLMService()

        logger.info("åˆ†å±‚æ£€ç´¢æœåŠ¡åˆå§‹åŒ–å®Œæˆ")

    async def retrieve(
        self,
        request: HierarchicalRetrievalRequest,
        hierarchical_indexes: Dict[str, Any]  # document_id -> HierarchicalIndex
    ) -> HierarchicalRetrievalResult:
        """
        æ‰§è¡Œåˆ†å±‚æ£€ç´¢

        Args:
            request: æ£€ç´¢è¯·æ±‚
            hierarchical_indexes: åˆ†å±‚ç´¢å¼•å­—å…¸

        Returns:
            HierarchicalRetrievalResult: æ£€ç´¢ç»“æœ
        """
        start_time = datetime.now()

        logger.info(f"ğŸ” å¼€å§‹åˆ†å±‚æ£€ç´¢: {request.query}")

        try:
            results = HierarchicalRetrievalResult(
                query=request.query,
                documents=[],
                chapters=[],
                chunks=[],
                merged_results=[]
            )

            # ç¬¬1å±‚ï¼šæ–‡æ¡£çº§æ£€ç´¢
            if request.use_summary:
                logger.info("  ğŸ“„ ç¬¬1å±‚ï¼šæ–‡æ¡£æ‘˜è¦æ£€ç´¢...")
                results.documents = await self._retrieve_from_documents(
                    request=request,
                    hierarchical_indexes=hierarchical_indexes
                )
                results.total_docs = len(results.documents)

                if not results.documents:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    return results

                logger.info(f"    âœ“ æ‰¾åˆ° {len(results.documents)} ä¸ªç›¸å…³æ–‡æ¡£")

                # è¿‡æ»¤å‡ºç›¸å…³æ–‡æ¡£çš„ç´¢å¼•
                relevant_doc_ids = [doc.document_id for doc in results.documents]
                relevant_indexes = {
                    doc_id: idx
                    for doc_id, idx in hierarchical_indexes.items()
                    if doc_id in relevant_doc_ids
                }
            else:
                relevant_indexes = hierarchical_indexes

            # ç¬¬2å±‚ï¼šç« èŠ‚çº§æ£€ç´¢
            if request.use_chapters and relevant_indexes:
                logger.info("  ğŸ“‘ ç¬¬2å±‚ï¼šç« èŠ‚æ£€ç´¢...")
                results.chapters = await self._retrieve_from_chapters(
                    request=request,
                    hierarchical_indexes=relevant_indexes,
                    max_chapters_per_doc=request.max_chapters_per_doc
                )
                results.total_chapters = len(results.chapters)

                if not results.chapters:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³ç« èŠ‚")
                    return results

                logger.info(f"    âœ“ æ‰¾åˆ° {len(results.chapters)} ä¸ªç›¸å…³ç« èŠ‚")

                # æå–ç›¸å…³ç« èŠ‚ID
                relevant_chapter_ids = [ch.chapter_id for ch in results.chapters]
            else:
                relevant_chapter_ids = None

            # ç¬¬3å±‚ï¼šç‰‡æ®µçº§æ£€ç´¢
            if request.use_chunks and relevant_indexes:
                logger.info("  âœ‚ï¸ ç¬¬3å±‚ï¼šç‰‡æ®µæ£€ç´¢...")
                results.chunks = await self._retrieve_from_chunks(
                    request=request,
                    hierarchical_indexes=relevant_indexes,
                    chapter_ids=relevant_chapter_ids,
                    max_chunks_per_chapter=request.max_chunks_per_chapter
                )
                results.total_chunks = len(results.chunks)

                logger.info(f"    âœ“ æ‰¾åˆ° {len(results.chunks)} ä¸ªç›¸å…³ç‰‡æ®µ")

            # åˆå¹¶ç»“æœ
            results.merged_results = self._merge_retrieval_results(
                results.documents,
                results.chapters,
                results.chunks
            )

            # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
            results.merged_results = results.merged_results[:request.top_k]

            # ç»Ÿè®¡è€—æ—¶
            results.retrieval_time = (datetime.now() - start_time).total_seconds()

            logger.info(
                f"âœ… åˆ†å±‚æ£€ç´¢å®Œæˆï¼"
                f"æ–‡æ¡£: {results.total_docs}, "
                f"ç« èŠ‚: {results.total_chapters}, "
                f"ç‰‡æ®µ: {results.total_chunks}, "
                f"è€—æ—¶: {results.retrieval_time:.2f}ç§’"
            )

            return results

        except Exception as e:
            logger.error(f"âŒ åˆ†å±‚æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
            raise

    async def _retrieve_from_documents(
        self,
        request: HierarchicalRetrievalRequest,
        hierarchical_indexes: Dict[str, Any]
    ) -> List[RetrievedDocument]:
        """
        ä»æ–‡æ¡£æ‘˜è¦å±‚æ£€ç´¢

        Args:
            request: æ£€ç´¢è¯·æ±‚
            hierarchical_indexes: åˆ†å±‚ç´¢å¼•å­—å…¸

        Returns:
            List[RetrievedDocument]: ç›¸å…³æ–‡æ¡£åˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self.embedding_service.embed_batch([request.query])
        query_embedding = query_embedding[0].tolist()

        retrieved_docs = []

        # éå†æ‰€æœ‰æ–‡æ¡£çš„æ‘˜è¦ç´¢å¼•
        for doc_id, index in hierarchical_indexes.items():
            document_summary = index.document_summary

            # æ£€æŸ¥æ˜¯å¦æœ‰é™å®šçš„æ–‡æ¡£ID
            if request.document_ids and doc_id not in request.document_ids:
                continue

            # å¦‚æœæœ‰åµŒå…¥å‘é‡ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
            if document_summary.embedding:
                score = self._cosine_similarity(
                    query_embedding,
                    document_summary.embedding
                )

                if score >= request.doc_threshold:
                    retrieved_docs.append(RetrievedDocument(
                        document_id=doc_id,
                        summary_text=document_summary.summary_text,
                        score=score,
                        keywords=document_summary.keywords,
                        entities=document_summary.entities
                    ))

        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰Nä¸ª
        retrieved_docs.sort(key=lambda x: x.score, reverse=True)
        return retrieved_docs[:request.max_docs]

    async def _retrieve_from_chapters(
        self,
        request: HierarchicalRetrievalRequest,
        hierarchical_indexes: Dict[str, Any],
        max_chapters_per_doc: int
    ) -> List[RetrievedChapter]:
        """
        ä»ç« èŠ‚å±‚æ£€ç´¢

        Args:
            request: æ£€ç´¢è¯·æ±‚
            hierarchical_indexes: åˆ†å±‚ç´¢å¼•å­—å…¸
            max_chapters_per_doc: æ¯ä¸ªæ–‡æ¡£æœ€å¤šè¿”å›çš„ç« èŠ‚æ•°

        Returns:
            List[RetrievedChapter]: ç›¸å…³ç« èŠ‚åˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self.embedding_service.embed_batch([request.query])
        query_embedding = query_embedding[0].tolist()

        retrieved_chapters = []

        # éå†æ‰€æœ‰æ–‡æ¡£çš„ç« èŠ‚
        for doc_id, index in hierarchical_indexes.items():
            chapters_per_doc = 0

            for chapter in index.chapters:
                # æ£€æŸ¥æ˜¯å¦æœ‰é™å®šçš„ç« èŠ‚ID
                if request.chapter_ids and chapter.chapter_id not in request.chapter_ids:
                    continue

                # å¦‚æœæœ‰åµŒå…¥å‘é‡ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
                if chapter.embedding:
                    score = self._cosine_similarity(
                        query_embedding,
                        chapter.embedding
                    )

                    if score >= request.chapter_threshold:
                        retrieved_chapters.append(RetrievedChapter(
                            chapter_id=chapter.chapter_id,
                            document_id=doc_id,
                            title=chapter.title,
                            summary=chapter.summary,
                            score=score,
                            level=chapter.level,
                            chunk_count=chapter.chunk_count
                        ))

                        chapters_per_doc += 1
                        if chapters_per_doc >= max_chapters_per_doc:
                            break

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        retrieved_chapters.sort(key=lambda x: x.score, reverse=True)
        return retrieved_chapters

    async def _retrieve_from_chunks(
        self,
        request: HierarchicalRetrievalRequest,
        hierarchical_indexes: Dict[str, Any],
        chapter_ids: Optional[List[str]] = None,
        max_chunks_per_chapter: int = 5
    ) -> List[RetrievedChunk]:
        """
        ä»ç‰‡æ®µå±‚æ£€ç´¢

        Args:
            request: æ£€ç´¢è¯·æ±‚
            hierarchical_indexes: åˆ†å±‚ç´¢å¼•å­—å…¸
            chapter_ids: é™å®šæ£€ç´¢çš„ç« èŠ‚IDåˆ—è¡¨
            max_chunks_per_chapter: æ¯ä¸ªç« èŠ‚æœ€å¤šè¿”å›çš„ç‰‡æ®µæ•°

        Returns:
            List[RetrievedChunk]: ç›¸å…³ç‰‡æ®µåˆ—è¡¨
        """
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        query_embedding = await self.embedding_service.embed_batch([request.query])
        query_embedding = query_embedding[0].tolist()

        retrieved_chunks = []

        # éå†æ‰€æœ‰æ–‡æ¡£çš„ç‰‡æ®µ
        for doc_id, index in hierarchical_indexes.items():
            chunks_per_chapter = {}

            for chunk in index.chunks:
                # æ£€æŸ¥æ˜¯å¦é™å®šåœ¨ç‰¹å®šç« èŠ‚
                if chapter_ids and chunk.chapter_id not in chapter_ids:
                    continue

                # å¦‚æœæœ‰åµŒå…¥å‘é‡ï¼Œè®¡ç®—ç›¸ä¼¼åº¦
                if chunk.embedding:
                    score = self._cosine_similarity(
                        query_embedding,
                        chunk.embedding
                    )

                    if score >= request.chunk_threshold:
                        # æŸ¥æ‰¾ç« èŠ‚æ ‡é¢˜
                        chapter_title = None
                        if chunk.chapter_id:
                            chapter = next(
                                (ch for ch in index.chapters
                                 if ch.chapter_id == chunk.chapter_id),
                                None
                            )
                            if chapter:
                                chapter_title = chapter.title

                        retrieved_chunks.append(RetrievedChunk(
                            chunk_id=chunk.chunk_id,
                            document_id=doc_id,
                            chapter_id=chunk.chapter_id,
                            content=chunk.content,
                            score=score,
                            chapter_title=chapter_title,
                            metadata=chunk.metadata
                        ))

                        # ç»Ÿè®¡æ¯ç« èŠ‚çš„ç‰‡æ®µæ•°
                        if chunk.chapter_id:
                            chunks_per_chapter[chunk.chapter_id] = \
                                chunks_per_chapter.get(chunk.chapter_id, 0) + 1

                            # é™åˆ¶æ¯ç« èŠ‚çš„ç‰‡æ®µæ•°
                            if chunks_per_chapter[chunk.chapter_id] > max_chunks_per_chapter:
                                # è¿‡æ»¤è¶…å‡ºé™åˆ¶çš„ç‰‡æ®µ
                                retrieved_chunks = [
                                    c for c in retrieved_chunks
                                    if not (c.chapter_id == chunk.chapter_id and
                                           chunks_per_chapter[chunk.chapter_id] > max_chunks_per_chapter)
                                ]

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        retrieved_chunks.sort(key=lambda x: x.score, reverse=True)
        return retrieved_chunks

    def _merge_retrieval_results(
        self,
        documents: List[RetrievedDocument],
        chapters: List[RetrievedChapter],
        chunks: List[RetrievedChunk]
    ) -> List[RetrievedChunk]:
        """
        åˆå¹¶ä¸‰å±‚æ£€ç´¢ç»“æœ

        ç­–ç•¥ï¼š
        - ä¼˜å…ˆä½¿ç”¨ç‰‡æ®µçº§ç»“æœ
        - ä¸ºç‰‡æ®µæ·»åŠ ç« èŠ‚å’Œæ–‡æ¡£ä¸Šä¸‹æ–‡
        - é‡æ–°è®¡ç®—ç»¼åˆå¾—åˆ†
        """
        if not chunks:
            # å¦‚æœæ²¡æœ‰ç‰‡æ®µç»“æœï¼Œä»ç« èŠ‚ç”Ÿæˆä¼ªç‰‡æ®µ
            merged = []
            for chapter in chapters[:10]:
                merged.append(RetrievedChunk(
                    chunk_id=f"{chapter.chapter_id}_pseudo",
                    content=chapter.summary,
                    score=chapter.score * 0.9,  # ç« èŠ‚å¾—åˆ†æ‰“æŠ˜
                    chapter_title=chapter.title,
                    chapter_id=chapter.chapter_id,
                    metadata={"source": "chapter", "level": chapter.level}
                ))
            return merged

        # ä¸ºç‰‡æ®µæ·»åŠ é¢å¤–çš„ä¸Šä¸‹æ–‡å¾—åˆ†
        for chunk in chunks:
            # å¦‚æœç‰‡æ®µæ‰€å±ç« èŠ‚å¾—åˆ†é«˜ï¼Œæå‡ç‰‡æ®µå¾—åˆ†
            if chunk.chapter_id:
                chapter = next((ch for ch in chapters if ch.chapter_id == chunk.chapter_id), None)
                if chapter and chapter.score > 0.7:
                    chunk.score = chunk.score * 1.1  # æå‡å¾—åˆ†

        return chunks

    def _cosine_similarity(
        self,
        vec1: List[float],
        vec2: List[float]
    ) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            float: ç›¸ä¼¼åº¦å¾—åˆ† [0, 1]
        """
        import numpy as np

        try:
            v1 = np.array(vec1)
            v2 = np.array(vec2)

            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            similarity = dot_product / (norm1 * norm2)

            # ç¡®ä¿èŒƒå›´åœ¨[0, 1]
            return float(max(0.0, min(1.0, similarity)))

        except Exception as e:
            logger.error(f"è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            return 0.0


# å…¨å±€å•ä¾‹
_hierarchical_retrieval_service = None


def get_hierarchical_retrieval_service() -> HierarchicalRetrievalService:
    """è·å–åˆ†å±‚æ£€ç´¢æœåŠ¡å•ä¾‹"""
    global _hierarchical_retrieval_service
    if _hierarchical_retrieval_service is None:
        _hierarchical_retrieval_service = HierarchicalRetrievalService()
    return _hierarchical_retrieval_service
