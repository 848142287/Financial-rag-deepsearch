"""
åˆ†å±‚ç´¢å¼•æŠ½å–æœåŠ¡
ä»æ–‡æ¡£ä¸­æŠ½å–ä¸‰å±‚ç´¢å¼•ï¼šæ–‡æ¡£æ‘˜è¦ã€ç« èŠ‚ã€ç‰‡æ®µ
"""

import re
import json
import uuid
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.structured_logging import get_structured_logger
from app.schemas.hierarchical_index import (
    DocumentSummaryIndex,
    ChapterIndex,
    ChunkIndex,
    HierarchicalIndex,
    ChunkType
)
from app.services.chunking.smart_chunking_service import SmartChunkingService, ChunkingStrategy

logger = get_structured_logger(__name__)


class HierarchicalIndexExtractor:
    """
    åˆ†å±‚ç´¢å¼•æŠ½å–å™¨

    åŠŸèƒ½ï¼š
    1. æŠ½å–æ–‡æ¡£æ‘˜è¦ç´¢å¼•ï¼ˆæ•´ä½“æ‘˜è¦ã€å…³é”®è¯ã€å®ä½“ã€ä¸»é¢˜ï¼‰
    2. æŠ½å–ç« èŠ‚ç´¢å¼•ï¼ˆå±‚çº§ç»“æ„ã€ç« èŠ‚æ‘˜è¦ï¼‰
    3. æŠ½å–ç‰‡æ®µç´¢å¼•ï¼ˆæ™ºèƒ½åˆ†å—ã€ä¿æŒä¸Šä¸‹æ–‡ï¼‰
    """

    def __init__(self):
        """åˆå§‹åŒ–æŠ½å–å™¨"""
        self.chunking_service = SmartChunkingService(
            default_strategy=ChunkingStrategy.INTELLIGENT
        )

        # ç« èŠ‚æ ‡é¢˜æ¨¡å¼ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        self.section_patterns = [
            r'^#+\s+(.+)$',  # Markdownæ ‡é¢˜
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« \s+(.+)$',  # ç¬¬Xç« 
            r'^ç¬¬\d+ç« \s+(.+)$',  # ç¬¬1ç« 
            r'^\d+\.\s+(.+)$',  # 1. æ ‡é¢˜
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ã€\s*(.+)$',  # ä¸€ã€æ ‡é¢˜
            r'^[ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*[)ï¼‰]\s*(.+)$',  # ï¼ˆä¸€ï¼‰æ ‡é¢˜
        ]

        logger.info("åˆ†å±‚ç´¢å¼•æŠ½å–å™¨åˆå§‹åŒ–å®Œæˆ")

    async def extract_hierarchical_index(
        self,
        document_id: str,
        markdown_content: str,
        deepseek_summary: Optional[Dict[str, Any]] = None,
        **kwargs
    ) -> HierarchicalIndex:
        """
        æŠ½å–å®Œæ•´çš„åˆ†å±‚ç´¢å¼•

        Args:
            document_id: æ–‡æ¡£ID
            markdown_content: Markdownæ ¼å¼çš„æ–‡æ¡£å†…å®¹
            deepseek_summary: Deepseekæ·±åº¦æ±‡æ€»ç»“æœ
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            HierarchicalIndex: åˆ†å±‚ç´¢å¼•ç»“æ„
        """
        start_time = datetime.now()

        logger.info(f"ğŸ“š å¼€å§‹æŠ½å–æ–‡æ¡£ {document_id} çš„åˆ†å±‚ç´¢å¼•")

        try:
            # 1. æŠ½å–æ–‡æ¡£æ‘˜è¦ç´¢å¼•
            logger.info("  ğŸ“ æŠ½å–æ–‡æ¡£æ‘˜è¦ç´¢å¼•...")
            document_summary = await self._extract_document_summary(
                document_id,
                markdown_content,
                deepseek_summary
            )

            # 2. æŠ½å–ç« èŠ‚ç´¢å¼•
            logger.info("  ğŸ“‘ æŠ½å–ç« èŠ‚ç´¢å¼•...")
            chapters = await self._extract_chapters(
                document_id,
                markdown_content
            )

            # 3. æŠ½å–ç‰‡æ®µç´¢å¼•
            logger.info("  âœ‚ï¸ æŠ½å–ç‰‡æ®µç´¢å¼•...")
            chunks = await self._extract_chunks(
                document_id,
                markdown_content,
                chapters
            )

            # 4. å»ºç«‹ç« èŠ‚å’Œç‰‡æ®µçš„å…³è”å…³ç³»
            logger.info("  ğŸ”— å»ºç«‹å…³è”å…³ç³»...")
            self._link_chunks_to_chapters(chunks, chapters)

            # 5. æ„å»ºå®Œæ•´çš„åˆ†å±‚ç´¢å¼•
            hierarchical_index = HierarchicalIndex(
                document_id=document_id,
                document_summary=document_summary,
                chapters=chapters,
                chunks=chunks,
                total_chapters=len(chapters),
                total_chunks=len(chunks),
                created_at=datetime.now(),
                processing_time=(datetime.now() - start_time).total_seconds()
            )

            logger.info(
                f"âœ… åˆ†å±‚ç´¢å¼•æŠ½å–å®Œæˆï¼"
                f"æ‘˜è¦: 1, ç« èŠ‚: {len(chapters)}, ç‰‡æ®µ: {len(chunks)}, "
                f"è€—æ—¶: {hierarchical_index.processing_time:.2f}ç§’"
            )

            return hierarchical_index

        except Exception as e:
            logger.error(f"âŒ åˆ†å±‚ç´¢å¼•æŠ½å–å¤±è´¥: {str(e)}", exc_info=True)
            raise

    async def _extract_document_summary(
        self,
        document_id: str,
        markdown_content: str,
        deepseek_summary: Optional[Dict[str, Any]] = None
    ) -> DocumentSummaryIndex:
        """
        æŠ½å–æ–‡æ¡£æ‘˜è¦ç´¢å¼•

        Args:
            document_id: æ–‡æ¡£ID
            markdown_content: æ–‡æ¡£å†…å®¹
            deepseek_summary: Deepseekæ±‡æ€»ç»“æœ

        Returns:
            DocumentSummaryIndex: æ–‡æ¡£æ‘˜è¦ç´¢å¼•
        """
        # æå–å…³é”®è¯
        keywords = self._extract_keywords_from_text(markdown_content, top_k=20)

        # æå–å®ä½“ï¼ˆç®€å•ç‰ˆæœ¬ï¼šè¯†åˆ«å¸¸è§çš„é‡‘èå®ä½“ï¼‰
        entities = self._extract_entities(markdown_content)

        # æå–ä¸»é¢˜
        topics = self._extract_topics(markdown_content)

        # ç”Ÿæˆæ‘˜è¦ï¼ˆä¼˜å…ˆä½¿ç”¨Deepseekæ±‡æ€»ï¼Œå¦åˆ™ä½¿ç”¨è§„åˆ™ç”Ÿæˆï¼‰
        if deepseek_summary and deepseek_summary.get("status") == "success":
            summary_text = self._generate_summary_from_deepseek(deepseek_summary)
        else:
            summary_text = self._generate_summary_by_rules(markdown_content)

        # ç»Ÿè®¡ä¿¡æ¯
        doc_length = len(markdown_content)
        section_count = len(self._extract_sections_from_markdown(markdown_content))

        return DocumentSummaryIndex(
            document_id=document_id,
            summary_text=summary_text,
            keywords=keywords,
            entities=entities,
            topics=topics,
            metadata={
                "source": "hierarchical_extractor",
                "has_deepseek_summary": deepseek_summary is not None
            },
            doc_length=doc_length,
            section_count=section_count,
            chunk_count=0,  # ç¨åæ›´æ–°
            created_at=datetime.now()
        )

    async def _extract_chapters(
        self,
        document_id: str,
        markdown_content: str
    ) -> List[ChapterIndex]:
        """
        æŠ½å–ç« èŠ‚ç´¢å¼•

        Args:
            document_id: æ–‡æ¡£ID
            markdown_content: æ–‡æ¡£å†…å®¹

        Returns:
            List[ChapterIndex]: ç« èŠ‚ç´¢å¼•åˆ—è¡¨
        """
        sections = self._extract_sections_from_markdown(markdown_content)
        chapters = []

        for idx, section in enumerate(sections):
            chapter_id = f"{document_id}_ch_{idx:03d}"

            # ç”Ÿæˆç« èŠ‚æ‘˜è¦
            summary = self._generate_section_summary(section["content"])

            # æå–ç« èŠ‚å…³é”®è¯
            keywords = self._extract_keywords_from_text(
                section["content"],
                top_k=5
            )

            chapter = ChapterIndex(
                chapter_id=chapter_id,
                document_id=document_id,
                title=section["title"],
                level=section["level"],
                summary=summary,
                keywords=keywords,
                parent_chapter_id=None,  # ç¨åè®¡ç®—
                child_chapter_ids=[],
                start_char=section.get("start_char", 0),
                end_char=section.get("end_char", 0),
                chunk_count=0,  # ç¨åæ›´æ–°
                created_at=datetime.now()
            )

            chapters.append(chapter)

        # è®¡ç®—å±‚çº§å…³ç³»
        self._calculate_chapter_hierarchy(chapters)

        return chapters

    async def _extract_chunks(
        self,
        document_id: str,
        markdown_content: str,
        chapters: List[ChapterIndex]
    ) -> List[ChunkIndex]:
        """
        æŠ½å–ç‰‡æ®µç´¢å¼•

        Args:
            document_id: æ–‡æ¡£ID
            markdown_content: æ–‡æ¡£å†…å®¹
            chapters: ç« èŠ‚åˆ—è¡¨

        Returns:
            List[ChunkIndex]: ç‰‡æ®µç´¢å¼•åˆ—è¡¨
        """
        # ä½¿ç”¨æ™ºèƒ½åˆ†å—æœåŠ¡
        chunk_results = self.chunking_service.chunk_document(
            text=markdown_content,
            strategy=ChunkingStrategy.INTELLIGENT,
            target_chunk_size=800,
            max_chunk_size=1500
        )

        chunks = []

        for chunk_result in chunk_results:
            chunk_id = f"{document_id}_chk_{chunk_result['index']:03d}"

            # ç¡®å®šç‰‡æ®µç±»å‹
            chunk_type = self._determine_chunk_type(chunk_result)

            # æŸ¥æ‰¾æ‰€å±ç« èŠ‚
            chapter_id = self._find_chapter_for_chunk(
                chunk_result,
                chapters
            )

            chunk = ChunkIndex(
                chunk_id=chunk_id,
                document_id=document_id,
                chapter_id=chapter_id,
                content=chunk_result["text"],
                chunk_type=chunk_type,
                chunk_index=chunk_result["index"],
                start_char=chunk_result.get("start_pos", 0),
                end_char=chunk_result.get("end_pos", 0),
                metadata=chunk_result.get("metadata", {}),
                created_at=datetime.now()
            )

            chunks.append(chunk)

        return chunks

    def _extract_sections_from_markdown(
        self,
        markdown_content: str
    ) -> List[Dict[str, Any]]:
        """
        ä»Markdownä¸­æå–ç« èŠ‚ç»“æ„

        Args:
            markdown_content: Markdownå†…å®¹

        Returns:
            List[Dict]: ç« èŠ‚åˆ—è¡¨
        """
        sections = []
        lines = markdown_content.split('\n')

        current_section = {
            "title": "æ¦‚è¿°",
            "level": 0,
            "content": [],
            "start_char": 0
        }

        current_char_pos = 0

        for line in lines:
            line_length = len(line) + 1  # +1 for newline

            # æ£€æµ‹æ ‡é¢˜
            is_title = False
            for pattern in self.section_patterns:
                match = re.match(pattern, line.strip())
                if match:
                    # ä¿å­˜ä¸Šä¸€ä¸ªç« èŠ‚
                    if current_section["content"]:
                        current_section["content"] = '\n'.join(current_section["content"])
                        current_section["end_char"] = current_char_pos
                        sections.append(current_section)

                    # åˆ›å»ºæ–°ç« èŠ‚
                    level = self._get_title_level(line)
                    title = match.group(1) if match.groups() else line.strip('#').strip()

                    current_section = {
                        "title": title,
                        "level": level,
                        "content": [],
                        "start_char": current_char_pos
                    }
                    is_title = True
                    break

            if not is_title:
                current_section["content"].append(line)

            current_char_pos += line_length

        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section["content"]:
            current_section["content"] = '\n'.join(current_section["content"])
            current_section["end_char"] = current_char_pos
            sections.append(current_section)

        return sections

    def _get_title_level(self, title_line: str) -> int:
        """è·å–æ ‡é¢˜å±‚çº§"""
        if title_line.startswith('#'):
            return len(title_line) - len(title_line.lstrip('#'))
        elif 'ç¬¬' in title_line and 'ç« ' in title_line:
            return 1
        elif re.match(r'^\d+\.\s+', title_line):
            return 2
        elif re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ã€', title_line):
            return 2
        elif re.match(r'^[ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+\s*[)ï¼‰]', title_line):
            return 3
        return 1

    def _generate_section_summary(self, content: str) -> str:
        """ç”Ÿæˆç« èŠ‚æ‘˜è¦ï¼ˆå‰200å­—ï¼‰"""
        # ç®€å•ç‰ˆæœ¬ï¼šå–å‰é¢çš„å†…å®¹
        clean_content = re.sub(r'\s+', ' ', content.strip())
        if len(clean_content) > 200:
            return clean_content[:200] + "..."
        return clean_content

    def _extract_keywords_from_text(
        self,
        text: str,
        top_k: int = 10
    ) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯

        Args:
            text: æ–‡æœ¬å†…å®¹
            top_k: è¿”å›å‰Kä¸ªå…³é”®è¯

        Returns:
            List[str]: å…³é”®è¯åˆ—è¡¨
        """
        from collections import Counter

        # æå–ä¸­æ–‡è¯ç»„ï¼ˆ2-4ä¸ªå­—ï¼‰
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,4}', text)

        # è¿‡æ»¤å¸¸è§åœç”¨è¯
        stopwords = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'å’Œ', 'ä¸', 'æˆ–', 'ä½†', 'ç­‰', 'åŠ', 'å¯¹'}
        filtered_words = [w for w in chinese_words if w not in stopwords]

        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(filtered_words)

        # è¿”å›é«˜é¢‘è¯
        keywords = [word for word, freq in word_freq.most_common(top_k)
                   if freq >= 2]

        return keywords

    def _extract_entities(self, text: str) -> List[str]:
        """
        æå–å®ä½“ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰

        è¯†åˆ«å¸¸è§çš„é‡‘èå®ä½“ï¼š
        - å…¬å¸åï¼ˆXXXå…¬å¸ã€XXXé›†å›¢ï¼‰
        - è‚¡ç¥¨ä»£ç ï¼ˆ000001ï¼‰
        - äººåæ¨¡å¼
        """
        entities = []

        # æå–å…¬å¸å
        company_pattern = r'([\u4e00-\u9fa5]{2,6})(å…¬å¸|é›†å›¢|è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™å…¬å¸)'
        companies = re.findall(company_pattern, text)
        for company in companies:
            entities.append(company[0] + company[1])

        # æå–è‚¡ç¥¨ä»£ç 
        stock_pattern = r'\d{6}'
        stocks = re.findall(stock_pattern, text)
        entities.extend(stocks[:5])  # æœ€å¤š5ä¸ªè‚¡ç¥¨ä»£ç 

        return list(set(entities))[:20]  # æœ€å¤š20ä¸ªå®ä½“

    def _extract_topics(self, text: str) -> List[str]:
        """æå–ä¸»é¢˜ï¼ˆåŸºäºå…³é”®è¯èšç±»ï¼‰"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šé¢„å®šä¹‰çš„é‡‘èä¸»é¢˜æ£€æµ‹
        topic_keywords = {
            "æŠ•èµ„åˆ†æ": ["æŠ•èµ„", "æ”¶ç›Š", "å›æŠ¥", "é£é™©", "èµ„äº§é…ç½®"],
            "è´¢åŠ¡åˆ†æ": ["è¥æ”¶", "åˆ©æ¶¦", "ç°é‡‘æµ", "è´Ÿå€º", "è´¢åŠ¡"],
            "è¡Œä¸šç ”ç©¶": ["è¡Œä¸š", "å¸‚åœº", "ç«äº‰", "è¶‹åŠ¿", "å‰æ™¯"],
            "å…¬å¸ç ”ç©¶": ["å…¬å¸", "ä¸šåŠ¡", "äº§å“", "ç®¡ç†", "æˆ˜ç•¥"],
            "å®è§‚ç»æµ": ["ç»æµ", "æ”¿ç­–", "å¢é•¿", "é€šèƒ€", "åˆ©ç‡"]
        }

        matched_topics = []
        for topic, keywords in topic_keywords.items():
            if any(keyword in text for keyword in keywords):
                matched_topics.append(topic)

        return matched_topics

    def _generate_summary_from_deepseek(
        self,
        deepseek_summary: Dict[str, Any]
    ) -> str:
        """ä»Deepseekæ±‡æ€»ç”Ÿæˆæ‘˜è¦"""
        enhanced_summary = deepseek_summary.get("enhanced_summary", "")

        if enhanced_summary and len(enhanced_summary) > 50:
            # æ¸…ç†æ ¼å¼
            summary = re.sub(r'\s+', ' ', enhanced_summary.strip())
            if len(summary) > 500:
                summary = summary[:500] + "..."
            return summary

        # å›é€€åˆ°è§„åˆ™æå–
        rule_based = deepseek_summary.get("rule_based_summary", {})
        sections = rule_based.get("sections", [])
        if sections:
            return f"æœ¬æ–‡æ¡£åŒ…å«{len(sections)}ä¸ªä¸»è¦ç« èŠ‚ï¼Œ" + \
                   f"ä¸»è¦è®¨è®º{', '.join([s['title'] for s in sections[:3]])}"

        return "æ–‡æ¡£æ‘˜è¦ç”Ÿæˆä¸­..."

    def _generate_summary_by_rules(self, markdown_content: str) -> str:
        """åŸºäºè§„åˆ™ç”Ÿæˆæ‘˜è¦"""
        sections = self._extract_sections_from_markdown(markdown_content)

        if not sections:
            return "æ— æ³•ç”Ÿæˆæ‘˜è¦"

        # å–å‰3ä¸ªç« èŠ‚
        top_sections = sections[:3]
        section_titles = [s["title"] for s in top_sections]

        summary = f"æœ¬æ–‡æ¡£å…±{len(sections)}ä¸ªç« èŠ‚ï¼Œ"
        summary += f"ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š{', '.join(section_titles)}"

        return summary

    def _determine_chunk_type(self, chunk_result: Dict[str, Any]) -> ChunkType:
        """ç¡®å®šç‰‡æ®µç±»å‹"""
        metadata = chunk_result.get("metadata", {})
        chunk_type_str = metadata.get("type", "text")

        if chunk_type_str == "table":
            return ChunkType.TABLE
        elif chunk_type_str == "list":
            return ChunkType.LIST
        elif chunk_type_str == "image":
            return ChunkType.IMAGE
        elif chunk_type_str == "mixed":
            return ChunkType.MIXED
        else:
            return ChunkType.TEXT

    def _find_chapter_for_chunk(
        self,
        chunk_result: Dict[str, Any],
        chapters: List[ChapterIndex]
    ) -> Optional[str]:
        """æŸ¥æ‰¾ç‰‡æ®µæ‰€å±çš„ç« èŠ‚"""
        start_pos = chunk_result.get("start_pos", 0)

        # æ‰¾åˆ°åŒ…å«è¯¥ä½ç½®çš„ç« èŠ‚
        for chapter in chapters:
            if chapter.start_char <= start_pos <= chapter.end_char:
                return chapter.chapter_id

        return None

    def _calculate_chapter_hierarchy(self, chapters: List[ChapterIndex]):
        """è®¡ç®—ç« èŠ‚çš„å±‚çº§å…³ç³»"""
        for i, current_chapter in enumerate(chapters):
            # æŸ¥æ‰¾çˆ¶ç« èŠ‚
            for j in range(i - 1, -1, -1):
                prev_chapter = chapters[j]
                if prev_chapter.level < current_chapter.level:
                    current_chapter.parent_chapter_id = prev_chapter.chapter_id
                    prev_chapter.child_chapter_ids.append(current_chapter.chapter_id)
                    break

    def _link_chunks_to_chapters(
        self,
        chunks: List[ChunkIndex],
        chapters: List[ChapterIndex]
    ):
        """å»ºç«‹ç‰‡æ®µå’Œç« èŠ‚çš„å…³è”å…³ç³»"""
        # ç»Ÿè®¡æ¯ä¸ªç« èŠ‚çš„ç‰‡æ®µæ•°é‡
        chapter_chunk_count = {}

        for chunk in chunks:
            if chunk.chapter_id:
                chapter_chunk_count[chunk.chapter_id] = \
                    chapter_chunk_count.get(chunk.chapter_id, 0) + 1

        # æ›´æ–°ç« èŠ‚çš„ç‰‡æ®µè®¡æ•°
        for chapter in chapters:
            chapter.chunk_count = chapter_chunk_count.get(chapter.chapter_id, 0)

        # æ›´æ–°æ–‡æ¡£æ‘˜è¦çš„ç‰‡æ®µæ€»æ•°
        # ï¼ˆæ³¨æ„ï¼šè¿™éœ€è¦åœ¨å¤–éƒ¨è°ƒç”¨æ—¶è®¾ç½®ï¼‰


# å…¨å±€å•ä¾‹
_index_extractor = None


def get_hierarchical_index_extractor() -> HierarchicalIndexExtractor:
    """è·å–åˆ†å±‚ç´¢å¼•æŠ½å–å™¨å•ä¾‹"""
    global _index_extractor
    if _index_extractor is None:
        _index_extractor = HierarchicalIndexExtractor()
    return _index_extractor
