"""
å¢å¼ºçš„æ™ºèƒ½æ–‡æ¡£æ‰¹é‡ä¸Šä¼ æ¥å£
é›†æˆå¤šå¼•æ“æ–‡æ¡£è§£æç³»ç»Ÿï¼Œæä¾›é«˜è´¨é‡çš„æ‰¹é‡æ–‡æ¡£å¤„ç†
"""

from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks, UploadFile, File, Form
from fastapi.responses import JSONResponse
from typing import List, Dict, Any, Optional, Union
import os
import uuid
import asyncio
import logging
from datetime import datetime
from pathlib import Path
import json

from ...core.database import get_db
from ...core.config import settings
from ...core.dependencies import get_current_user
from ...models.user import User
from ...models.document import Document
from ...schemas.document import DocumentResponse, BatchUploadResponse
from ...services.document_intelligence.enhanced_parser import EnhancedDocumentParser
from ...services.document_intelligence.config.enhanced_parser_config import EnhancedParserConfig, ConfigManager
from ...services.document_intelligence.integration_example import DocumentIntelligenceSystem
from ...services.vector_store.vector_store_manager import vector_store_manager
from ...services.knowledge_graph.knowledge_graph_manager import knowledge_graph_manager

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/documents/enhanced-batch", tags=["æ™ºèƒ½æ‰¹é‡æ–‡æ¡£å¤„ç†"])


class EnhancedBatchProcessor:
    """å¢å¼ºçš„æ‰¹é‡å¤„ç†å™¨"""

    def __init__(self):
        self.config_manager = ConfigManager()
        self.config = self.config_manager.get_config()
        self.parser = EnhancedDocumentParser(self.config)
        self.vector_store = vector_store_manager
        self.kg_manager = knowledge_graph_manager

    async def process_document_with_intelligence(
        self,
        file_path: str,
        user_id: str,
        options: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æ™ºèƒ½è§£æç³»ç»Ÿå¤„ç†å•ä¸ªæ–‡æ¡£"""

        try:
            # è®¾ç½®é»˜è®¤è§£æé€‰é¡¹
            if options is None:
                options = {
                    "enable_auto_repair": True,
                    "integrity_threshold": 0.7,
                    "output_format": "structured",
                    "quality_assessment": True,
                    "cross_validation": True,
                    "generate_vectors": True,
                    "generate_knowledge_graph": True
                }

            # 1. æ™ºèƒ½æ–‡æ¡£è§£æ
            logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½è§£ææ–‡æ¡£: {file_path}")
            parse_result = await self.parser.parse_document(file_path, options)

            # 2. ç”Ÿæˆå‘é‡è¡¨ç¤º
            vectors_generated = False
            vector_ids = []
            if options.get("generate_vectors", True):
                try:
                    vector_result = await self._generate_document_vectors(parse_result, user_id)
                    vectors_generated = True
                    vector_ids = vector_result.get("vector_ids", [])
                    logger.info(f"âœ… å‘é‡ç”ŸæˆæˆåŠŸ: {len(vector_ids)} ä¸ªå‘é‡")
                except Exception as e:
                    logger.warning(f"âš ï¸ å‘é‡ç”Ÿæˆå¤±è´¥: {e}")

            # 3. æ„å»ºçŸ¥è¯†å›¾è°±
            kg_entities = []
            kg_relations = []
            if options.get("generate_knowledge_graph", True):
                try:
                    kg_result = await self._build_knowledge_graph(parse_result, user_id)
                    kg_entities = kg_result.get("entities", [])
                    kg_relations = kg_result.get("relations", [])
                    logger.info(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸ: {len(kg_entities)} ä¸ªå®ä½“, {len(kg_relations)} ä¸ªå…³ç³»")
                except Exception as e:
                    logger.warning(f"âš ï¸ çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥: {e}")

            # 4. ç”ŸæˆçŸ¥è¯†å›¾ç‰‡
            knowledge_images = []
            try:
                knowledge_images = await self._generate_knowledge_images(parse_result, user_id)
                logger.info(f"âœ… çŸ¥è¯†å›¾è°±ç”ŸæˆæˆåŠŸ: {len(knowledge_images)} å¼ å›¾ç‰‡")
            except Exception as e:
                logger.warning(f"âš ï¸ çŸ¥è¯†å›¾è°±ç”Ÿæˆå¤±è´¥: {e}")

            # 5. ä¿å­˜æ–‡æ¡£è®°å½•åˆ°æ•°æ®åº“
            document_record = await self._save_document_record(
                parse_result, user_id, vectors_generated, kg_entities, knowledge_images
            )

            return {
                "document_id": document_record.get("id"),
                "parse_result": parse_result,
                "vector_ids": vector_ids,
                "kg_entities": len(kg_entities),
                "kg_relations": len(kg_relations),
                "knowledge_images": len(knowledge_images),
                "processing_status": "completed",
                "quality_score": parse_result.get("integrity_score", 0),
                "engines_used": parse_result.get("engines_used", []),
                "content_summary": {
                    "total_pages": parse_result.get("total_pages", 0),
                    "total_sections": parse_result.get("total_chapters", 0),
                    "content_blocks": parse_result.get("total_content_blocks", 0),
                    "tables_count": len(parse_result.get("tables", [])),
                    "images_count": len(parse_result.get("images", [])),
                    "formulas_count": len(parse_result.get("formulas", []))
                }
            }

        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½æ–‡æ¡£å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return {
                "document_id": None,
                "parse_result": None,
                "processing_status": "failed",
                "error": str(e),
                "vector_ids": [],
                "kg_entities": 0,
                "kg_relations": 0,
                "knowledge_images": 0
            }

    async def _generate_document_vectors(
        self,
        parse_result: Dict[str, Any],
        user_id: str
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ–‡æ¡£å‘é‡è¡¨ç¤º"""
        vector_ids = []

        # æå–æ–‡æœ¬å†…å®¹
        text_content = self._extract_text_content(parse_result)

        # åˆ†å—å¤„ç†
        chunk_size = 500
        chunks = [text_content[i:i+chunk_size] for i in range(0, len(text_content), chunk_size)]

        for i, chunk in enumerate(chunks):
            if chunk.strip():
                # ç”Ÿæˆå‘é‡ï¼ˆè¿™é‡Œéœ€è¦é›†æˆå®é™…çš„å‘é‡ç”ŸæˆæœåŠ¡ï¼‰
                vector_data = {
                    "text": chunk,
                    "metadata": {
                        "document_id": parse_result.get("document_id"),
                        "chunk_id": i,
                        "user_id": user_id,
                        "content_type": "text",
                        "timestamp": datetime.utcnow().isoformat()
                    }
                }

                # æ¨¡æ‹Ÿå‘é‡ç”Ÿæˆ
                vector_id = f"vec_{uuid.uuid4().hex[:8]}"
                vector_ids.append(vector_id)

                # ä¿å­˜åˆ°å‘é‡æ•°æ®åº“
                # await self.vector_store.insert_vector(vector_id, vector_data)

        return {"vector_ids": vector_ids, "total_chunks": len(chunks)}

    async def _build_knowledge_graph(
        self,
        parse_result: Dict[str, Any],
        user_id: str
    ) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        entities = []
        relations = []

        # ä»è§£æç»“æœä¸­æå–å®ä½“
        for section in parse_result.get("sections", []):
            # æå–ç« èŠ‚æ ‡é¢˜ä½œä¸ºå®ä½“
            if section.get("title"):
                entity = {
                    "id": f"entity_{uuid.uuid4().hex[:8]}",
                    "name": section["title"],
                    "type": "section",
                    "properties": {
                        "level": section.get("level", 1),
                        "document_id": parse_result.get("document_id"),
                        "user_id": user_id
                    }
                }
                entities.append(entity)

        # ä»è¡¨æ ¼ä¸­æå–å®ä½“
        for table in parse_result.get("tables", []):
            if table.get("title"):
                entity = {
                    "id": f"entity_{uuid.uuid4().hex[:8]}",
                    "name": table["title"],
                    "type": "table",
                    "properties": {
                        "rows": table.get("rows", 0),
                        "columns": table.get("columns", 0),
                        "document_id": parse_result.get("document_id"),
                        "user_id": user_id
                    }
                }
                entities.append(entity)

        # æ„å»ºå…³ç³»
        for i, entity in enumerate(entities):
            if i > 0:
                relation = {
                    "id": f"rel_{uuid.uuid4().hex[:8]}",
                    "source": entities[i-1]["id"],
                    "target": entity["id"],
                    "type": "precedes",
                    "properties": {
                        "document_id": parse_result.get("document_id"),
                        "user_id": user_id
                    }
                }
                relations.append(relation)

        return {"entities": entities, "relations": relations}

    async def _generate_knowledge_images(
        self,
        parse_result: Dict[str, Any],
        user_id: str
    ) -> List[Dict[str, Any]]:
        """ç”ŸæˆçŸ¥è¯†å›¾ç‰‡"""
        knowledge_images = []

        # ä¸ºè¡¨æ ¼ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡
        for i, table in enumerate(parse_result.get("tables", [])):
            # æ¨¡æ‹ŸçŸ¥è¯†å›¾ç‰‡ç”Ÿæˆ
            image_data = {
                "id": f"img_{uuid.uuid4().hex[:8]}",
                "type": "table_visualization",
                "source_content": "table",
                "source_id": table.get("id"),
                "image_path": f"/knowledge_images/table_{i}.png",
                "metadata": {
                    "title": f"è¡¨æ ¼å¯è§†åŒ– - {table.get('title', f'è¡¨æ ¼{i+1}')}",
                    "document_id": parse_result.get("document_id"),
                    "user_id": user_id,
                    "generated_at": datetime.utcnow().isoformat()
                }
            }
            knowledge_images.append(image_data)

        # ä¸ºå…¬å¼ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡
        for i, formula in enumerate(parse_result.get("formulas", [])):
            image_data = {
                "id": f"img_{uuid.uuid4().hex[:8]}",
                "type": "formula_rendering",
                "source_content": "formula",
                "source_id": formula.get("id"),
                "image_path": f"/knowledge_images/formula_{i}.png",
                "metadata": {
                    "title": f"å…¬å¼æ¸²æŸ“ - {formula.get('content', '')[:50]}...",
                    "document_id": parse_result.get("document_id"),
                    "user_id": user_id,
                    "generated_at": datetime.utcnow().isoformat()
                }
            }
            knowledge_images.append(image_data)

        return knowledge_images

    def _extract_text_content(self, parse_result: Dict[str, Any]) -> str:
        """æå–æ–‡æ¡£çš„æ–‡æœ¬å†…å®¹"""
        text_parts = []

        # æå–ç« èŠ‚æ–‡æœ¬
        for section in parse_result.get("sections", []):
            if section.get("text"):
                text_parts.append(section["text"])

        # æå–è¡¨æ ¼æ–‡æœ¬
        for table in parse_result.get("tables", []):
            if table.get("text"):
                text_parts.append(table["text"])

        return " ".join(text_parts)

    async def _save_document_record(
        self,
        parse_result: Dict[str, Any],
        user_id: str,
        vectors_generated: bool,
        kg_entities: List,
        knowledge_images: List
    ) -> Dict[str, Any]:
        """ä¿å­˜æ–‡æ¡£è®°å½•åˆ°æ•°æ®åº“"""
        # è¿™é‡Œåº”è¯¥ä¿å­˜åˆ°å®é™…çš„æ•°æ®åº“
        # æ¨¡æ‹Ÿæ•°æ®åº“ä¿å­˜
        document_record = {
            "id": f"doc_{uuid.uuid4().hex[:8]}",
            "document_id": parse_result.get("document_id"),
            "user_id": user_id,
            "title": parse_result.get("title", "æœªå‘½åæ–‡æ¡£"),
            "file_path": parse_result.get("file_path"),
            "total_pages": parse_result.get("total_pages", 0),
            "total_sections": parse_result.get("total_chapters", 0),
            "integrity_score": parse_result.get("integrity_score", 0),
            "engines_used": parse_result.get("engines_used", []),
            "vectors_generated": vectors_generated,
            "kg_entities_count": len(kg_entities),
            "knowledge_images_count": len(knowledge_images),
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow()
        }

        return document_record


# å…¨å±€å¤„ç†å™¨å®ä¾‹
enhanced_processor = EnhancedBatchProcessor()


@router.post("/upload", response_model=Dict[str, Any])
async def enhanced_batch_upload(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    enable_vector_generation: bool = Form(True),
    enable_knowledge_graph: bool = Form(True),
    enable_knowledge_images: bool = Form(True),
    enable_auto_repair: bool = Form(True),
    integrity_threshold: float = Form(0.7),
    current_user: User = Depends(get_current_user),
    db: Any = Depends(get_db)
):
    """
    å¢å¼ºçš„æ™ºèƒ½æ‰¹é‡ä¸Šä¼ æ¥å£

    åŠŸèƒ½ç‰¹ç‚¹ï¼š
    - å¤šå¼•æ“æ™ºèƒ½æ–‡æ¡£è§£æ
    - è‡ªåŠ¨å‘é‡ç”Ÿæˆ
    - çŸ¥è¯†å›¾è°±æ„å»º
    - çŸ¥è¯†å›¾ç‰‡ç”Ÿæˆ
    - è´¨é‡è¯„ä¼°ä¸è‡ªåŠ¨ä¿®å¤
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ•°é‡é™åˆ¶
        if len(files) > 20:
            raise HTTPException(status_code=400, detail="å•æ¬¡æœ€å¤šä¸Šä¼ 20ä¸ªæ–‡ä»¶")

        # éªŒè¯æ–‡ä»¶ç±»å‹å’Œå¤§å°
        valid_files = []
        for file in files:
            if not _is_valid_file_type(file.filename):
                logger.warning(f"æ— æ•ˆæ–‡ä»¶ç±»å‹: {file.filename}")
                continue

            if file.size and not _is_valid_file_size(file.size):
                logger.warning(f"æ–‡ä»¶è¿‡å¤§: {file.filename}")
                continue

            valid_files.append(file)

        if not valid_files:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶")

        # åˆ›å»ºæ‰¹æ¬¡ID
        batch_id = str(uuid.uuid4())
        upload_dir = os.path.join(settings.upload_dir, "enhanced", batch_id)
        os.makedirs(upload_dir, exist_ok=True)

        # ä¿å­˜æ–‡ä»¶
        file_paths = []
        upload_results = []

        for file in valid_files:
            safe_filename = _generate_safe_filename(file.filename)
            file_path = os.path.join(upload_dir, safe_filename)

            with open(file_path, "wb") as buffer:
                content = await file.read()
                buffer.write(content)

            file_paths.append(file_path)
            upload_results.append({
                "filename": file.filename,
                "original_name": file.filename,
                "safe_filename": safe_filename,
                "file_path": file_path,
                "status": "uploaded",
                "size": len(content),
                "upload_time": datetime.utcnow().isoformat()
            })

        # å‡†å¤‡å¤„ç†é€‰é¡¹
        processing_options = {
            "enable_auto_repair": enable_auto_repair,
            "integrity_threshold": integrity_threshold,
            "output_format": "structured",
            "quality_assessment": True,
            "cross_validation": True,
            "generate_vectors": enable_vector_generation,
            "generate_knowledge_graph": enable_knowledge_graph,
            "generate_knowledge_images": enable_knowledge_images,
            "parallel_processing": True,
            "max_concurrent": 3
        }

        # å¯åŠ¨æ™ºèƒ½å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            _enhanced_process_uploaded_files,
            batch_id,
            file_paths,
            upload_results,
            processing_options,
            current_user.id
        )

        return {
            "batch_id": batch_id,
            "total_files": len(valid_files),
            "uploaded_files": len(upload_results),
            "status": "processing",
            "processing_options": processing_options,
            "estimated_time": len(valid_files) * 30,  # ä¼°ç®—æ—¶é—´ï¼ˆç§’ï¼‰
            "message": f"âœ… æˆåŠŸä¸Šä¼  {len(valid_files)} ä¸ªæ–‡ä»¶ï¼Œå¯åŠ¨æ™ºèƒ½å¤„ç†...",
            "capabilities": [
                "ğŸ” å¤šå¼•æ“æ–‡æ¡£è§£æ",
                "ğŸ§  æ™ºèƒ½è¯­ä¹‰ä¿®å¤",
                "ğŸ“Š è´¨é‡è¯„ä¼°",
                "ğŸ¯ å‘é‡ç”Ÿæˆ",
                "ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±æ„å»º",
                "ğŸ–¼ï¸ çŸ¥è¯†å›¾ç‰‡ç”Ÿæˆ"
            ]
        }

    except Exception as e:
        logger.error(f"å¢å¼ºæ‰¹é‡ä¸Šä¼ å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡ä¸Šä¼ å¤±è´¥: {str(e)}")


@router.get("/upload-status/{batch_id}")
async def get_enhanced_batch_status(
    batch_id: str,
    current_user: User = Depends(get_current_user)
):
    """è·å–å¢å¼ºæ‰¹é‡å¤„ç†çŠ¶æ€"""
    try:
        status = await _get_enhanced_batch_status(batch_id)
        return status
    except Exception as e:
        logger.error(f"è·å–å¢å¼ºæ‰¹é‡å¤„ç†çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–çŠ¶æ€å¤±è´¥")


@router.get("/processing-results/{batch_id}")
async def get_processing_results(
    batch_id: str,
    current_user: User = Depends(get_current_user),
    db: Any = Depends(get_db)
):
    """è·å–æ‰¹é‡å¤„ç†ç»“æœ"""
    try:
        results = await _get_processing_results(batch_id, current_user.id)
        return results
    except Exception as e:
        logger.error(f"è·å–å¤„ç†ç»“æœå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail="è·å–ç»“æœå¤±è´¥")


@router.post("/process-folder")
async def process_folder_documents(
    background_tasks: BackgroundTasks,
    folder_path: str = Form(...),
    recursive: bool = Form(True),
    file_pattern: str = Form("*.pdf"),
    processing_options: str = Form("{}"),
    current_user: User = Depends(get_current_user)
):
    """
    å¤„ç†æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ–‡æ¡£

    ä¸“é—¨ç”¨äºå¤„ç†åˆ¸å•†ç ”æŠ¥ç­‰ç°æœ‰æ–‡æ¡£é›†åˆ
    """
    try:
        # è§£æå¤„ç†é€‰é¡¹
        options = json.loads(processing_options) if processing_options else {}

        # æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶
        folder = Path(folder_path)
        if not folder.exists() or not folder.is_dir():
            raise HTTPException(status_code=400, detail="æŒ‡å®šçš„æ–‡ä»¶å¤¹ä¸å­˜åœ¨")

        # æœç´¢æ–‡ä»¶
        if recursive:
            files = list(folder.rglob(file_pattern))
        else:
            files = list(folder.glob(file_pattern))

        if not files:
            raise HTTPException(status_code=404, detail="æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶")

        # é™åˆ¶æ–‡ä»¶æ•°é‡
        max_files = 100
        if len(files) > max_files:
            files = files[:max_files]

        # åˆ›å»ºæ‰¹æ¬¡ID
        batch_id = str(uuid.uuid4())

        # å‡†å¤‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        file_paths = [str(f) for f in files]

        # é»˜è®¤å¤„ç†é€‰é¡¹
        default_options = {
            "enable_auto_repair": True,
            "integrity_threshold": 0.7,
            "output_format": "structured",
            "quality_assessment": True,
            "cross_validation": True,
            "generate_vectors": True,
            "generate_knowledge_graph": True,
            "generate_knowledge_images": True,
            "parallel_processing": True,
            "max_concurrent": 2
        }

        # åˆå¹¶ç”¨æˆ·é€‰é¡¹
        final_options = {**default_options, **options}

        # å¯åŠ¨å¤„ç†ä»»åŠ¡
        background_tasks.add_task(
            _enhanced_process_folder_files,
            batch_id,
            file_paths,
            final_options,
            current_user.id
        )

        return {
            "batch_id": batch_id,
            "folder_path": folder_path,
            "file_pattern": file_pattern,
            "recursive": recursive,
            "total_files_found": len(list(folder.rglob(file_pattern)) if recursive else list(folder.glob(file_pattern))),
            "files_to_process": len(file_paths),
            "status": "processing",
            "processing_options": final_options,
            "estimated_time": len(file_paths) * 45,
            "message": f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„ {len(file_paths)} ä¸ªæ–‡æ¡£..."
        }

    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="å¤„ç†é€‰é¡¹æ ¼å¼é”™è¯¯")
    except Exception as e:
        logger.error(f"æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ–‡ä»¶å¤¹å¤„ç†å¤±è´¥: {str(e)}")


# åå°å¤„ç†å‡½æ•°
async def _enhanced_process_uploaded_files(
    batch_id: str,
    file_paths: List[str],
    upload_results: List[Dict[str, Any]],
    processing_options: Dict[str, Any],
    user_id: str
):
    """å¢å¼ºçš„ä¸Šä¼ æ–‡ä»¶å¤„ç†"""
    try:
        logger.info(f"ğŸš€ å¼€å§‹å¢å¼ºæ‰¹é‡å¤„ç†: {batch_id}")

        # æ›´æ–°åˆå§‹çŠ¶æ€
        await _update_enhanced_batch_status(
            batch_id, "processing", 0, len(file_paths), []
        )

        processed_count = 0
        errors = []
        processing_results = []

        # å¹¶å‘å¤„ç†æ–‡æ¡£
        semaphore = asyncio.Semaphore(processing_options.get("max_concurrent", 2))

        async def process_single_file(file_path: str, upload_info: Dict[str, Any]):
            async with semaphore:
                try:
                    result = await enhanced_processor.process_document_with_intelligence(
                        file_path, user_id, processing_options
                    )

                    return {
                        "filename": upload_info["filename"],
                        "file_path": file_path,
                        "result": result,
                        "processing_time": result.get("processing_time", 0),
                        "status": "completed" if result.get("processing_status") == "completed" else "failed"
                    }

                except Exception as e:
                    logger.error(f"æ–‡ä»¶å¤„ç†å¼‚å¸¸: {file_path}, é”™è¯¯: {e}")
                    return {
                        "filename": upload_info["filename"],
                        "file_path": file_path,
                        "result": None,
                        "error": str(e),
                        "status": "failed"
                    }

        # æ‰§è¡Œå¹¶å‘å¤„ç†
        tasks = [
            process_single_file(file_path, upload_info)
            for file_path, upload_info in zip(file_paths, upload_results)
        ]

        results = await asyncio.gather(*tasks)

        # ç»Ÿè®¡ç»“æœ
        for result in results:
            if result["status"] == "completed":
                processed_count += 1
                processing_results.append(result)
            else:
                errors.append(f"{result['filename']}: {result.get('error', 'Unknown error')}")

            # æ›´æ–°è¿›åº¦
            await _update_enhanced_batch_status(
                batch_id, "processing",
                len([r for r in results if r["status"] in ["completed", "failed"]]),
                len(file_paths),
                errors
            )

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in processing_results if r["status"] == "completed"]

        if successful_results:
            avg_quality_score = sum(
                r["result"].get("quality_score", 0) for r in successful_results
            ) / len(successful_results)

            total_vectors = sum(
                len(r["result"].get("vector_ids", [])) for r in successful_results
            )

            total_kg_entities = sum(
                r["result"].get("kg_entities", 0) for r in successful_results
            )

            total_kg_relations = sum(
                r["result"].get("kg_relations", 0) for r in successful_results
            )

            total_knowledge_images = sum(
                r["result"].get("knowledge_images", 0) for r in successful_results
            )
        else:
            avg_quality_score = 0
            total_vectors = 0
            total_kg_entities = 0
            total_kg_relations = 0
            total_knowledge_images = 0

        # æ›´æ–°æœ€ç»ˆçŠ¶æ€
        final_status = "completed" if not errors else "completed_with_errors"
        await _update_enhanced_batch_status(
            batch_id, final_status, processed_count, len(file_paths), errors,
            {
                "avg_quality_score": avg_quality_score,
                "total_vectors": total_vectors,
                "total_kg_entities": total_kg_entities,
                "total_kg_relations": total_kg_relations,
                "total_knowledge_images": total_knowledge_images,
                "processing_results": processing_results
            }
        )

        logger.info(f"âœ… å¢å¼ºæ‰¹é‡å¤„ç†å®Œæˆ: {batch_id}")
        logger.info(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {processed_count}/{len(file_paths)}")
        logger.info(f"ğŸ¯ è´¨é‡åˆ†æ•°: {avg_quality_score:.3f}")
        logger.info(f"ğŸ”¢ å‘é‡æ•°é‡: {total_vectors}")
        logger.info(f"ğŸ•¸ï¸ çŸ¥è¯†å®ä½“: {total_kg_entities}")
        logger.info(f"ğŸ–¼ï¸ çŸ¥è¯†å›¾ç‰‡: {total_knowledge_images}")

    except Exception as e:
        logger.error(f"âŒ å¢å¼ºæ‰¹é‡å¤„ç†å¼‚å¸¸: {batch_id}, é”™è¯¯: {e}")
        await _update_enhanced_batch_status(
            batch_id, "failed", 0, len(file_paths), [str(e)]
        )


async def _enhanced_process_folder_files(
    batch_id: str,
    file_paths: List[str],
    processing_options: Dict[str, Any],
    user_id: str
):
    """å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶"""
    # ä¸ä¸Šé¢çš„å¤„ç†é€»è¾‘ç±»ä¼¼ï¼Œä½†ä¸“é—¨å¤„ç†æ–‡ä»¶å¤¹åœºæ™¯
    await _enhanced_process_uploaded_files(
        batch_id, file_paths,
        [{"filename": Path(f).name, "file_path": f} for f in file_paths],
        processing_options, user_id
    )


# è¾…åŠ©å‡½æ•°
def _is_valid_file_type(filename: str) -> bool:
    """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦æœ‰æ•ˆ"""
    if not filename:
        return False

    valid_extensions = {
        '.pdf', '.docx', '.xlsx', '.txt', '.md',
        '.jpg', '.jpeg', '.png', '.tiff'
    }

    extension = os.path.splitext(filename)[1].lower()
    return extension in valid_extensions


def _is_valid_file_size(size: int) -> bool:
    """æ£€æŸ¥æ–‡ä»¶å¤§å°æ˜¯å¦æœ‰æ•ˆ"""
    max_size = 50 * 1024 * 1024  # 50MB
    return 0 < size <= max_size


def _generate_safe_filename(filename: str) -> str:
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    safe_chars = "-_.() abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789"
    safe_filename = ''.join(c for c in filename if c in safe_chars)

    if len(safe_filename) > 100:
        name, ext = os.path.splitext(safe_filename)
        safe_filename = name[:100-len(ext)] + ext

    if not safe_filename:
        safe_filename = f"document_{uuid.uuid4().hex[:8]}"

    return safe_filename


# çŠ¶æ€ç®¡ç†å‡½æ•°ï¼ˆç®€åŒ–å®ç°ï¼‰
async def _update_enhanced_batch_status(
    batch_id: str,
    status: str,
    processed: int,
    total: int,
    errors: List[str],
    metrics: Optional[Dict[str, Any]] = None
):
    """æ›´æ–°å¢å¼ºæ‰¹é‡å¤„ç†çŠ¶æ€"""
    # è¿™é‡Œåº”è¯¥ä¿å­˜åˆ°Redisæˆ–æ•°æ®åº“
    pass


async def _get_enhanced_batch_status(batch_id: str) -> Dict[str, Any]:
    """è·å–å¢å¼ºæ‰¹é‡å¤„ç†çŠ¶æ€"""
    # ç®€åŒ–å®ç°
    return {
        "batch_id": batch_id,
        "status": "processing",
        "progress": 50,
        "total_files": 10,
        "processed_files": 5,
        "errors": [],
        "metrics": {
            "avg_quality_score": 0.85,
            "total_vectors": 150,
            "total_kg_entities": 80,
            "total_knowledge_images": 25
        }
    }


async def _get_processing_results(batch_id: str, user_id: str) -> Dict[str, Any]:
    """è·å–å¤„ç†ç»“æœè¯¦æƒ…"""
    # ç®€åŒ–å®ç°
    return {
        "batch_id": batch_id,
        "results": [
            {
                "document_id": "doc_123",
                "filename": "research_report.pdf",
                "status": "completed",
                "quality_score": 0.92,
                "processing_time": 45.2,
                "engines_used": ["qwen-vl-max", "mineru", "mathpix"],
                "content_summary": {
                    "total_pages": 15,
                    "tables_count": 8,
                    "images_count": 5,
                    "formulas_count": 12
                },
                "vectors_generated": 25,
                "kg_entities": 18,
                "knowledge_images": 6
            }
        ]
    }