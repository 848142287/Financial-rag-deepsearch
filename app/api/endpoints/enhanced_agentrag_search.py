"""
å¢å¼ºç‰ˆAgentRAGæœç´¢APIç«¯ç‚¹
ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£è§£æç»“æœè¿›è¡Œæ£€ç´¢ï¼Œæ”¯æŒå®Œæ•´çš„æ–‡æ¡£ç‰‡æ®µæ˜¾ç¤ºå’Œæº¯æºåŠŸèƒ½
"""

from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from typing import Dict, Any, List, Optional
from pydantic import BaseModel
import logging
import asyncio
from datetime import datetime

from app.core.database import get_db
from app.services.smart_embedding_service import SmartEmbeddingService
from app.services.neo4j_service import Neo4jService
from app.services.milvus_service import MilvusService
from app.models.document import Document as DocumentModel

logger = logging.getLogger(__name__)

router = APIRouter()

class EnhancedSearchRequest(BaseModel):
    query: str
    top_k: int = 5
    enable_multi_stage_retrieval: bool = True
    include_document_fragments: bool = True
    include_source_tracing: bool = True
    use_knowledge_graph: bool = True
    use_vector_search: bool = True

class DocumentFragment(BaseModel):
    """æ–‡æ¡£ç‰‡æ®µæ¨¡å‹"""
    document_id: int
    section_id: Optional[str] = None
    chunk_id: Optional[str] = None
    section_title: str
    content: str
    content_type: str  # text, table, image_caption, formula
    page_number: Optional[int] = None
    relevance_score: float

class SourceTrace(BaseModel):
    """æº¯æºä¿¡æ¯æ¨¡å‹"""
    document_id: int
    document_title: str
    document_filename: str
    sections: List[Dict[str, Any]]
    chunks: List[Dict[str, Any]]
    retrieval_path: List[str]  # æ£€ç´¢è·¯å¾„
    confidence_score: float

class EnhancedSearchResult(BaseModel):
    """å¢å¼ºæœç´¢ç»“æœæ¨¡å‹"""
    query: str
    answer: str
    retrieval_info: Dict[str, Any]
    document_fragments: List[DocumentFragment]
    source_traces: List[SourceTrace]
    performance_metrics: Dict[str, Any]

@router.post("/enhanced-agentrag-search")
async def enhanced_agentrag_search(
    request: EnhancedSearchRequest,
    db: Session = Depends(get_db)
) -> EnhancedSearchResult:
    """
    å¢å¼ºç‰ˆAgentRAGæœç´¢æ¥å£
    ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£è§£æç»“æœè¿›è¡Œæ£€ç´¢ï¼Œæ”¯æŒå®Œæ•´çš„æ–‡æ¡£ç‰‡æ®µæ˜¾ç¤ºå’Œæº¯æº
    """
    try:
        logger.info(f"ğŸš€ å¢å¼ºç‰ˆAgentRAGæœç´¢: '{request.query}'")
        start_time = datetime.now()

        # åˆå§‹åŒ–æœåŠ¡
        embedding_service = SmartEmbeddingService()
        milvus_service = MilvusService()
        neo4j_service = Neo4jService()

        retrieval_info = {
            "query_understanding": {},
            "vector_search": {},
            "knowledge_graph": {},
            "structured_query": {},
            "document_search": {},
            "data_sources_used": [],
            "retrieval_stages": []
        }

        # é˜¶æ®µ1: Query Understanding (æŸ¥è¯¢ç†è§£)
        query_analysis = await _analyze_query(request.query)
        retrieval_info["query_understanding"] = query_analysis
        retrieval_info["retrieval_stages"].append("Query Understanding")
        retrieval_info["data_sources_used"].append("AI Query Analyzer")

        document_fragments = []
        source_traces = []

        # é˜¶æ®µ2: Vector Search (å‘é‡æœç´¢)
        if request.use_vector_search:
            vector_results = await _perform_vector_search(
                request.query, request.top_k, embedding_service, milvus_service, db
            )
            retrieval_info["vector_search"] = vector_results
            retrieval_info["retrieval_stages"].append("Vector Search (Milvus)")
            retrieval_info["data_sources_used"].append("milvus")

            # æå–æ–‡æ¡£ç‰‡æ®µ
            fragments = await _extract_document_fragments(vector_results, db, "vector_search")
            document_fragments.extend(fragments)

            # ç”Ÿæˆæº¯æºä¿¡æ¯
            traces = await _generate_source_traces(vector_results, db, "vector_search")
            source_traces.extend(traces)

        # é˜¶æ®µ3: Knowledge Graph Traversal (çŸ¥è¯†å›¾è°±éå†)
        if request.use_knowledge_graph:
            graph_results = await _perform_knowledge_graph_search(
                query_analysis, request.top_k, neo4j_service
            )
            retrieval_info["knowledge_graph"] = graph_results
            retrieval_info["retrieval_stages"].append("Knowledge Graph Traversal (Neo4j)")
            retrieval_info["data_sources_used"].append("neo4j")

            # æå–å›¾è°±ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
            fragments = await _extract_document_fragments_from_graph(graph_results, db)
            document_fragments.extend(fragments)

        # é˜¶æ®µ4: Structured Query (ç»“æ„åŒ–æŸ¥è¯¢)
        structured_results = await _perform_structured_query(query_analysis, db)
        retrieval_info["structured_query"] = structured_results
        retrieval_info["retrieval_stages"].append("Structured Query (MySQL)")
        retrieval_info["data_sources_used"].append("mysql")

        # é˜¶æ®µ5: Document Content Search (æ–‡æ¡£å†…å®¹æœç´¢)
        content_results = await _perform_document_content_search(request.query, db)
        retrieval_info["document_search"] = content_results
        retrieval_info["retrieval_stages"].append("Document Content Search (MongoDB)")
        retrieval_info["data_sources_used"].append("mongodb")

        # ä»æ–‡æ¡£å†…å®¹æœç´¢ç»“æœä¸­æå–ç‰‡æ®µ
        if content_results.get("documents"):
            fragments = await _extract_document_fragments_from_content_search(content_results, db)
            document_fragments.extend(fragments)

        # å»é‡å¹¶æ’åºæ–‡æ¡£ç‰‡æ®µ
        document_fragments = _deduplicate_and_rank_fragments(document_fragments, request.top_k)
        source_traces = _deduplicate_source_traces(source_traces)

        # ç”Ÿæˆç»¼åˆå›ç­”
        answer = await _generate_comprehensive_answer(
            request.query, document_fragments, retrieval_info
        )

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        end_time = datetime.now()
        retrieval_time = (end_time - start_time).total_seconds() * 1000
        performance_metrics = {
            "retrieval_time_ms": retrieval_time,
            "documents_found": len(set(f.document_id for f in document_fragments)),
            "fragments_found": len(document_fragments),
            "source_traces": len(source_traces),
            "retrieval_stages": len(retrieval_info["retrieval_stages"])
        }

        # è®¡ç®—ç½®ä¿¡åº¦
        confidence_score = _calculate_confidence_score(document_fragments, retrieval_info)
        retrieval_info["confidence_score"] = confidence_score

        logger.info(f"âœ… å¢å¼ºç‰ˆæœç´¢å®Œæˆ: {len(document_fragments)}ä¸ªç‰‡æ®µ, {retrieval_time:.1f}ms")

        return EnhancedSearchResult(
            query=request.query,
            answer=answer,
            retrieval_info=retrieval_info,
            document_fragments=document_fragments,
            source_traces=source_traces,
            performance_metrics=performance_metrics
        )

    except Exception as e:
        logger.error(f"å¢å¼ºç‰ˆæœç´¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æœç´¢å¤±è´¥: {str(e)}")

async def _analyze_query(query: str) -> Dict[str, Any]:
    """æŸ¥è¯¢ç†è§£é˜¶æ®µ"""
    # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„æŸ¥è¯¢ç†è§£é€»è¾‘
    return {
        "original_query": query,
        "key_entities": _extract_entities(query),
        "query_intent": _classify_intent(query),
        "query_complexity": "medium" if len(query.split()) > 5 else "simple"
    }

def _extract_entities(query: str) -> List[str]:
    """æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“"""
    # ç®€å•çš„å®ä½“æå–é€»è¾‘
    import re
    # æå–ä¸­æ–‡å®ä½“è¯
    entities = re.findall(r'[\u4e00-\u9fff]+(?:è¯åˆ¸|é“¶è¡Œ|ä¿é™©|åŸºé‡‘|è‚¡ç¥¨|ç­–ç•¥|ç ”ç©¶|æŠ¥å‘Š)', query)
    return list(set(entities))

def _classify_intent(query: str) -> str:
    """åˆ†ç±»æŸ¥è¯¢æ„å›¾"""
    if any(word in query for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚']):
        return "comparative_analysis"
    elif any(word in query for word in ['ç­–ç•¥', 'å»ºè®®', 'å¦‚ä½•']):
        return "application_guidance"
    elif any(word in query for word in ['æ•°æ®', 'ç»Ÿè®¡', 'å…·ä½“']):
        return "data_specific"
    else:
        return "factual_recall"

async def _perform_vector_search(query: str, top_k: int, embedding_service, milvus_service, db) -> Dict[str, Any]:
    """æ‰§è¡Œå‘é‡æœç´¢"""
    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = await embedding_service.encode_single(query)

        # åœ¨Milvusä¸­æœç´¢ç›¸ä¼¼å‘é‡
        search_results = await milvus_service.search_vectors(query_embedding, top_k)

        # è·å–å¯¹åº”çš„æ–‡æ¡£ä¿¡æ¯
        documents = []
        for result in search_results:
            doc_id = result.get('document_id')
            if doc_id:
                doc = db.query(DocumentModel).filter(DocumentModel.id == doc_id).first()
                if doc:
                    documents.append({
                        "document_id": doc_id,
                        "title": doc.title or doc.filename,
                        "filename": doc.filename,
                        "score": result.get('score', 0),
                        "chunk_id": result.get('chunk_id'),
                        "parsed_content": doc.parsed_content
                    })

        return {
            "query_vector_dimension": len(query_embedding),
            "similar_vectors_found": len(search_results),
            "documents_matched": len(documents),
            "documents": documents
        }

    except Exception as e:
        logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
        return {"error": str(e), "documents_matched": 0}

async def _perform_knowledge_graph_search(query_analysis: Dict, top_k: int, neo4j_service) -> Dict[str, Any]:
    """æ‰§è¡ŒçŸ¥è¯†å›¾è°±æœç´¢"""
    try:
        entities = query_analysis.get("key_entities", [])
        if not entities:
            return {"entities_found": 0, "relationships": [], "documents": []}

        # åœ¨Neo4jä¸­æœç´¢ç›¸å…³å®ä½“å’Œå…³ç³»
        all_relationships = []
        all_documents = []

        for entity in entities:
            relationships = await neo4j_service.get_entity_relationships(entity)
            documents = await neo4j_service.get_entity_documents(entity)

            all_relationships.extend(relationships)
            all_documents.extend(documents)

        return {
            "entities_searched": entities,
            "relationships_found": len(all_relationships),
            "documents_found": len(all_documents),
            "relationships": all_relationships[:10],  # é™åˆ¶è¿”å›æ•°é‡
            "documents": all_documents
        }

    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±æœç´¢å¤±è´¥: {e}")
        return {"error": str(e), "documents_found": 0}

async def _perform_structured_query(query_analysis: Dict, db) -> Dict[str, Any]:
    """æ‰§è¡Œç»“æ„åŒ–æŸ¥è¯¢"""
    try:
        entities = query_analysis.get("key_entities", [])
        if not entities:
            return {"records_found": 0, "records": []}

        # åœ¨MySQLä¸­æœç´¢ç›¸å…³è®°å½•
        # è¿™é‡Œç®€åŒ–ä¸ºæœç´¢æ–‡æ¡£æ ‡é¢˜
        all_records = []
        for entity in entities:
            records = db.query(DocumentModel).filter(
                DocumentModel.title.contains(entity)
            ).limit(10).all()

            for record in records:
                all_records.append({
                    "document_id": record.id,
                    "title": record.title,
                    "filename": record.filename,
                    "status": record.status
                })

        return {
            "entities_searched": entities,
            "records_found": len(all_records),
            "records": all_records
        }

    except Exception as e:
        logger.error(f"ç»“æ„åŒ–æŸ¥è¯¢å¤±è´¥: {e}")
        return {"error": str(e), "records_found": 0}

async def _perform_document_content_search(query: str, db) -> Dict[str, Any]:
    """æ‰§è¡Œæ–‡æ¡£å†…å®¹æœç´¢"""
    try:
        # æœç´¢åŒ…å«æŸ¥è¯¢è¯çš„æ–‡æ¡£
        documents = db.query(DocumentModel).filter(
            DocumentModel.title.contains(query) |
            DocumentModel.filename.contains(query)
        ).limit(10).all()

        search_results = []
        for doc in documents:
            # å¦‚æœæœ‰è§£æå†…å®¹ï¼Œæœç´¢ç« èŠ‚æ ‡é¢˜
            matched_sections = []
            if doc.parsed_content:
                # å¤„ç†ä¸åŒæ ¼å¼çš„parsed_content
                parsed = None
                if isinstance(doc.parsed_content, dict):
                    parsed = doc.parsed_content
                elif isinstance(doc.parsed_content, str):
                    try:
                        parsed = json.loads(doc.parsed_content)
                    except:
                        # çº¯æ–‡æœ¬æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                        parsed = {"content": doc.parsed_content}

                if parsed:
                    # ä»contentæ•°ç»„ä¸­æŸ¥æ‰¾åŒ¹é…çš„ç« èŠ‚
                    content_list = parsed.get('content', []) if isinstance(parsed, dict) else []
                    for item in content_list:
                        if isinstance(item, dict):
                            content_text = item.get('content', '')
                            content_type = item.get('type', 'text')
                            # æ£€æŸ¥å†…å®¹æ˜¯å¦åŒ…å«æŸ¥è¯¢è¯
                            if query.lower() in content_text.lower():
                                matched_sections.append({
                                    "section_title": content_text[:100],
                                    "section_content": content_text[:500],
                                    "type": content_type
                                })
                                if len(matched_sections) >= 3:  # é™åˆ¶åŒ¹é…æ•°é‡
                                    break

                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„å†…å®¹ï¼Œè¿”å›æ•´ä¸ªæ–‡æ¡£çš„å‰500å­—ç¬¦
                    if not matched_sections and isinstance(parsed.get('content'), str):
                        matched_sections.append({
                            "section_title": "æ–‡æ¡£å†…å®¹",
                            "section_content": parsed['content'][:500],
                            "type": "text"
                        })
                    elif not matched_sections and isinstance(doc.parsed_content, str):
                        # çº¯æ–‡æœ¬æ ¼å¼
                        matched_sections.append({
                            "section_title": "æ–‡æ¡£å†…å®¹",
                            "section_content": doc.parsed_content[:500],
                            "type": "text"
                        })

            search_results.append({
                "document_id": doc.id,
                "title": doc.title,
                "filename": doc.filename,
                "matched_sections": matched_sections,
                "has_parsed_content": bool(doc.parsed_content)
            })

        return {
            "query": query,
            "documents_found": len(search_results),
            "documents": search_results
        }

    except Exception as e:
        logger.error(f"æ–‡æ¡£å†…å®¹æœç´¢å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"error": str(e), "documents_found": 0}

async def _extract_document_fragments(search_results: Dict, db, search_type: str) -> List[DocumentFragment]:
    """ä»æœç´¢ç»“æœä¸­æå–æ–‡æ¡£ç‰‡æ®µ"""
    fragments = []

    try:
        documents = search_results.get("documents", [])

        for doc_info in documents:
            doc_id = doc_info.get("document_id")
            if not doc_id:
                continue

            # è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯
            doc = db.query(DocumentModel).filter(DocumentModel.id == doc_id).first()
            if not doc or not doc.parsed_content:
                continue

            # å¤„ç†ä¸åŒæ ¼å¼çš„parsed_content
            parsed = None
            if isinstance(doc.parsed_content, dict):
                parsed = doc.parsed_content
            elif isinstance(doc.parsed_content, str):
                try:
                    parsed = json.loads(doc.parsed_content)
                except:
                    # çº¯æ–‡æœ¬æ ¼å¼ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„fragment
                    fragment = DocumentFragment(
                        document_id=doc_id,
                        section_title="æ–‡æ¡£å†…å®¹",
                        content=doc.parsed_content[:500],
                        content_type="text",
                        relevance_score=0.7
                    )
                    fragments.append(fragment)
                    continue

            if not parsed:
                continue

            # ä»contentæ•°ç»„ä¸­æå–ç‰‡æ®µ
            content_list = parsed.get('content', []) if isinstance(parsed, dict) else []
            if content_list:
                for i, item in enumerate(content_list[:3]):  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£æœ€å¤š3ä¸ªç‰‡æ®µ
                    if isinstance(item, dict):
                        content_text = item.get('content', '')
                        content_type = item.get('type', 'text')

                        if content_text:
                            fragment = DocumentFragment(
                                document_id=doc_id,
                                section_id=item.get('id'),
                                chunk_id=doc_info.get('chunk_id'),
                                section_title=content_text[:50] + ("..." if len(content_text) > 50 else ""),
                                content=content_text[:500],
                                content_type=content_type,
                                page_number=item.get('metadata', {}).get('page_number'),
                                relevance_score=doc_info.get('score', 0.8)
                            )
                            fragments.append(fragment)
            elif isinstance(parsed.get('content'), str):
                # å•ä¸ªcontentå­—ç¬¦ä¸²
                fragment = DocumentFragment(
                    document_id=doc_id,
                    section_title="æ–‡æ¡£å†…å®¹",
                    content=parsed['content'][:500],
                    content_type="text",
                    relevance_score=0.7
                )
                fragments.append(fragment)

    except Exception as e:
        logger.error(f"æå–æ–‡æ¡£ç‰‡æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    return fragments

async def _extract_document_fragments_from_content_search(content_results: Dict, db) -> List[DocumentFragment]:
    """ä»æ–‡æ¡£å†…å®¹æœç´¢ç»“æœä¸­æå–ç‰‡æ®µ"""
    fragments = []

    try:
        documents = content_results.get("documents", [])

        for doc_info in documents:
            doc_id = doc_info.get("document_id")
            if not doc_id:
                continue

            # è·å–matched_sections
            matched_sections = doc_info.get("matched_sections", [])

            for section in matched_sections[:3]:  # é™åˆ¶æ¯ä¸ªæ–‡æ¡£æœ€å¤š3ä¸ªç‰‡æ®µ
                section_title = section.get("section_title", "æ–‡æ¡£å†…å®¹")
                section_content = section.get("section_content", "")
                content_type = section.get("type", "text")

                if section_content:
                    fragment = DocumentFragment(
                        document_id=doc_id,
                        section_title=section_title[:100],
                        content=section_content[:500],
                        content_type=content_type,
                        relevance_score=0.8
                    )
                    fragments.append(fragment)

    except Exception as e:
        logger.error(f"ä»å†…å®¹æœç´¢æå–ç‰‡æ®µå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

    return fragments

async def _extract_document_fragments_from_graph(graph_results: Dict, db) -> List[DocumentFragment]:
    """ä»çŸ¥è¯†å›¾è°±ç»“æœä¸­æå–æ–‡æ¡£ç‰‡æ®µ"""
    fragments = []

    try:
        documents = graph_results.get("documents", [])

        for doc_info in documents:
            doc_id = doc_info.get("id")
            if not doc_id:
                continue

            doc = db.query(DocumentModel).filter(DocumentModel.id == doc_id).first()
            if not doc or not doc.parsed_content:
                continue

            # ä»å›¾è°±ç»“æœä¸­æå–ç›¸å…³ç‰‡æ®µ
            parsed_content = doc.parsed_content
            if isinstance(parsed_content, dict) and 'sections' in parsed_content:
                sections = parsed_content['sections'][:2]
                for section in sections:
                    fragment = DocumentFragment(
                        document_id=doc_id,
                        section_title=section.get('title', 'å›¾è°±ç›¸å…³ç« èŠ‚'),
                        content=str(section.get('content', ''))[:300],
                        content_type=_detect_content_type(section.get('content', '')),
                        relevance_score=0.8  # å›¾è°±ç›¸å…³æ–‡æ¡£é»˜è®¤é«˜åˆ†
                    )
                    fragments.append(fragment)

    except Exception as e:
        logger.error(f"ä»å›¾è°±æå–æ–‡æ¡£ç‰‡æ®µå¤±è´¥: {e}")

    return fragments

async def _generate_source_traces(search_results: Dict, db, search_type: str) -> List[SourceTrace]:
    """ç”Ÿæˆæº¯æºä¿¡æ¯"""
    traces = []

    try:
        documents = search_results.get("documents", [])

        for doc_info in documents:
            doc_id = doc_info.get("document_id")
            if not doc_id:
                continue

            doc = db.query(DocumentModel).filter(DocumentModel.id == doc_id).first()
            if not doc:
                continue

            # æ”¶é›†æ–‡æ¡£çš„ç« èŠ‚å’Œå—ä¿¡æ¯
            sections = []
            chunks = []

            if doc.parsed_content and isinstance(doc.parsed_content, dict):
                parsed_sections = doc.parsed_content.get('sections', [])
                for section in parsed_sections[:3]:
                    sections.append({
                        "id": section.get('id'),
                        "title": section.get('title', ''),
                        "type": section.get('type', 'text')
                    })

                parsed_chunks = doc.parsed_content.get('chunks', [])
                for chunk in parsed_chunks[:2]:
                    chunks.append({
                        "id": chunk.get('id'),
                        "type": chunk.get('type', 'text')
                    })

            trace = SourceTrace(
                document_id=doc_id,
                document_title=doc.title or doc.filename,
                document_filename=doc.filename,
                sections=sections,
                chunks=chunks,
                retrieval_path=[search_type, "document_content"],
                confidence_score=doc_info.get('score', 0.8)
            )
            traces.append(trace)

    except Exception as e:
        logger.error(f"ç”Ÿæˆæº¯æºä¿¡æ¯å¤±è´¥: {e}")

    return traces

def _deduplicate_and_rank_fragments(fragments: List[DocumentFragment], top_k: int) -> List[DocumentFragment]:
    """å»é‡å¹¶æ’åºæ–‡æ¡£ç‰‡æ®µ"""
    # æŒ‰æ–‡æ¡£IDå’Œå†…å®¹å»é‡
    seen = set()
    unique_fragments = []

    for fragment in fragments:
        key = (fragment.document_id, fragment.section_title[:100])
        if key not in seen:
            seen.add(key)
            unique_fragments.append(fragment)

    # æŒ‰ç›¸å…³æ€§è¯„åˆ†æ’åº
    unique_fragments.sort(key=lambda x: x.relevance_score, reverse=True)

    return unique_fragments[:top_k]

def _deduplicate_source_traces(traces: List[SourceTrace]) -> List[SourceTrace]:
    """å»é‡æº¯æºä¿¡æ¯"""
    seen_docs = set()
    unique_traces = []

    for trace in traces:
        if trace.document_id not in seen_docs:
            seen_docs.add(trace.document_id)
            unique_traces.append(trace)

    return unique_traces

def _detect_content_type(content: Any) -> str:
    """æ£€æµ‹å†…å®¹ç±»å‹"""
    content_str = str(content).lower()

    if 'table' in content_str or '|' in content_str:
        return 'table'
    elif any(word in content_str for word in ['å…¬å¼', 'formula', '=', '+', '-', '*', '/']):
        return 'formula'
    elif any(word in content_str for word in ['å›¾', 'image', 'chart', 'å›¾å½¢']):
        return 'image_caption'
    else:
        return 'text'

async def _generate_comprehensive_answer(query: str, fragments: List[DocumentFragment], retrieval_info: Dict) -> str:
    """ç”Ÿæˆç»¼åˆå›ç­”"""
    if not fragments:
        return "æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°ä¸æ‚¨æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£å†…å®¹ã€‚"

    # åŸºäºæ–‡æ¡£ç‰‡æ®µç”Ÿæˆå›ç­”
    answer_parts = []
    answer_parts.append(f"æ ¹æ®æ£€ç´¢åˆ°çš„ {len(fragments)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼Œé’ˆå¯¹æ‚¨çš„é—®é¢˜ '{query}' çš„åˆ†æå¦‚ä¸‹ï¼š\n")

    # æŒ‰æ–‡æ¡£åˆ†ç»„å±•ç¤ºç»“æœ
    doc_groups = {}
    for fragment in fragments:
        if fragment.document_id not in doc_groups:
            doc_groups[fragment.document_id] = []
        doc_groups[fragment.document_id].append(fragment)

    for doc_id, doc_fragments in list(doc_groups.items())[:3]:  # æœ€å¤šå±•ç¤º3ä¸ªæ–‡æ¡£
        answer_parts.append(f"\nğŸ“„ **æ–‡æ¡£ {doc_id} çš„ç›¸å…³å†…å®¹ï¼š**")
        for fragment in doc_fragments[:2]:  # æ¯ä¸ªæ–‡æ¡£æœ€å¤š2ä¸ªç‰‡æ®µ
            answer_parts.append(f"â€¢ **{fragment.section_title}**: {fragment.content[:200]}...")

    answer_parts.append(f"\nğŸ” **æ£€ç´¢ä¿¡æ¯:**")
    answer_parts.append(f"â€¢ æ£€ç´¢é˜¶æ®µ: {' â†’ '.join(retrieval_info.get('retrieval_stages', []))}")
    answer_parts.append(f"â€¢ æ•°æ®æº: {', '.join(retrieval_info.get('data_sources_used', []))}")
    answer_parts.append(f"â€¢ ç½®ä¿¡åº¦: {retrieval_info.get('confidence_score', 0):.3f}")

    return "\n".join(answer_parts)

def _calculate_confidence_score(fragments: List[DocumentFragment], retrieval_info: Dict) -> float:
    """è®¡ç®—æ£€ç´¢ç½®ä¿¡åº¦"""
    if not fragments:
        return 0.0

    # åŸºäºç‰‡æ®µæ•°é‡ã€ç›¸å…³æ€§è¯„åˆ†å’Œæ£€ç´¢é˜¶æ®µè®¡ç®—ç½®ä¿¡åº¦
    avg_fragment_score = sum(f.relevance_score for f in fragments) / len(fragments)
    stages_bonus = min(len(retrieval_info.get('retrieval_stages', [])) * 0.1, 0.3)

    confidence = min(avg_fragment_score + stages_bonus, 1.0)
    return round(confidence, 3)